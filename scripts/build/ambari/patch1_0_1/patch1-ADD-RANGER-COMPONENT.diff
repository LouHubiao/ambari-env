Subject: [PATCH] feature: 完成ranger组建的集成
---
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/service_advisor.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/service_advisor.py	(date 1719626239000)
@@ -0,0 +1,862 @@
+#!/usr/bin/env ambari-python-wrap
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+# Python imports
+import imp
+import os
+import traceback
+import re
+import socket
+import fnmatch
+
+
+from resource_management.core.logger import Logger
+
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../../../stacks/')
+PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+
+try:
+  if "BASE_SERVICE_ADVISOR" in os.environ:
+    PARENT_FILE = os.environ["BASE_SERVICE_ADVISOR"]
+  with open(PARENT_FILE, 'rb') as fp:
+    service_advisor = imp.load_module('service_advisor', fp, PARENT_FILE, ('.py', 'rb', imp.PY_SOURCE))
+except Exception as e:
+  traceback.print_exc()
+  print "Failed to load parent"
+
+DB_TYPE_DEFAULT_PORT_MAP = {"MYSQL":"3306", "ORACLE":"1521", "POSTGRES":"5432", "MSSQL":"1433", "SQLA":"2638"}
+
+class RangerServiceAdvisor(service_advisor.ServiceAdvisor):
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(RangerServiceAdvisor, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    # Always call these methods
+    self.modifyMastersWithMultipleInstances()
+    self.modifyCardinalitiesDict()
+    self.modifyHeapSizeProperties()
+    self.modifyNotValuableComponents()
+    self.modifyComponentsNotPreferableOnServer()
+    self.modifyComponentLayoutSchemes()
+
+  def modifyMastersWithMultipleInstances(self):
+    """
+    Modify the set of masters with multiple instances.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyCardinalitiesDict(self):
+    """
+    Modify the dictionary of cardinalities.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyHeapSizeProperties(self):
+    """
+    Modify the dictionary of heap size properties.
+    Must be overriden in child class.
+    """
+    pass
+
+  def modifyNotValuableComponents(self):
+    """
+    Modify the set of components whose host assignment is based on other services.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentsNotPreferableOnServer(self):
+    """
+    Modify the set of components that are not preferable on the server.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentLayoutSchemes(self):
+    """
+    Modify layout scheme dictionaries for components.
+    The scheme dictionary basically maps the number of hosts to
+    host index where component should exist.
+    Must be overriden in child class.
+    """
+
+    self.componentLayoutSchemes.update({'RANGER_ADMIN': {3: 0, 6: 1, 31: 2, "else": 2}})
+    self.componentLayoutSchemes.update({'RANGER_USERSYNC': {3: 0, 6: 1, 31: 2, "else": 2}})
+
+  def getServiceComponentLayoutValidations(self, services, hosts):
+    """
+    Get a list of errors.
+    Must be overriden in child class.
+    """
+
+    return self.getServiceComponentCardinalityValidations(services, hosts, "RANGER")
+
+  def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overriden in child class.
+    """
+    # Logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    recommender = RangerRecommender()
+    recommender.recommendRangerConfigurationsFromHDP206(configurations, clusterData, services, hosts)
+    recommender.recommendRangerConfigurationsFromHDP22(configurations, clusterData, services, hosts)
+    recommender.recommendRangerConfigurationsFromHDP23(configurations, clusterData, services, hosts)
+    recommender.recommendRangerConfigurationsFromHDP25(configurations, clusterData, services, hosts)
+    recommender.recommendRangerConfigurationsFromHDP26(configurations, clusterData, services, hosts)
+    recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+    recommender.recommendConfigurationsForHDP30(configurations, clusterData, services, hosts)
+    recommender.recommendConfigurationsForLDAP(configurations, clusterData, services, hosts)
+
+
+  def getServiceConfigurationRecommendationsForSSO(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overriden in child class.
+    """
+    recommender = RangerRecommender()
+    recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+    """
+    Entry point.
+    Validate configurations for the service. Return a list of errors.
+    The code for this function should be the same for each Service Advisor.
+    """
+    # Logger.info("Class: %s, Method: %s. Validating Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    validator = RangerValidator()
+    # Calls the methods of the validator using arguments,
+    # method(siteProperties, siteRecommendations, configurations, services, hosts)
+    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
+
+  @staticmethod
+  def isKerberosEnabled(services, configurations):
+    """
+    Determines if security is enabled by testing the value of core-site/hadoop.security.authentication enabled.
+    If the property exists and is equal to "kerberos", then is it enabled; otherwise is it assumed to be
+    disabled.
+
+    :type services: dict
+    :param services: the dictionary containing the existing configuration values
+    :type configurations: dict
+    :param configurations: the dictionary containing the updated configuration values
+    :rtype: bool
+    :return: True or False
+    """
+    if configurations and "core-site" in configurations and \
+            "hadoop.security.authentication" in configurations["core-site"]["properties"]:
+      return configurations["core-site"]["properties"]["hadoop.security.authentication"].lower() == "kerberos"
+    elif services and "core-site" in services["configurations"] and \
+            "hadoop.security.authentication" in services["configurations"]["core-site"]["properties"]:
+      return services["configurations"]["core-site"]["properties"]["hadoop.security.authentication"].lower() == "kerberos"
+    else:
+      return False
+
+
+class RangerRecommender(service_advisor.ServiceAdvisor):
+  """
+  Ranger Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(RangerRecommender, self)
+    self.as_super.__init__(*args, **kwargs)
+
+  def recommendRangerConfigurationsFromHDP206(self, configurations, clusterData, services, hosts):
+
+    putRangerAdminProperty = self.putProperty(configurations, "admin-properties", services)
+
+    # Build policymgr_external_url
+    protocol = 'http'
+    ranger_admin_host = 'localhost'
+    port = '6080'
+
+    # Check if http is disabled. For HDP-2.3 this can be checked in ranger-admin-site/ranger.service.http.enabled
+    # For Ranger-0.4.0 this can be checked in ranger-site/http.enabled
+    if ('ranger-site' in services['configurations'] and 'http.enabled' in services['configurations']['ranger-site']['properties'] \
+        and services['configurations']['ranger-site']['properties']['http.enabled'].lower() == 'false') or \
+            ('ranger-admin-site' in services['configurations'] and 'ranger.service.http.enabled' in services['configurations']['ranger-admin-site']['properties'] \
+             and services['configurations']['ranger-admin-site']['properties']['ranger.service.http.enabled'].lower() == 'false'):
+      # HTTPS protocol is used
+      protocol = 'https'
+      # Starting Ranger-0.5.0.2.3 port stored in ranger-admin-site ranger.service.https.port
+      if 'ranger-admin-site' in services['configurations'] and \
+              'ranger.service.https.port' in services['configurations']['ranger-admin-site']['properties']:
+        port = services['configurations']['ranger-admin-site']['properties']['ranger.service.https.port']
+      # In Ranger-0.4.0 port stored in ranger-site https.service.port
+      elif 'ranger-site' in services['configurations'] and \
+              'https.service.port' in services['configurations']['ranger-site']['properties']:
+        port = services['configurations']['ranger-site']['properties']['https.service.port']
+    else:
+      # HTTP protocol is used
+      # Starting Ranger-0.5.0.2.3 port stored in ranger-admin-site ranger.service.http.port
+      if 'ranger-admin-site' in services['configurations'] and \
+              'ranger.service.http.port' in services['configurations']['ranger-admin-site']['properties']:
+        port = services['configurations']['ranger-admin-site']['properties']['ranger.service.http.port']
+      # In Ranger-0.4.0 port stored in ranger-site http.service.port
+      elif 'ranger-site' in services['configurations'] and \
+              'http.service.port' in services['configurations']['ranger-site']['properties']:
+        port = services['configurations']['ranger-site']['properties']['http.service.port']
+
+    ranger_admin_hosts = self.getComponentHostNames(services, "RANGER", "RANGER_ADMIN")
+    if ranger_admin_hosts:
+      if len(ranger_admin_hosts) > 1 \
+              and services['configurations'] \
+              and 'admin-properties' in services['configurations'] and 'policymgr_external_url' in services['configurations']['admin-properties']['properties'] \
+              and services['configurations']['admin-properties']['properties']['policymgr_external_url'] \
+              and services['configurations']['admin-properties']['properties']['policymgr_external_url'].strip():
+
+        # in case of HA deployment keep the policymgr_external_url specified in the config
+        policymgr_external_url = services['configurations']['admin-properties']['properties']['policymgr_external_url']
+      else:
+
+        ranger_admin_host = ranger_admin_hosts[0]
+        policymgr_external_url = "%s://%s:%s" % (protocol, ranger_admin_host, port)
+
+      putRangerAdminProperty('policymgr_external_url', policymgr_external_url)
+
+    rangerServiceVersion = [service['StackServices']['service_version'] for service in services["services"] if service['StackServices']['service_name'] == 'RANGER'][0]
+    if rangerServiceVersion == '0.4.0':
+
+      # Set Ranger Admin Authentication method
+      if 'admin-properties' in services['configurations'] and 'usersync-properties' in services['configurations'] and \
+              'SYNC_SOURCE' in services['configurations']['usersync-properties']['properties']:
+        rangerUserSyncSource = services['configurations']['usersync-properties']['properties']['SYNC_SOURCE']
+        authenticationMethod = rangerUserSyncSource.upper()
+        if authenticationMethod != 'FILE':
+          putRangerAdminProperty('authentication_method', authenticationMethod)
+
+      # Recommend xasecure.audit.destination.hdfs.dir
+      # For Ranger version 0.4.0
+      servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+      putRangerEnvProperty = self.putProperty(configurations, "ranger-env", services)
+      include_hdfs = "HDFS" in servicesList
+      if include_hdfs:
+        if 'core-site' in services['configurations'] and ('fs.defaultFS' in services['configurations']['core-site']['properties']):
+          default_fs = services['configurations']['core-site']['properties']['fs.defaultFS']
+          default_fs += '/ranger/audit/%app-type%/%time:yyyyMMdd%'
+          putRangerEnvProperty('xasecure.audit.destination.hdfs.dir', default_fs)
+
+      # Recommend Ranger Audit properties for ranger supported services
+      # For Ranger version 0.4.0
+      ranger_services = [
+        {'service_name': 'HDFS', 'audit_file': 'ranger-hdfs-plugin-properties'},
+        {'service_name': 'HBASE', 'audit_file': 'ranger-hbase-plugin-properties'},
+        {'service_name': 'HIVE', 'audit_file': 'ranger-hive-plugin-properties'},
+        {'service_name': 'KNOX', 'audit_file': 'ranger-knox-plugin-properties'},
+        {'service_name': 'STORM', 'audit_file': 'ranger-storm-plugin-properties'},
+        {'service_name': 'OZONE', 'audit_file': 'ranger-ozone-plugin-properties'},
+        {'service_name': 'DORIS', 'audit_file': 'ranger-doris-plugin-properties'},
+        {'service_name': 'SPARK3', 'audit_file': 'ranger-spark3-plugin-properties'},
+        {'service_name': 'KUDU', 'audit_file': 'ranger-kudu-plugin-properties'}
+      ]
+
+      for item in range(len(ranger_services)):
+        if ranger_services[item]['service_name'] in servicesList:
+          component_audit_file =  ranger_services[item]['audit_file']
+          if component_audit_file in services["configurations"]:
+            ranger_audit_dict = [
+              {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.db', 'target_configname': 'XAAUDIT.DB.IS_ENABLED'},
+              {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.hdfs', 'target_configname': 'XAAUDIT.HDFS.IS_ENABLED'},
+              {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.hdfs.dir', 'target_configname': 'XAAUDIT.HDFS.DESTINATION_DIRECTORY'}
+            ]
+            putRangerAuditProperty = self.putProperty(configurations, component_audit_file, services)
+
+            for item in ranger_audit_dict:
+              if item['filename'] in services["configurations"] and item['configname'] in  services["configurations"][item['filename']]["properties"]:
+                if item['filename'] in configurations and item['configname'] in  configurations[item['filename']]["properties"]:
+                  rangerAuditProperty = configurations[item['filename']]["properties"][item['configname']]
+                else:
+                  rangerAuditProperty = services["configurations"][item['filename']]["properties"][item['configname']]
+                putRangerAuditProperty(item['target_configname'], rangerAuditProperty)
+
+  def recommendRangerConfigurationsFromHDP22(self, configurations, clusterData, services, hosts):
+    putRangerEnvProperty = self.putProperty(configurations, "ranger-env")
+    cluster_env = self.getServicesSiteProperties(services, "cluster-env")
+    security_enabled = cluster_env is not None and "security_enabled" in cluster_env and \
+                       cluster_env["security_enabled"].lower() == "true"
+    if "ranger-env" in configurations and not security_enabled:
+      putRangerEnvProperty("ranger-storm-plugin-enabled", "No")
+
+  def getDBConnectionHostPort(self, db_type, db_host):
+    connection_string = ""
+    if db_type is None or db_type == "":
+      return connection_string
+    else:
+      colon_count = db_host.count(':')
+      if colon_count == 0:
+        if DB_TYPE_DEFAULT_PORT_MAP.has_key(db_type):
+          connection_string = db_host + ":" + DB_TYPE_DEFAULT_PORT_MAP[db_type]
+        else:
+          connection_string = db_host
+      elif colon_count == 1:
+        connection_string = db_host
+      elif colon_count == 2:
+        connection_string = db_host
+
+    return connection_string
+
+  def getOracleDBConnectionHostPort(self, db_type, db_host, rangerDbName):
+    connection_string = self.getDBConnectionHostPort(db_type, db_host)
+    colon_count = db_host.count(':')
+    if colon_count == 1 and '/' in db_host:
+      connection_string = "//" + connection_string
+    elif colon_count == 0 or colon_count == 1:
+      connection_string = "//" + connection_string + "/" + rangerDbName if rangerDbName else "//" + connection_string
+
+    return connection_string
+
+  def recommendRangerUrlConfigurations(self, configurations, services, requiredServices):
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+
+    policymgr_external_url = ""
+    if 'admin-properties' in services['configurations'] and 'policymgr_external_url' in services['configurations']['admin-properties']['properties']:
+      if 'admin-properties' in configurations and 'policymgr_external_url' in configurations['admin-properties']['properties']:
+        policymgr_external_url = configurations['admin-properties']['properties']['policymgr_external_url']
+      else:
+        policymgr_external_url = services['configurations']['admin-properties']['properties']['policymgr_external_url']
+
+    for index in range(len(requiredServices)):
+      if requiredServices[index]['service_name'] in servicesList:
+        component_config_type = requiredServices[index]['config_type']
+        component_name = requiredServices[index]['service_name']
+        component_config_property = 'ranger.plugin.{0}.policy.rest.url'.format(component_name.lower())
+        if requiredServices[index]['service_name'] == 'RANGER_KMS':
+          component_config_property = 'ranger.plugin.kms.policy.rest.url'
+        putRangerSecurityProperty = self.putProperty(configurations, component_config_type, services)
+        if component_config_type in services["configurations"] and component_config_property in services["configurations"][component_config_type]["properties"]:
+          putRangerSecurityProperty(component_config_property, policymgr_external_url)
+
+  def recommendRangerConfigurationsFromHDP23(self, configurations, clusterData, services, hosts):
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    putRangerAdminProperty = self.putProperty(configurations, "ranger-admin-site", services)
+    putRangerEnvProperty = self.putProperty(configurations, "ranger-env", services)
+    putRangerUgsyncSite = self.putProperty(configurations, "ranger-ugsync-site", services)
+
+    if 'admin-properties' in services['configurations'] and ('DB_FLAVOR' in services['configurations']['admin-properties']['properties']) \
+            and ('db_host' in services['configurations']['admin-properties']['properties']) and ('db_name' in services['configurations']['admin-properties']['properties']):
+
+      rangerDbFlavor = services['configurations']["admin-properties"]["properties"]["DB_FLAVOR"]
+      rangerDbHost =   services['configurations']["admin-properties"]["properties"]["db_host"]
+      rangerDbName =   services['configurations']["admin-properties"]["properties"]["db_name"]
+      ranger_db_url_dict = {
+        'MYSQL': {'ranger.jpa.jdbc.driver': 'com.mysql.jdbc.Driver',
+                  'ranger.jpa.jdbc.url': 'jdbc:mysql://' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + '/' + rangerDbName},
+        'ORACLE': {'ranger.jpa.jdbc.driver': 'oracle.jdbc.driver.OracleDriver',
+                   'ranger.jpa.jdbc.url': 'jdbc:oracle:thin:@' + self.getOracleDBConnectionHostPort(rangerDbFlavor, rangerDbHost, rangerDbName)},
+        'POSTGRES': {'ranger.jpa.jdbc.driver': 'org.postgresql.Driver',
+                     'ranger.jpa.jdbc.url': 'jdbc:postgresql://' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + '/' + rangerDbName},
+        'MSSQL': {'ranger.jpa.jdbc.driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver',
+                  'ranger.jpa.jdbc.url': 'jdbc:sqlserver://' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + ';databaseName=' + rangerDbName},
+        'SQLA': {'ranger.jpa.jdbc.driver': 'sap.jdbc4.sqlanywhere.IDriver',
+                 'ranger.jpa.jdbc.url': 'jdbc:sqlanywhere:host=' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + ';database=' + rangerDbName}
+      }
+      rangerDbProperties = ranger_db_url_dict.get(rangerDbFlavor, ranger_db_url_dict['MYSQL'])
+      for key in rangerDbProperties:
+        putRangerAdminProperty(key, rangerDbProperties.get(key))
+
+      if 'admin-properties' in services['configurations'] and ('DB_FLAVOR' in services['configurations']['admin-properties']['properties']) \
+              and ('db_host' in services['configurations']['admin-properties']['properties']):
+
+        rangerDbFlavor = services['configurations']["admin-properties"]["properties"]["DB_FLAVOR"]
+        rangerDbHost =   services['configurations']["admin-properties"]["properties"]["db_host"]
+        ranger_db_privelege_url_dict = {
+          'MYSQL': {'ranger_privelege_user_jdbc_url': 'jdbc:mysql://' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost)},
+          'ORACLE': {'ranger_privelege_user_jdbc_url': 'jdbc:oracle:thin:@' + self.getOracleDBConnectionHostPort(rangerDbFlavor, rangerDbHost, None)},
+          'POSTGRES': {'ranger_privelege_user_jdbc_url': 'jdbc:postgresql://' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + '/postgres'},
+          'MSSQL': {'ranger_privelege_user_jdbc_url': 'jdbc:sqlserver://' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + ';'},
+          'SQLA': {'ranger_privelege_user_jdbc_url': 'jdbc:sqlanywhere:host=' + self.getDBConnectionHostPort(rangerDbFlavor, rangerDbHost) + ';'}
+        }
+        rangerPrivelegeDbProperties = ranger_db_privelege_url_dict.get(rangerDbFlavor, ranger_db_privelege_url_dict['MYSQL'])
+        for key in rangerPrivelegeDbProperties:
+          putRangerEnvProperty(key, rangerPrivelegeDbProperties.get(key))
+
+    # Recommend Ranger Authentication method
+    authMap = {
+      'org.apache.ranger.unixusersync.process.UnixUserGroupBuilder': 'UNIX',
+      'org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder': 'LDAP'
+    }
+
+    if 'ranger-ugsync-site' in services['configurations'] and 'ranger.usersync.source.impl.class' in services['configurations']["ranger-ugsync-site"]["properties"]:
+      rangerUserSyncClass = services['configurations']["ranger-ugsync-site"]["properties"]["ranger.usersync.source.impl.class"]
+      if rangerUserSyncClass in authMap:
+        rangerSqlConnectorProperty = authMap.get(rangerUserSyncClass)
+        putRangerAdminProperty('ranger.authentication.method', rangerSqlConnectorProperty)
+
+
+    if 'ranger-env' in services['configurations'] and 'is_solrCloud_enabled' in services['configurations']["ranger-env"]["properties"]:
+      isSolrCloudEnabled = services['configurations']["ranger-env"]["properties"]["is_solrCloud_enabled"]  == "true"
+    else:
+      isSolrCloudEnabled = False
+
+    if isSolrCloudEnabled:
+      zookeeper_host_port = self.getZKHostPortString(services)
+      ranger_audit_zk_port = ''
+      if zookeeper_host_port:
+        ranger_audit_zk_port = '{0}/{1}'.format(zookeeper_host_port, 'ranger_audits')
+        putRangerAdminProperty('ranger.audit.solr.zookeepers', ranger_audit_zk_port)
+    else:
+      putRangerAdminProperty('ranger.audit.solr.zookeepers', 'NONE')
+
+    # Recommend ranger.audit.solr.zookeepers and xasecure.audit.destination.hdfs.dir
+    include_hdfs = "HDFS" in servicesList
+    if include_hdfs:
+      if 'core-site' in services['configurations'] and ('fs.defaultFS' in services['configurations']['core-site']['properties']):
+        default_fs = services['configurations']['core-site']['properties']['fs.defaultFS']
+        putRangerEnvProperty('xasecure.audit.destination.hdfs.dir', '{0}/{1}/{2}'.format(default_fs,'ranger','audit'))
+
+    # Recommend Ranger supported service's audit properties
+    ranger_services = [
+      {'service_name': 'HDFS', 'audit_file': 'ranger-hdfs-audit'},
+      {'service_name': 'YARN', 'audit_file': 'ranger-yarn-audit'},
+      {'service_name': 'HBASE', 'audit_file': 'ranger-hbase-audit'},
+      {'service_name': 'HIVE', 'audit_file': 'ranger-hive-audit'},
+      {'service_name': 'KNOX', 'audit_file': 'ranger-knox-audit'},
+      {'service_name': 'KAFKA', 'audit_file': 'ranger-kafka-audit'},
+      {'service_name': 'STORM', 'audit_file': 'ranger-storm-audit'},
+      {'service_name': 'OZONE', 'audit_file': 'ranger-ozone-audit'},
+      {'service_name': 'DORIS', 'audit_file': 'ranger-doris-audit'},
+      {'service_name': 'SPARK3', 'audit_file': 'ranger-spark3-audit'}
+    ]
+
+    for item in range(len(ranger_services)):
+      if ranger_services[item]['service_name'] in servicesList:
+        component_audit_file =  ranger_services[item]['audit_file']
+        if component_audit_file in services["configurations"]:
+          ranger_audit_dict = [
+            {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.db', 'target_configname': 'xasecure.audit.destination.db'},
+            {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.hdfs', 'target_configname': 'xasecure.audit.destination.hdfs'},
+            {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.hdfs.dir', 'target_configname': 'xasecure.audit.destination.hdfs.dir'},
+            {'filename': 'ranger-env', 'configname': 'xasecure.audit.destination.solr', 'target_configname': 'xasecure.audit.destination.solr'},
+            {'filename': 'ranger-admin-site', 'configname': 'ranger.audit.solr.urls', 'target_configname': 'xasecure.audit.destination.solr.urls'},
+            {'filename': 'ranger-admin-site', 'configname': 'ranger.audit.solr.zookeepers', 'target_configname': 'xasecure.audit.destination.solr.zookeepers'}
+          ]
+          putRangerAuditProperty = self.putProperty(configurations, component_audit_file, services)
+
+          for item in ranger_audit_dict:
+            if item['filename'] in services["configurations"] and item['configname'] in  services["configurations"][item['filename']]["properties"]:
+              if item['filename'] in configurations and item['configname'] in  configurations[item['filename']]["properties"]:
+                rangerAuditProperty = configurations[item['filename']]["properties"][item['configname']]
+              else:
+                rangerAuditProperty = services["configurations"][item['filename']]["properties"][item['configname']]
+              putRangerAuditProperty(item['target_configname'], rangerAuditProperty)
+
+    audit_solr_flag = 'false'
+    audit_db_flag = 'false'
+    ranger_audit_source_type = 'solr'
+    if 'ranger-env' in services['configurations'] and 'xasecure.audit.destination.solr' in services['configurations']["ranger-env"]["properties"]:
+      audit_solr_flag = services['configurations']["ranger-env"]["properties"]['xasecure.audit.destination.solr']
+    if 'ranger-env' in services['configurations'] and 'xasecure.audit.destination.db' in services['configurations']["ranger-env"]["properties"]:
+      audit_db_flag = services['configurations']["ranger-env"]["properties"]['xasecure.audit.destination.db']
+
+    if audit_db_flag == 'true' and audit_solr_flag == 'false':
+      ranger_audit_source_type = 'db'
+    putRangerAdminProperty('ranger.audit.source.type',ranger_audit_source_type)
+
+    knox_host = 'localhost'
+    knox_port = '8443'
+    if 'KNOX' in servicesList:
+      knox_hosts = self.getComponentHostNames(services, "KNOX", "KNOX_GATEWAY")
+      if len(knox_hosts) > 0:
+        knox_hosts.sort()
+        knox_host = knox_hosts[0]
+      if 'gateway-site' in services['configurations'] and 'gateway.port' in services['configurations']["gateway-site"]["properties"]:
+        knox_port = services['configurations']["gateway-site"]["properties"]['gateway.port']
+      putRangerAdminProperty('ranger.sso.providerurl', 'https://{0}:{1}/gateway/knoxsso/api/v1/websso'.format(knox_host, knox_port))
+
+    required_services = [
+      {'service_name': 'HDFS', 'config_type': 'ranger-hdfs-security'},
+      {'service_name': 'YARN', 'config_type': 'ranger-yarn-security'},
+      {'service_name': 'HBASE', 'config_type': 'ranger-hbase-security'},
+      {'service_name': 'HIVE', 'config_type': 'ranger-hive-security'},
+      {'service_name': 'KNOX', 'config_type': 'ranger-knox-security'},
+      {'service_name': 'KAFKA', 'config_type': 'ranger-kafka-security'},
+      {'service_name': 'RANGER_KMS','config_type': 'ranger-kms-security'},
+      {'service_name': 'STORM', 'config_type': 'ranger-storm-security'},
+      {'service_name': 'OZONE', 'config_type': 'ranger-ozone-security'},
+      {'service_name': 'DORIS', 'config_type': 'ranger-doris-security'},
+      {'service_name': 'SPARK3', 'config_type': 'ranger-spark3-security'}
+    ]
+
+    # recommendation for ranger url for ranger-supported plugins
+    self.recommendRangerUrlConfigurations(configurations, services, required_services)
+
+  def recommendRangerConfigurationsFromHDP25(self, configurations, clusterData, services, hosts):
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    has_ranger_tagsync = False
+
+    putTagsyncAppProperty = self.putProperty(configurations, "tagsync-application-properties", services)
+    putTagsyncSiteProperty = self.putProperty(configurations, "ranger-tagsync-site", services)
+    putRangerAdminProperty = self.putProperty(configurations, "ranger-admin-site", services)
+    putRangerEnvProperty = self.putProperty(configurations, "ranger-env", services)
+
+    application_properties = self.getServicesSiteProperties(services, "application-properties")
+
+    ranger_tagsync_host = self.getHostsForComponent(services, "RANGER", "RANGER_TAGSYNC")
+    has_ranger_tagsync = len(ranger_tagsync_host) > 0
+
+    if 'ATLAS' in servicesList and has_ranger_tagsync:
+      atlas_hosts = self.getHostNamesWithComponent("ATLAS", "ATLAS_SERVER", services)
+      atlas_host = 'localhost' if len(atlas_hosts) == 0 else atlas_hosts[0]
+      protocol = 'http'
+      atlas_port = '21000'
+
+      if application_properties and 'atlas.enableTLS' in application_properties and application_properties['atlas.enableTLS'].lower() == 'true':
+        protocol = 'https'
+        if 'atlas.server.https.port' in application_properties:
+          atlas_port = application_properties['atlas.server.https.port']
+      else:
+        protocol = 'http'
+        if application_properties and 'atlas.server.http.port' in application_properties:
+          atlas_port = application_properties['atlas.server.http.port']
+
+      atlas_rest_endpoint = '{0}://{1}:{2}'.format(protocol, atlas_host, atlas_port)
+
+      putTagsyncSiteProperty('ranger.tagsync.source.atlas', 'true')
+      putTagsyncSiteProperty('ranger.tagsync.source.atlasrest.endpoint', atlas_rest_endpoint)
+
+    zookeeper_host_port = self.getZKHostPortString(services)
+    if zookeeper_host_port and has_ranger_tagsync:
+      putTagsyncAppProperty('atlas.kafka.zookeeper.connect', zookeeper_host_port)
+
+    if 'KAFKA' in servicesList and has_ranger_tagsync:
+      kafka_hosts = self.getHostNamesWithComponent("KAFKA", "KAFKA_BROKER", services)
+      kafka_port = '6667'
+      if 'kafka-broker' in services['configurations'] and (
+              'port' in services['configurations']['kafka-broker']['properties']):
+        kafka_port = services['configurations']['kafka-broker']['properties']['port']
+      kafka_host_port = []
+      for i in range(len(kafka_hosts)):
+        kafka_host_port.append(kafka_hosts[i] + ':' + kafka_port)
+
+      final_kafka_host = ",".join(kafka_host_port)
+      putTagsyncAppProperty('atlas.kafka.bootstrap.servers', final_kafka_host)
+
+    is_solr_cloud_enabled = False
+    if 'ranger-env' in services['configurations'] and 'is_solrCloud_enabled' in services['configurations']['ranger-env']['properties']:
+      is_solr_cloud_enabled = services['configurations']['ranger-env']['properties']['is_solrCloud_enabled']  == 'true'
+
+    is_external_solr_cloud_enabled = False
+    if 'ranger-env' in services['configurations'] and 'is_external_solrCloud_enabled' in services['configurations']['ranger-env']['properties']:
+      is_external_solr_cloud_enabled = services['configurations']['ranger-env']['properties']['is_external_solrCloud_enabled']  == 'true'
+
+    ranger_audit_zk_port = ''
+
+    if 'AMBARI_INFRA_SOLR' in servicesList and zookeeper_host_port and is_solr_cloud_enabled and not is_external_solr_cloud_enabled:
+      zookeeper_host_port = zookeeper_host_port.split(',')
+      zookeeper_host_port.sort()
+      zookeeper_host_port = ",".join(zookeeper_host_port)
+      infra_solr_znode = '/infra-solr'
+
+      if 'infra-solr-env' in services['configurations'] and \
+              ('infra_solr_znode' in services['configurations']['infra-solr-env']['properties']):
+        infra_solr_znode = services['configurations']['infra-solr-env']['properties']['infra_solr_znode']
+        ranger_audit_zk_port = '{0}{1}'.format(zookeeper_host_port, infra_solr_znode)
+      putRangerAdminProperty('ranger.audit.solr.zookeepers', ranger_audit_zk_port)
+    elif zookeeper_host_port and is_solr_cloud_enabled and is_external_solr_cloud_enabled:
+      ranger_audit_zk_port = '{0}/{1}'.format(zookeeper_host_port, 'ranger_audits')
+      putRangerAdminProperty('ranger.audit.solr.zookeepers', ranger_audit_zk_port)
+    else:
+      putRangerAdminProperty('ranger.audit.solr.zookeepers', 'NONE')
+
+    ranger_services = [
+      {'service_name': 'HDFS', 'audit_file': 'ranger-hdfs-audit'},
+      {'service_name': 'YARN', 'audit_file': 'ranger-yarn-audit'},
+      {'service_name': 'HBASE', 'audit_file': 'ranger-hbase-audit'},
+      {'service_name': 'HIVE', 'audit_file': 'ranger-hive-audit'},
+      {'service_name': 'KNOX', 'audit_file': 'ranger-knox-audit'},
+      {'service_name': 'KAFKA', 'audit_file': 'ranger-kafka-audit'},
+      {'service_name': 'STORM', 'audit_file': 'ranger-storm-audit'},
+      {'service_name': 'RANGER_KMS', 'audit_file': 'ranger-kms-audit'},
+      {'service_name': 'ATLAS', 'audit_file': 'ranger-atlas-audit'},
+      {'service_name': 'OZONE', 'audit_file': 'ranger-ozone-audit'},
+      {'service_name': 'DORIS', 'audit_file': 'ranger-doris-audit'},
+      {'service_name': 'SPARK3', 'audit_file': 'ranger-spark3-audit'},
+    ]
+
+    for item in range(len(ranger_services)):
+      if ranger_services[item]['service_name'] in servicesList:
+        component_audit_file =  ranger_services[item]['audit_file']
+        if component_audit_file in services["configurations"]:
+          ranger_audit_dict = [
+            {'filename': 'ranger-admin-site', 'configname': 'ranger.audit.solr.urls', 'target_configname': 'xasecure.audit.destination.solr.urls'},
+            {'filename': 'ranger-admin-site', 'configname': 'ranger.audit.solr.zookeepers', 'target_configname': 'xasecure.audit.destination.solr.zookeepers'}
+          ]
+          putRangerAuditProperty = self.putProperty(configurations, component_audit_file, services)
+
+          for item in ranger_audit_dict:
+            if item['filename'] in services["configurations"] and item['configname'] in  services["configurations"][item['filename']]["properties"]:
+              if item['filename'] in configurations and item['configname'] in  configurations[item['filename']]["properties"]:
+                rangerAuditProperty = configurations[item['filename']]["properties"][item['configname']]
+              else:
+                rangerAuditProperty = services["configurations"][item['filename']]["properties"][item['configname']]
+              putRangerAuditProperty(item['target_configname'], rangerAuditProperty)
+
+    if "HDFS" in servicesList:
+      hdfs_user = None
+      if "hadoop-env" in services["configurations"] and "hdfs_user" in services["configurations"]["hadoop-env"]["properties"]:
+        hdfs_user = services["configurations"]["hadoop-env"]["properties"]["hdfs_user"]
+        putRangerAdminProperty('ranger.kms.service.user.hdfs', hdfs_user)
+
+    if "HIVE" in servicesList:
+      hive_user = None
+      if "hive-env" in services["configurations"] and "hive_user" in services["configurations"]["hive-env"]["properties"]:
+        hive_user = services["configurations"]["hive-env"]["properties"]["hive_user"]
+        putRangerAdminProperty('ranger.kms.service.user.hive', hive_user)
+
+    ranger_plugins_serviceuser = [
+      {'service_name': 'HDFS', 'file_name': 'hadoop-env', 'config_name': 'hdfs_user', 'target_configname': 'ranger.plugins.hdfs.serviceuser'},
+      {'service_name': 'HIVE', 'file_name': 'hive-env', 'config_name': 'hive_user', 'target_configname': 'ranger.plugins.hive.serviceuser'},
+      {'service_name': 'YARN', 'file_name': 'yarn-env', 'config_name': 'yarn_user', 'target_configname': 'ranger.plugins.yarn.serviceuser'},
+      {'service_name': 'HBASE', 'file_name': 'hbase-env', 'config_name': 'hbase_user', 'target_configname': 'ranger.plugins.hbase.serviceuser'},
+      {'service_name': 'KNOX', 'file_name': 'knox-env', 'config_name': 'knox_user', 'target_configname': 'ranger.plugins.knox.serviceuser'},
+      {'service_name': 'STORM', 'file_name': 'storm-env', 'config_name': 'storm_user', 'target_configname': 'ranger.plugins.storm.serviceuser'},
+      {'service_name': 'KAFKA', 'file_name': 'kafka-env', 'config_name': 'kafka_user', 'target_configname': 'ranger.plugins.kafka.serviceuser'},
+      {'service_name': 'RANGER_KMS', 'file_name': 'kms-env', 'config_name': 'kms_user', 'target_configname': 'ranger.plugins.kms.serviceuser'},
+      {'service_name': 'ATLAS', 'file_name': 'atlas-env', 'config_name': 'metadata_user', 'target_configname': 'ranger.plugins.atlas.serviceuser'},
+      {'service_name': 'OZONE', 'file_name': 'ozone-env', 'config_name': 'ozone_user', 'target_configname': 'ranger.plugins.ozone.serviceuser'},
+      {'service_name': 'DORIS', 'file_name': 'doris-env', 'config_name': 'doris_user', 'target_configname': 'ranger.plugins.doris.serviceuser'},
+      {'service_name': 'SPARK3', 'file_name': 'spark3-env', 'config_name': 'spark_user', 'target_configname': 'ranger.plugins.saprk3.serviceuser'}
+
+
+    ]
+
+    for item in range(len(ranger_plugins_serviceuser)):
+      if ranger_plugins_serviceuser[item]['service_name'] in servicesList:
+        file_name = ranger_plugins_serviceuser[item]['file_name']
+        config_name = ranger_plugins_serviceuser[item]['config_name']
+        target_configname = ranger_plugins_serviceuser[item]['target_configname']
+        if file_name in services["configurations"] and config_name in services["configurations"][file_name]["properties"]:
+          service_user = services["configurations"][file_name]["properties"][config_name]
+          putRangerAdminProperty(target_configname, service_user)
+
+    if "ATLAS" in servicesList:
+      if "ranger-env" in services["configurations"]:
+        putAtlasRangerAuditProperty = self.putProperty(configurations, 'ranger-atlas-audit', services)
+        xasecure_audit_destination_hdfs = ''
+        xasecure_audit_destination_hdfs_dir = ''
+        xasecure_audit_destination_solr = ''
+        if 'xasecure.audit.destination.hdfs' in configurations['ranger-env']['properties']:
+          xasecure_audit_destination_hdfs = configurations['ranger-env']['properties']['xasecure.audit.destination.hdfs']
+        else:
+          xasecure_audit_destination_hdfs = services['configurations']['ranger-env']['properties']['xasecure.audit.destination.hdfs']
+
+        if 'core-site' in services['configurations'] and ('fs.defaultFS' in services['configurations']['core-site']['properties']):
+          xasecure_audit_destination_hdfs_dir = '{0}/{1}/{2}'.format(services['configurations']['core-site']['properties']['fs.defaultFS'] ,'ranger','audit')
+
+        if 'xasecure.audit.destination.solr' in configurations['ranger-env']['properties']:
+          xasecure_audit_destination_solr = configurations['ranger-env']['properties']['xasecure.audit.destination.solr']
+        else:
+          xasecure_audit_destination_solr = services['configurations']['ranger-env']['properties']['xasecure.audit.destination.solr']
+
+        putAtlasRangerAuditProperty('xasecure.audit.destination.hdfs',xasecure_audit_destination_hdfs)
+        putAtlasRangerAuditProperty('xasecure.audit.destination.hdfs.dir',xasecure_audit_destination_hdfs_dir)
+        putAtlasRangerAuditProperty('xasecure.audit.destination.solr',xasecure_audit_destination_solr)
+    required_services = [
+      {'service_name': 'ATLAS', 'config_type': 'ranger-atlas-security'}
+    ]
+
+    # recommendation for ranger url for ranger-supported plugins
+    self.recommendRangerUrlConfigurations(configurations, services, required_services)
+
+  def recommendRangerConfigurationsFromHDP26(self, configurations, clusterData, services, hosts):
+
+    putRangerUgsyncSite = self.putProperty(configurations, 'ranger-ugsync-site', services)
+
+    delta_sync_enabled = False
+    if 'ranger-ugsync-site' in services['configurations'] and 'ranger.usersync.ldap.deltasync' in services['configurations']['ranger-ugsync-site']['properties']:
+      delta_sync_enabled = services['configurations']['ranger-ugsync-site']['properties']['ranger.usersync.ldap.deltasync'] == "true"
+
+    if delta_sync_enabled:
+      putRangerUgsyncSite("ranger.usersync.group.searchenabled", "true")
+    else:
+      putRangerUgsyncSite("ranger.usersync.group.searchenabled", "false")
+
+
+  def recommendConfigurationsForSSO(self, configurations, clusterData, services, hosts):
+    ambari_configuration = self.get_ambari_configuration(services)
+    ambari_sso_details = ambari_configuration.get_ambari_sso_details() if ambari_configuration else None
+
+    if ambari_sso_details and ambari_sso_details.is_managing_services():
+      putRangerAdminSiteProperty = self.putProperty(configurations, "ranger-admin-site", services)
+
+      # If SSO should be enabled for this service, continue
+      if ambari_sso_details.should_enable_sso('RANGER'):
+        putRangerAdminSiteProperty('ranger.sso.enabled', "true")
+        putRangerAdminSiteProperty('ranger.sso.providerurl', ambari_sso_details.get_sso_provider_url())
+        putRangerAdminSiteProperty('ranger.sso.publicKey', ambari_sso_details.get_sso_provider_certificate(False, True))
+        putRangerAdminSiteProperty('ranger.sso.browser.useragent', 'Mozilla,chrome')
+
+      # If SSO should be disabled for this service
+      elif ambari_sso_details.should_disable_sso('RANGER'):
+        putRangerAdminSiteProperty('ranger.sso.enabled', "false")
+
+
+  def recommendConfigurationsForHDP30(self, configurations, clusterData, services, hosts):
+    putRangerAdminProperty = self.putProperty(configurations, 'ranger-admin-site', services)
+
+    enable_usersync_ldap_starttls = False
+    if 'ranger-ugsync-site' in services['configurations'] and 'ranger.usersync.ldap.starttls' in services['configurations']['ranger-ugsync-site']['properties']:
+      enable_usersync_ldap_starttls = services['configurations']['ranger-ugsync-site']['properties']['ranger.usersync.ldap.starttls'] == "true"
+
+    if enable_usersync_ldap_starttls:
+      putRangerAdminProperty("ranger.ldap.starttls", "true")
+    else:
+      putRangerAdminProperty("ranger.ldap.starttls", "false")
+
+  def recommendConfigurationsForLDAP(self, configurations, clusterData, services, hosts):
+    ambari_configuration = self.get_ambari_configuration(services)
+    ldap_properties = ambari_configuration.get_ambari_server_configuration_category("ldap-configuration")
+
+    if ldap_properties and 'ambari.ldap.authentication.enabled' in ldap_properties and ldap_properties['ambari.ldap.authentication.enabled'].lower() == "true":
+
+      putRangerUgsyncSite = self.putProperty(configurations, "ranger-ugsync-site", services)
+
+      if 'ambari.ldap.attributes.user.search_base' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.ldap.searchBase', ldap_properties['ambari.ldap.attributes.user.search_base'])
+      if 'ambari.ldap.attributes.group.member_attr' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.group.memberattributename', ldap_properties['ambari.ldap.attributes.group.member_attr'])
+      if 'ambari.ldap.attributes.group.name_attr' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.group.nameattribute', ldap_properties['ambari.ldap.attributes.group.name_attr'])
+      if 'ambari.ldap.attributes.group.object_class' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.group.objectclass', ldap_properties['ambari.ldap.attributes.group.object_class'])
+      if 'ambari.ldap.connectivity.bind_dn' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.ldap.binddn', ldap_properties['ambari.ldap.connectivity.bind_dn'])
+      if 'ambari.ldap.connectivity.server.host' in ldap_properties:
+        ldap_protocol =  'ldap://'
+        if 'ambari.ldap.connectivity.use_ssl' in ldap_properties and ldap_properties['ambari.ldap.connectivity.use_ssl'] == 'true':
+          ldap_protocol =  'ldaps://'
+        ldap_port = '389'
+        if 'ambari.ldap.connectivity.server.port' in ldap_properties:
+          ldap_port = ldap_properties['ambari.ldap.connectivity.server.port']
+        ldapUrl = ldap_protocol + ldap_properties['ambari.ldap.connectivity.server.host'] + ":" + ldap_port
+        putRangerUgsyncSite('ranger.usersync.ldap.url', ldapUrl)
+      if 'ambari.ldap.attributes.user.object_class' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.ldap.user.objectclass', ldap_properties['ambari.ldap.attributes.user.object_class'])
+      if 'ambari.ldap.attributes.user.name_attr' in ldap_properties:
+        putRangerUgsyncSite('ranger.usersync.ldap.user.nameattribute', ldap_properties['ambari.ldap.attributes.user.name_attr'])
+
+class RangerValidator(service_advisor.ServiceAdvisor):
+  """
+  Ranger Validator checks the correctness of properties whenever the service is first added or the user attempts to
+  change configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(RangerValidator, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    self.validators = [("ranger-env", self.validateRangerConfigurationsEnvFromHDP22),
+                       ("admin-properties", self.validateRangerAdminConfigurationsFromHDP23),
+                       ("ranger-env", self.validateRangerConfigurationsEnvFromHDP23),
+                       ("ranger-tagsync-site", self.validateRangerTagsyncConfigurationsFromHDP25),
+                       ("ranger-ugsync-site", self.validateRangerUsersyncConfigurationsFromHDP26),
+                       ("ranger-env", self.validateRangerPasswordConfigurations)]
+
+  def validateRangerConfigurationsEnvFromHDP22(self, properties, recommendedDefaults, configurations, services, hosts):
+    ranger_env_properties = properties
+    validationItems = []
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    if "ranger-storm-plugin-enabled" in ranger_env_properties and ranger_env_properties['ranger-storm-plugin-enabled'].lower() == 'yes' and not 'KERBEROS' in servicesList:
+      validationItems.append({"config-name": "ranger-storm-plugin-enabled",
+                              "item": self.getWarnItem("Ranger Storm plugin should not be enabled in non-kerberos environment.")})
+    return self.toConfigurationValidationProblems(validationItems, "ranger-env")
+
+  def validateRangerAdminConfigurationsFromHDP23(self, properties, recommendedDefaults, configurations, services, hosts):
+    ranger_site = properties
+    validationItems = []
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    if 'RANGER' in servicesList and 'policymgr_external_url' in ranger_site:
+      policymgr_mgr_url = ranger_site['policymgr_external_url']
+      if policymgr_mgr_url.endswith('/'):
+        validationItems.append({'config-name':'policymgr_external_url',
+                                'item':self.getWarnItem('Ranger External URL should not contain trailing slash "/"')})
+    return self.toConfigurationValidationProblems(validationItems,'admin-properties')
+
+  def validateRangerConfigurationsEnvFromHDP23(self, properties, recommendedDefaults, configurations, services, hosts):
+    ranger_env_properties = properties
+    validationItems = []
+    security_enabled = RangerServiceAdvisor.isKerberosEnabled(services, configurations)
+
+    if "ranger-kafka-plugin-enabled" in ranger_env_properties and ranger_env_properties["ranger-kafka-plugin-enabled"].lower() == 'yes' and not security_enabled:
+      validationItems.append({"config-name": "ranger-kafka-plugin-enabled",
+                              "item": self.getWarnItem(
+                                "Ranger Kafka plugin should not be enabled in non-kerberos environment.")})
+
+    validationProblems = self.toConfigurationValidationProblems(validationItems, "ranger-env")
+    return validationProblems
+
+  def validateRangerTagsyncConfigurationsFromHDP25(self, properties, recommendedDefaults, configurations, services, hosts):
+    ranger_tagsync_properties = properties
+    validationItems = []
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+
+    has_atlas = False
+    if "RANGER" in servicesList:
+      has_atlas = not "ATLAS" in servicesList
+
+      if has_atlas and 'ranger.tagsync.source.atlas' in ranger_tagsync_properties and \
+              ranger_tagsync_properties['ranger.tagsync.source.atlas'].lower() == 'true':
+        validationItems.append({"config-name": "ranger.tagsync.source.atlas",
+                                "item": self.getWarnItem(
+                                  "Need to Install ATLAS service to set ranger.tagsync.source.atlas as true.")})
+
+    return self.toConfigurationValidationProblems(validationItems, "ranger-tagsync-site")
+
+  def validateRangerUsersyncConfigurationsFromHDP26(self, properties, recommendedDefaults, configurations, services, hosts):
+    ranger_usersync_properties = properties
+    validationItems = []
+
+    delta_sync_enabled = 'ranger.usersync.ldap.deltasync' in ranger_usersync_properties \
+                         and ranger_usersync_properties['ranger.usersync.ldap.deltasync'].lower() == 'true'
+    group_sync_enabled = 'ranger.usersync.group.searchenabled' in ranger_usersync_properties \
+                         and ranger_usersync_properties['ranger.usersync.group.searchenabled'].lower() == 'true'
+    usersync_source_ldap_enabled = 'ranger.usersync.source.impl.class' in ranger_usersync_properties \
+                                   and ranger_usersync_properties['ranger.usersync.source.impl.class'] == 'org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder'
+
+    if usersync_source_ldap_enabled and delta_sync_enabled and not group_sync_enabled:
+      validationItems.append({"config-name": "ranger.usersync.group.searchenabled",
+                              "item": self.getWarnItem(
+                                "Need to set ranger.usersync.group.searchenabled as true, as ranger.usersync.ldap.deltasync is enabled")})
+
+    return self.toConfigurationValidationProblems(validationItems, "ranger-ugsync-site")
+
+  def validateRangerPasswordConfigurations(self, properties, recommendedDefaults, configurations, services, hosts):
+    ranger_env_properties = properties
+    validationItems = []
+
+    ranger_password_properties = ['admin_password', 'ranger_admin_password', 'rangerusersync_user_password', 'rangertagsync_user_password', 'keyadmin_user_password']
+    for password_property in ranger_password_properties:
+      if password_property in ranger_env_properties:
+        password = ranger_env_properties[password_property]
+        if not bool(re.search(r'^(?=.*[0-9])(?=.*[a-zA-Z]).{10,}$', password)) or bool(re.search('[\\\`"\']', password)):
+          validationItems.append({"config-name": password_property, "item": self.getNotApplicableItem("Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are  \" ' \ `")})
+
+    return self.toConfigurationValidationProblems(validationItems, "ranger-env")
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/role_command_order.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/role_command_order.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/role_command_order.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/role_command_order.json	(date 1719626239000)
@@ -0,0 +1,9 @@
+{
+  "general_deps" : {
+    "_comment" : "dependencies for RANGER",
+    "RANGER_SERVICE_CHECK-SERVICE_CHECK" : ["RANGER_ADMIN-START"],
+    "RANGER_SERVICE_CHECK-SERVICE_CHECK" : ["RANGER_USERSYNC-START"],
+    "RANGER_USERSYNC-START" : ["RANGER_ADMIN-START"],
+    "RANGER_ADMIN-START": ["ZOOKEEPER_SERVER-START", "INFRA_SOLR-START"]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/database.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/database.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/database.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/database.json	(date 1719626239000)
@@ -0,0 +1,317 @@
+{
+  "name": "database",
+  "configuration": {
+    "placement": {
+      "configuration-layout": "database",
+      "configs": [
+        {
+          "config": "kms-properties/DB_FLAVOR",
+          "subsection-name": "subsection-kms-db-row1-col1"
+        },
+        {
+          "config": "kms-properties/db_name",
+          "subsection-name": "subsection-kms-db-row1-col1"
+        },
+        {
+          "config": "dbks-site/ranger.ks.jpa.jdbc.url",
+          "subsection-name": "subsection-kms-db-row1-col1"
+        },
+        {
+          "config": "kms-properties/db_user",
+          "subsection-name": "subsection-kms-db-row1-col1"
+        },
+        {
+          "config": "kms-properties/db_host",
+          "subsection-name": "subsection-kms-db-row1-col2"
+        },
+        {
+          "config": "kms-properties/SQL_CONNECTOR_JAR",
+          "subsection-name": "subsection-kms-db-row1-col2",
+          "depends-on" : [
+            {
+              "configs":[
+                "kms-properties/DB_FLAVOR"
+              ],
+              "if": "${kms-properties/DB_FLAVOR} === SQLA",
+              "then": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "dbks-site/ranger.ks.jpa.jdbc.driver",
+          "subsection-name": "subsection-kms-db-row1-col2"
+        },
+        {
+          "config": "kms-properties/db_password",
+          "subsection-name": "subsection-kms-db-row1-col2"
+        },
+        {
+          "config" : "kms-env/create_db_user",
+          "subsection-name": "subsection-kms-create-db-user-row2-col"
+        },
+        {
+          "config": "kms-env/test_db_kms_connection",
+          "subsection-name": "subsection-kms-create-db-user-row2-col",
+          "property_value_attributes": {
+            "ui_only_property": true
+          },
+          "depends-on": [
+            {
+              "configs":[
+                "kms-env/create_db_user"
+              ],
+              "if": "${kms-env/create_db_user}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "kms-properties/db_root_user",
+          "subsection-name": "subsection-kms-db-root-user-row3-col1"
+        },
+        {
+          "config": "kms-env/ranger_kms_privelege_user_jdbc_url",
+          "subsection-name": "subsection-kms-db-root-user-row3-col1"
+        },
+        {
+          "config": "kms-properties/db_root_password",
+          "subsection-name": "subsection-kms-db-root-user-row3-col2"
+        },
+        {
+          "config": "kms-env/test_root_db_kms_connection",
+          "subsection-name": "subsection-kms-db-root-user-row3-col1",
+          "property_value_attributes": {
+            "ui_only_property": true
+          }
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "kms-properties/DB_FLAVOR",
+        "widget": {
+          "type": "combo"
+        }
+      },
+      {
+        "config": "kms-properties/db_user",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "kms-properties/db_name",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "kms-properties/SQL_CONNECTOR_JAR",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "kms-properties/db_root_user",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "kms-properties/db_host",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "kms-properties/db_password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "kms-properties/db_root_password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "kms-env/create_db_user",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "kms-env/test_db_kms_connection",
+        "widget": {
+          "type": "test-db-connection",
+          "display-name": "Test Connection",
+          "required-properties": {
+            "jdbc.driver.class": "dbks-site/ranger.ks.jpa.jdbc.driver",
+            "jdbc.driver.url": "dbks-site/ranger.ks.jpa.jdbc.url",
+            "db.connection.source.host": "ranger_kms-site/ranger_kms_server_hosts",
+            "db.type": "kms-properties/DB_FLAVOR",
+            "db.connection.destination.host": "kms-properties/db_host",
+            "db.connection.user": "kms-properties/db_user",
+            "db.connection.password": "kms-properties/db_password"
+          }
+        }
+      },
+      {
+        "config": "kms-env/test_root_db_kms_connection",
+        "widget": {
+          "type": "test-db-connection",
+          "display-name": "Test Connection",
+          "required-properties": {
+            "jdbc.driver.class": "dbks-site/ranger.ks.jpa.jdbc.driver",
+            "jdbc.driver.url": "kms-env/ranger_kms_privelege_user_jdbc_url",
+            "db.connection.source.host": "ranger_kms-site/ranger_kms_server_hosts",
+            "db.type": "kms-properties/DB_FLAVOR",
+            "db.connection.destination.host": "kms-properties/db_host",
+            "db.connection.user": "kms-properties/db_root_user",
+            "db.connection.password": "kms-properties/db_root_password"
+          }
+        }
+      },
+      {
+        "config": "kms-env/ranger_kms_privelege_user_jdbc_url",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.ks.jpa.jdbc.driver",
+        "widget" : {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.ks.jpa.jdbc.url",
+        "widget": {
+          "type": "text-field"
+        }
+      }
+    ],
+    "layouts": [
+      {
+        "name": "database",
+        "tabs": [
+          {
+            "name": "ranger_kms_database",
+            "display-name": "Ranger KMS",
+            "layout": {
+              "tab-columns": "2",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "section-kms-db-settings",
+                  "removed" : false,
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "3",
+                  "column-span": "2",
+                  "section-columns": "2",
+                  "section-rows": "3",
+                  "subsections": [
+                    {
+                      "name": "subsection-kms-db-row1-col1",
+                      "display-name": "Ranger KMS DB",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-kms-db-row1-col2",
+                      "row-index": "0",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-kms-create-db-user-row2-col",
+                      "display-name": "Setup Database and Database User",
+                      "row-index": "1",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "2"
+                    },
+                    {
+                      "name": "subsection-kms-db-root-user-row3-col1",
+                      "display-name": "Ranger KMS Root DB",
+                      "row-index": "2",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "depends-on": [
+                        {
+                          "configs":[
+                            "kms-env/create_db_user"
+                          ],
+                          "if": "${kms-env/create_db_user}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    },
+                    {
+                      "name": "subsection-kms-db-root-user-row3-col2",
+                      "row-index": "2",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "depends-on": [
+                        {
+                          "configs":[
+                            "kms-env/create_db_user"
+                          ],
+                          "if": "${kms-env/create_db_user}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/credentials.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/credentials.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/credentials.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/credentials.json	(date 1719626239000)
@@ -0,0 +1,51 @@
+{
+  "name": "credentials",
+  "configuration": {
+    "layouts": [
+      {
+        "name": "credentials",
+        "tabs": [
+          {
+            "name": "credentials",
+            "layout": {
+              "sections": [
+                {
+                  "name": "credentials",
+                  "subsections": [
+                    {
+                      "name" : "subsection-ranger-kms-credential",
+                      "display-name": "Ranger KMS master key password"
+                    },
+                    {
+                      "name" : "subsection-ranger-kms-db-credential",
+                      "display-name": "Ranger KMS DB Credentials"
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ],
+    "placement": {
+      "configuration-layout": "credentials",
+      "configs": [
+        {
+          "config": "kms-properties/KMS_MASTER_KEY_PASSWD",
+          "subsection-name": "subsection-ranger-kms-credential"
+        },
+        {
+          "config": "kms-properties/db_user",
+          "subsection-name": "subsection-ranger-kms-db-credential"
+        },
+        {
+          "config": "kms-properties/db_password",
+          "subsection-name": "subsection-ranger-kms-db-credential"
+        }
+      ]
+    },
+    "widgets": [
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/directories.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/directories.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/directories.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/directories.json	(date 1719626239000)
@@ -0,0 +1,88 @@
+{
+  "name": "directories",
+  "description": "Directories theme for RANGER KMS service",
+  "configuration": {
+    "layouts": [
+      {
+        "name": "directories",
+        "tabs": [
+          {
+            "name": "directories",
+            "display-name": "Directories",
+            "layout": {
+              "tab-columns": "1",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "subsection-log-dirs",
+                  "display-name": "LOG DIR",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-log-dirs",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                },
+                {
+                  "name": "subsection-pid-dirs",
+                  "display-name": "PID DIR",
+                  "row-index": "2",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-pid-dirs",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ],
+    "placement": {
+      "configuration-layout": "directories",
+      "configs": [
+        {
+          "config": "kms-env/kms_log_dir",
+          "subsection-name": "subsection-log-dirs"
+        },
+        {
+          "config": "kms-env/ranger_kms_pid_dir",
+          "subsection-name": "subsection-pid-dirs"
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "kms-env/kms_log_dir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "kms-env/ranger_kms_pid_dir",
+        "widget": {
+          "type": "text-field"
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/theme_version_2.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/theme_version_2.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/theme_version_2.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/themes/theme_version_2.json	(date 1719626239000)
@@ -0,0 +1,156 @@
+{
+  "name": "default",
+  "configuration": {
+    "layouts": [{
+      "name": "default",
+      "tabs": [{
+        "name": "kms_keysecure",
+        "display-name": "KMS KEYSECURE",
+        "layout": {
+          "tab-columns": "1",
+          "tab-rows": "1",
+          "sections": [{
+            "name": "section-kms-keysecure",
+            "display-name": "",
+            "row-index": "0",
+            "column-index": "0",
+            "row-span": "1",
+            "column-span": "1",
+            "section-columns": "1",
+            "section-rows": "2",
+            "subsections": [{
+                "name": "subsection-kms-keysecure-row1-col1",
+                "display-name": "Ranger KMS Keysecure Enabled",
+                "row-index": "0",
+                "column-index": "0",
+                "row-span": "1",
+                "column-span": "1"
+              },
+              {
+                "name": "subsection-kms-keysecure-row2-col1",
+                "display-name": "Configuration Settings",
+                "row-index": "1",
+                "column-index": "0",
+                "row-span": "1",
+                "column-span": "1",
+                "depends-on": [{
+                  "configs": [
+                    "dbks-site/ranger.kms.keysecure.enabled"
+                  ],
+                  "if": "${dbks-site/ranger.kms.keysecure.enabled}",
+                  "then": {
+                    "property_value_attributes": {
+                      "visible": true
+                    }
+                  },
+                  "else": {
+                    "property_value_attributes": {
+                      "visible": false
+                    }
+                  }
+                }]
+              }
+            ]
+          }]
+        }
+      }]
+    }],
+    "placement": {
+      "configuration-layout": "default",
+      "configs": [
+        {
+          "config": "dbks-site/ranger.kms.keysecure.enabled",
+          "subsection-name": "subsection-kms-keysecure-row1-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.UserPassword.Authentication",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.masterkey.name",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.login.username",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.login.password",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.login.password.alias",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.hostname",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.masterkey.size",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        },
+        {
+          "config": "dbks-site/ranger.kms.keysecure.sunpkcs11.cfg.filepath",
+          "subsection-name": "subsection-kms-keysecure-row2-col1"
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "dbks-site/ranger.kms.keysecure.enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.UserPassword.Authentication",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.masterkey.name",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.login.username",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.login.password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.login.password.alias",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.hostname",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.masterkey.size",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "dbks-site/ranger.kms.keysecure.sunpkcs11.cfg.filepath",
+        "widget": {
+          "type": "text-field"
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms.py	(date 1719626239000)
@@ -0,0 +1,716 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import sys
+import fileinput
+import os
+import ambari_simplejson as json # simplejson is much faster comparing to Python 2.6 json module and has the same functions set.
+import urllib2, base64, httplib
+from StringIO import StringIO as BytesIO
+from datetime import datetime
+from resource_management.core.resources.system import File, Directory, Execute
+from resource_management.libraries.resources.xml_config import XmlConfig
+from resource_management.libraries.resources.modify_properties_file import ModifyPropertiesFile
+from resource_management.core.source import DownloadSource, Template, InlineTemplate
+from resource_management.core.exceptions import Fail
+from resource_management.core.logger import Logger
+from resource_management.libraries.functions.is_empty import is_empty
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.generate_logfeeder_input_config import generate_logfeeder_input_config
+from resource_management.libraries.functions.ranger_functions import Rangeradmin
+from resource_management.libraries.functions.ranger_functions_v2 import RangeradminV2
+from resource_management.libraries.functions.decorator import safe_retry
+from resource_management.core.utils import PasswordString
+from resource_management.core.shell import as_sudo
+import re
+import time
+import socket
+
+def password_validation(password, key):
+  import params
+  if password.strip() == "":
+    raise Fail("Blank password is not allowed for {0} property. Please enter valid password.".format(key))
+  if re.search("[\\\`'\"]",password):
+    raise Fail("{0} password contains one of the unsupported special characters like \" ' \ `".format(key))
+  else:
+    Logger.info("Password validated")
+
+def setup_kms_db(stack_version=None):
+  import params
+
+  if params.has_ranger_admin:
+
+    kms_home = params.kms_home
+
+    if stack_version is not None:
+      kms_home = format("{stack_root}/{stack_version}/ranger-kms")
+
+    password_validation(params.kms_master_key_password, 'KMS master key')
+
+    copy_jdbc_connector(kms_home)
+
+    env_dict = {'RANGER_KMS_HOME':kms_home, 'JAVA_HOME': params.java_home}
+    if params.db_flavor.lower() == 'sqla':
+      env_dict = {'RANGER_KMS_HOME':kms_home, 'JAVA_HOME': params.java_home, 'LD_LIBRARY_PATH':params.ld_library_path}
+
+    dba_setup = format('ambari-python-wrap {kms_home}/dba_script.py -q')
+    db_setup = format('ambari-python-wrap {kms_home}/db_setup.py')
+
+    if params.create_db_user:
+      Logger.info('Setting up Ranger KMS DB and DB User')
+      Execute(dba_setup, environment=env_dict, logoutput=True, user=params.kms_user, tries=5, try_sleep=10)
+    else:
+      Logger.info('Separate DBA property not set. Assuming Ranger KMS DB and DB User exists!')
+    Execute(db_setup, environment=env_dict, logoutput=True, user=params.kms_user, tries=5, try_sleep=10)
+
+def setup_java_patch():
+  import params
+
+  if params.has_ranger_admin:
+
+    kms_home = params.kms_home
+    setup_java_patch = format('ambari-python-wrap {kms_home}/db_setup.py -javapatch')
+
+    env_dict = {'RANGER_KMS_HOME':kms_home, 'JAVA_HOME': params.java_home}
+    if params.db_flavor.lower() == 'sqla':
+      env_dict = {'RANGER_KMS_HOME':kms_home, 'JAVA_HOME': params.java_home, 'LD_LIBRARY_PATH':params.ld_library_path}
+
+    Execute(setup_java_patch, environment=env_dict, logoutput=True, user=params.kms_user, tries=5, try_sleep=10)
+
+    kms_lib_path = format('{kms_home}/ews/webapp/lib/')
+    files = os.listdir(kms_lib_path)
+    hadoop_jar_files = []
+
+    for x in files:
+      if x.startswith('hadoop-common') and x.endswith('.jar'):
+        hadoop_jar_files.append(x)
+
+    if len(hadoop_jar_files) != 0:
+      for f in hadoop_jar_files:
+        Execute((format('{java_home}/bin/jar'),'-uf', format('{kms_home}/ews/webapp/lib/{f}'), format('{kms_home}/ews/webapp/META-INF/services/org.apache.hadoop.crypto.key.KeyProviderFactory')),
+          user=params.kms_user)
+
+        File(format('{kms_home}/ews/webapp/lib/{f}'), owner=params.kms_user, group=params.kms_group)
+
+def do_keystore_setup(cred_provider_path, credential_alias, credential_password):
+  import params
+
+  if cred_provider_path is not None:
+    java_bin = format('{java_home}/bin/java')
+    file_path = format('jceks://file{cred_provider_path}')
+    cmd = (java_bin, '-cp', params.cred_lib_path, 'org.apache.ranger.credentialapi.buildks', 'create', credential_alias, '-value', PasswordString(credential_password), '-provider', file_path)
+    Execute(cmd,
+            environment={'JAVA_HOME': params.java_home},
+            logoutput=True,
+            sudo=True,
+    )
+
+    File(cred_provider_path,
+      owner = params.kms_user,
+      group = params.kms_group,
+      only_if = format("test -e {cred_provider_path}"),
+      mode = 0640
+    )
+
+    dot_jceks_crc_file_path = os.path.join(os.path.dirname(cred_provider_path), "." + os.path.basename(cred_provider_path) + ".crc")
+
+    File(dot_jceks_crc_file_path,
+      owner = params.kms_user,
+      group = params.kms_group,
+      only_if = format("test -e {dot_jceks_crc_file_path}"),
+      mode = 0640
+    )
+
+def kms(upgrade_type=None):
+  import params
+
+  if params.has_ranger_admin:
+
+    #ranger2.3.0
+    Directory(format('{kms_home}/ews/webapp/META-INF/services/org.apache.hadoop.crypto.key.KeyProviderFactory'),
+              mode = 0755,
+              owner = params.kms_user,
+              group = params.kms_group,
+              recursive_ownership = True,
+              create_parents = True
+    )
+    
+    Directory(params.kms_conf_dir,
+      owner = params.kms_user,
+      group = params.kms_group,
+      create_parents = True
+    )
+
+    Directory("/etc/security/serverKeys",
+      create_parents = True,
+      cd_access = "a"
+    )
+
+    Directory("/etc/ranger/kms",
+      create_parents = True,
+      cd_access = "a"
+    )
+
+    copy_jdbc_connector(params.kms_home)
+
+    File(format("/usr/lib/ambari-agent/{check_db_connection_jar_name}"),
+      content = DownloadSource(format("{jdk_location}/{check_db_connection_jar_name}")),
+      mode = 0644,
+    )
+
+    cp = format("{check_db_connection_jar}")
+    if params.db_flavor.lower() == 'sqla':
+      cp = cp + os.pathsep + format("{kms_home}/ews/webapp/lib/sajdbc4.jar")
+    else:
+      path_to_jdbc = format("{kms_home}/ews/webapp/lib/{jdbc_jar_name}")
+      if not os.path.isfile(path_to_jdbc):
+        path_to_jdbc = format("{kms_home}/ews/webapp/lib/") + \
+                       params.default_connectors_map[params.db_flavor.lower()] if params.db_flavor.lower() in params.default_connectors_map else None
+        if not os.path.isfile(path_to_jdbc):
+          path_to_jdbc = format("{kms_home}/ews/webapp/lib/") + "*"
+          error_message = "Error! Sorry, but we can't find jdbc driver with default name " + params.default_connectors_map[params.db_flavor] + \
+                " in ranger kms lib dir. So, db connection check can fail. Please run 'ambari-server setup --jdbc-db={db_name} --jdbc-driver={path_to_jdbc} on server host.'"
+          Logger.error(error_message)
+
+      cp = cp + os.pathsep + path_to_jdbc
+
+    db_connection_check_command = format(
+      "{java_home}/bin/java -cp {cp} org.apache.ambari.server.DBConnectionVerification '{ranger_kms_jdbc_connection_url}' {db_user} {db_password!p} {ranger_kms_jdbc_driver}")
+
+    env_dict = {}
+    if params.db_flavor.lower() == 'sqla':
+      env_dict = {'LD_LIBRARY_PATH':params.ld_library_path}
+
+    Execute(db_connection_check_command, path='/usr/sbin:/sbin:/usr/local/bin:/bin:/usr/bin', tries=5, try_sleep=10, environment=env_dict)
+
+    if params.xa_audit_db_is_enabled and params.driver_source is not None and not params.driver_source.endswith("/None"):
+      if params.xa_previous_jdbc_jar and os.path.isfile(params.xa_previous_jdbc_jar):
+        File(params.xa_previous_jdbc_jar, action='delete')
+
+      File(params.downloaded_connector_path,
+        content = DownloadSource(params.driver_source),
+        mode = 0644
+      )
+
+      Execute(('cp', '--remove-destination', params.downloaded_connector_path, params.driver_target),
+          path=["/bin", "/usr/bin/"],
+          sudo=True)
+
+      File(params.driver_target, mode=0644)
+
+    Directory(os.path.join(params.kms_home, 'ews', 'webapp', 'WEB-INF', 'classes', 'lib'),
+        mode=0755,
+        owner=params.kms_user,
+        group=params.kms_group
+      )
+
+    Execute(('cp',format('{kms_home}/ranger-kms-initd'),'/etc/init.d/ranger-kms'),
+    not_if=format('ls /etc/init.d/ranger-kms'),
+    only_if=format('ls {kms_home}/ranger-kms-initd'),
+    sudo=True)
+
+    File('/etc/init.d/ranger-kms',
+      mode = 0755
+    )
+
+    Directory(format('{kms_home}/'),
+              owner = params.kms_user,
+              group = params.kms_group,
+              recursive_ownership = True,
+    )
+
+    Directory(params.ranger_kms_pid_dir,
+      mode = 0755,
+      owner = params.kms_user,
+      group = params.user_group,
+      cd_access = "a",
+      create_parents = True
+    )
+
+    Directory(params.kms_log_dir,
+      owner = params.kms_user,
+      group = params.kms_group,
+      cd_access = 'a',
+      create_parents = True,
+      mode = 0755
+    )
+
+    generate_logfeeder_input_config('ranger-kms', Template("input.config-ranger-kms.json.j2", extra_imports=[default]))
+
+    File(format('{kms_conf_dir}/ranger-kms-env.sh'),
+      content = InlineTemplate(params.kms_env_content),
+      owner = params.kms_user,
+      group = params.kms_group,
+      mode = 0755
+    )
+
+    Execute(('ln','-sf', format('{kms_home}/ranger-kms'),'/usr/bin/ranger-kms'),
+      not_if=format('ls /usr/bin/ranger-kms'),
+      only_if=format('ls {kms_home}/ranger-kms'),
+      sudo=True)
+
+    File('/usr/bin/ranger-kms', mode = 0755)
+
+    Execute(('ln','-sf', format('{kms_home}/ranger-kms'),'/usr/bin/ranger-kms-services.sh'),
+      not_if=format('ls /usr/bin/ranger-kms-services.sh'),
+      only_if=format('ls {kms_home}/ranger-kms'),
+      sudo=True)
+
+    File('/usr/bin/ranger-kms-services.sh', mode = 0755)
+
+    Execute(('ln','-sf', format('{kms_home}/ranger-kms-initd'),format('{kms_home}/ranger-kms-services.sh')),
+      not_if=format('ls {kms_home}/ranger-kms-services.sh'),
+      only_if=format('ls {kms_home}/ranger-kms-initd'),
+      sudo=True)
+
+    File(format('{kms_home}/ranger-kms-services.sh'), mode = 0755)
+
+    do_keystore_setup(params.credential_provider_path, params.jdbc_alias, params.db_password)
+    do_keystore_setup(params.credential_provider_path, params.masterkey_alias, params.kms_master_key_password)
+    if params.stack_support_kms_hsm and params.enable_kms_hsm:
+      do_keystore_setup(params.credential_provider_path, params.hms_partition_alias, unicode(params.hms_partition_passwd))
+    if params.stack_supports_ranger_kms_ssl and params.ranger_kms_ssl_enabled:
+      do_keystore_setup(params.ranger_kms_cred_ssl_path, params.ranger_kms_ssl_keystore_alias, params.ranger_kms_ssl_passwd)
+    if params.enable_kms_keysecure and not is_empty(params.keysecure_login_password) and params.keysecure_login_password != "_":
+      do_keystore_setup(params.credential_provider_path, params.keysecure_login_password_alias, params.keysecure_login_password)
+
+    # remove plain-text password from xml configs
+    dbks_site_copy = {}
+    dbks_site_copy.update(params.config['configurations']['dbks-site'])
+
+    for prop in params.dbks_site_password_properties:
+      if prop in dbks_site_copy:
+        dbks_site_copy[prop] = "_"
+
+    XmlConfig("dbks-site.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=dbks_site_copy,
+      configuration_attributes=params.config['configurationAttributes']['dbks-site'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0644
+    )
+
+    ranger_kms_site_copy = {}
+    ranger_kms_site_copy.update(params.config['configurations']['ranger-kms-site'])
+    if params.stack_supports_ranger_kms_ssl:
+      # remove plain-text password from xml configs
+      for prop in params.ranger_kms_site_password_properties:
+        if prop in ranger_kms_site_copy:
+          ranger_kms_site_copy[prop] = "_"
+
+    XmlConfig("ranger-kms-site.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=ranger_kms_site_copy,
+      configuration_attributes=params.config['configurationAttributes']['ranger-kms-site'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0644
+    )
+
+    kms_site_copy = {}
+    kms_site_copy.update(params.config['configurations']['kms-site'])
+
+    if 'hadoop.kms.ha.authentication.kerberos.keytab' in kms_site_copy:
+      kms_site_copy['hadoop.kms.authentication.kerberos.keytab'] = kms_site_copy['hadoop.kms.ha.authentication.kerberos.keytab']
+
+    XmlConfig("kms-site.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=kms_site_copy,
+      configuration_attributes=params.config['configurationAttributes']['kms-site'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0644
+    )
+
+    File(os.path.join(params.kms_conf_dir, "kms-log4j.properties"),
+      owner=params.kms_user,
+      group=params.kms_group,
+      content=InlineTemplate(params.kms_log4j),
+      mode=0644
+    )
+
+    # core-site.xml linking required by setup for HDFS encryption
+    XmlConfig("core-site.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=params.config['configurations']['core-site'],
+      configuration_attributes=params.config['configurationAttributes']['core-site'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0644,
+      xml_include_file=params.mount_table_xml_inclusion_file_full_path
+    )
+
+    if params.mount_table_content:
+      File(params.mount_table_xml_inclusion_file_full_path,
+        owner=params.kms_user,
+        group=params.kms_group,
+        content=params.mount_table_content,
+        mode=0644
+      )
+
+def copy_jdbc_connector(kms_home):
+  import params
+
+  if params.jdbc_jar_name is None and params.driver_curl_source.endswith("/None"):
+    error_message = "Error! Sorry, but we can't find jdbc driver related to {0} database to download from {1}. \
+    Please run 'ambari-server setup --jdbc-db={db_name} --jdbc-driver={path_to_jdbc} on server host.'".format(params.db_flavor, params.jdk_location)
+    Logger.error(error_message)
+
+  if params.driver_curl_source and not params.driver_curl_source.endswith("/None"):
+    if params.previous_jdbc_jar and os.path.isfile(params.previous_jdbc_jar):
+      File(params.previous_jdbc_jar, action='delete')
+
+  driver_curl_target = format("{kms_home}/ews/webapp/lib/{jdbc_jar_name}")
+
+  File(params.downloaded_custom_connector,
+    content = DownloadSource(params.driver_curl_source),
+    mode = 0644
+  )
+
+  Directory(os.path.join(kms_home, 'ews', 'lib'),
+    mode=0755
+  )
+
+  if params.db_flavor.lower() == 'sqla':
+    Execute(('tar', '-xvf', params.downloaded_custom_connector, '-C', params.tmp_dir), sudo = True)
+
+    Execute(('cp', '--remove-destination', params.jar_path_in_archive, os.path.join(kms_home, 'ews', 'webapp', 'lib')),
+      path=["/bin", "/usr/bin/"],
+      sudo=True)
+
+    Directory(params.jdbc_libs_dir,
+      cd_access="a",
+      create_parents=True)
+
+    Execute(as_sudo(['yes', '|', 'cp', params.libs_path_in_archive, params.jdbc_libs_dir], auto_escape=False),
+      path=["/bin", "/usr/bin/"])
+
+    File(os.path.join(kms_home, 'ews', 'webapp', 'lib', 'sajdbc4.jar'), mode=0644)
+  else:
+    Execute(('cp', '--remove-destination', params.downloaded_custom_connector, os.path.join(kms_home, 'ews', 'webapp', 'lib/')),
+      path=["/bin", "/usr/bin/"],
+      sudo=True)
+    filePath = os.path.join(kms_home, 'ews', 'webapp', 'lib', params.jdbc_jar_name)
+    Logger.info("========= filePath is =====".format(filePath))
+    File(filePath, mode=0644)
+
+  ModifyPropertiesFile(format("{kms_home}/install.properties"),
+    properties = params.config['configurations']['kms-properties'],
+    owner = params.kms_user
+  )
+
+  if params.db_flavor.lower() == 'sqla':
+    ModifyPropertiesFile(format("{kms_home}/install.properties"),
+      properties = {'SQL_CONNECTOR_JAR': format('{kms_home}/ews/webapp/lib/sajdbc4.jar')},
+      owner = params.kms_user,
+    )
+  else:
+    ModifyPropertiesFile(format("{kms_home}/install.properties"),
+      properties = {'SQL_CONNECTOR_JAR': format('{driver_curl_target}')},
+      owner = params.kms_user,
+    )
+
+def enable_kms_plugin():
+
+  import params
+
+  if params.has_ranger_admin:
+
+    ranger_flag = False
+
+    if params.stack_supports_ranger_kerberos and params.security_enabled:
+      if not is_empty(params.rangerkms_principal) and params.rangerkms_principal != '':
+        ranger_flag = check_ranger_service_support_kerberos(params.kms_user, params.rangerkms_keytab, params.rangerkms_principal)
+      else:
+        ranger_flag = check_ranger_service_support_kerberos(params.kms_user, params.spengo_keytab, params.spnego_principal)
+    else:
+      ranger_flag = check_ranger_service()
+
+    if not ranger_flag:
+      Logger.error('Error in Get/Create service for Ranger Kms.')
+
+    current_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
+
+    File(format('{kms_conf_dir}/ranger-security.xml'),
+      owner = params.kms_user,
+      group = params.kms_group,
+      mode = 0644,
+      content = format('<ranger>\n<enabled>{current_datetime}</enabled>\n</ranger>')
+    )
+
+    Directory([os.path.join('/etc', 'ranger', params.repo_name), os.path.join('/etc', 'ranger', params.repo_name, 'policycache')],
+      owner = params.kms_user,
+      group = params.kms_group,
+      mode=0775,
+      create_parents = True
+    )
+
+    File(os.path.join('/etc', 'ranger', params.repo_name, 'policycache',format('kms_{repo_name}.json')),
+      owner = params.kms_user,
+      group = params.kms_group,
+      mode = 0644
+    )
+
+    # remove plain-text password from xml configs
+    plugin_audit_properties_copy = {}
+    plugin_audit_properties_copy.update(params.config['configurations']['ranger-kms-audit'])
+
+    if params.plugin_audit_password_property in plugin_audit_properties_copy:
+      plugin_audit_properties_copy[params.plugin_audit_password_property] = "crypted"
+
+    XmlConfig("ranger-kms-audit.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=plugin_audit_properties_copy,
+      configuration_attributes=params.config['configurationAttributes']['ranger-kms-audit'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0744)
+
+    XmlConfig("ranger-kms-security.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=params.config['configurations']['ranger-kms-security'],
+      configuration_attributes=params.config['configurationAttributes']['ranger-kms-security'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0744)
+
+    # remove plain-text password from xml configs
+    ranger_kms_policymgr_ssl_copy = {}
+    ranger_kms_policymgr_ssl_copy.update(params.config['configurations']['ranger-kms-policymgr-ssl'])
+
+    for prop in params.kms_plugin_password_properties:
+      if prop in ranger_kms_policymgr_ssl_copy:
+        ranger_kms_policymgr_ssl_copy[prop] = "crypted"
+
+    XmlConfig("ranger-policymgr-ssl.xml",
+      conf_dir=params.kms_conf_dir,
+      configurations=ranger_kms_policymgr_ssl_copy,
+      configuration_attributes=params.config['configurationAttributes']['ranger-kms-policymgr-ssl'],
+      owner=params.kms_user,
+      group=params.kms_group,
+      mode=0744)
+
+    if params.xa_audit_db_is_enabled:
+      cred_setup = params.cred_setup_prefix + ('-f', params.credential_file, '-k', 'auditDBCred', '-v', PasswordString(params.xa_audit_db_password), '-c', '1')
+      Execute(cred_setup, environment={'JAVA_HOME': params.java_home}, logoutput=True, sudo=True)
+
+    cred_setup = params.cred_setup_prefix + ('-f', params.credential_file, '-k', 'sslKeyStore', '-v', PasswordString(params.ssl_keystore_password), '-c', '1')
+    Execute(cred_setup, environment={'JAVA_HOME': params.java_home}, logoutput=True, sudo=True)
+
+    cred_setup = params.cred_setup_prefix + ('-f', params.credential_file, '-k', 'sslTrustStore', '-v', PasswordString(params.ssl_truststore_password), '-c', '1')
+    Execute(cred_setup, environment={'JAVA_HOME': params.java_home}, logoutput=True, sudo=True)
+
+    File(params.credential_file,
+      owner = params.kms_user,
+      group = params.kms_group,
+      only_if = format("test -e {credential_file}"),
+      mode = 0640
+    )
+
+    dot_jceks_crc_file_path = os.path.join(os.path.dirname(params.credential_file), "." + os.path.basename(params.credential_file) + ".crc")
+
+    File(dot_jceks_crc_file_path,
+      owner = params.kms_user,
+      group = params.kms_group,
+      only_if = format("test -e {dot_jceks_crc_file_path}"),
+      mode = 0640
+    )
+
+    # create ranger kms audit directory
+    if params.xa_audit_hdfs_is_enabled and params.has_namenode and params.has_hdfs_client_on_node:
+      try:
+        params.HdfsResource("/ranger/audit",
+                          type="directory",
+                          action="create_on_execute",
+                          owner=params.hdfs_user,
+                          group=params.hdfs_user,
+                          mode=0755,
+                          recursive_chmod=True
+        )
+        params.HdfsResource("/ranger/audit/kms",
+                          type="directory",
+                          action="create_on_execute",
+                          owner=params.kms_user,
+                          group=params.kms_group,
+                          mode=0750,
+                          recursive_chmod=True
+        )
+        params.HdfsResource(None, action="execute")
+      except Exception, err:
+        Logger.exception("Audit directory creation in HDFS for RANGER KMS Ranger plugin failed with error:\n{0}".format(err))
+
+    if params.xa_audit_hdfs_is_enabled and len(params.namenode_host) > 1:
+      Logger.info('Audit to Hdfs enabled in NameNode HA environment, creating hdfs-site.xml')
+      XmlConfig("hdfs-site.xml",
+        conf_dir=params.kms_conf_dir,
+        configurations=params.config['configurations']['hdfs-site'],
+        configuration_attributes=params.config['configurationAttributes']['hdfs-site'],
+        owner=params.kms_user,
+        group=params.kms_group,
+        mode=0644
+      )
+    else:
+      File(format('{kms_conf_dir}/hdfs-site.xml'), action="delete")
+
+def setup_kms_jce():
+  import params
+
+  if params.jce_name is not None:
+    Directory(params.jce_source_dir,
+      create_parents = True
+    )
+
+    jce_target = format('{jce_source_dir}/{jce_name}')
+
+    File(jce_target,
+      content = DownloadSource(format('{jdk_location}/{jce_name}')),
+      mode = 0644,
+    )
+
+    File([format("{java_home}/jre/lib/security/local_policy.jar"), format("{java_home}/jre/lib/security/US_export_policy.jar")],
+      action = "delete",
+    )
+
+    unzip_cmd = ("unzip", "-o", "-j", "-q", jce_target, "-d", format("{java_home}/jre/lib/security"))
+
+    Execute(unzip_cmd,
+      only_if = format("test -e {java_home}/jre/lib/security && test -f {jce_target}"),
+      path = ['/bin/','/usr/bin'],
+      sudo = True
+    )
+  else:
+    Logger.warning("Required jce policy zip is not available, need to setup manually")
+
+def check_ranger_service():
+  import params
+
+  policymgr_mgr_url = params.policymgr_mgr_url
+  if policymgr_mgr_url.endswith('/'):
+    policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
+  ranger_adm_obj = Rangeradmin(url=policymgr_mgr_url)
+  ambari_username_password_for_ranger = format("{ambari_ranger_admin}:{ambari_ranger_password}")
+  response_code = ranger_adm_obj.check_ranger_login_urllib2(policymgr_mgr_url)
+
+  if response_code is not None and response_code == 200:
+    user_resp_code = ranger_adm_obj.create_ambari_admin_user(params.ambari_ranger_admin, params.ambari_ranger_password, params.admin_uname_password)
+    if user_resp_code is not None and user_resp_code == 200:
+      get_repo_flag = get_repo(policymgr_mgr_url, params.repo_name, ambari_username_password_for_ranger)
+      if not get_repo_flag:
+        return create_repo(policymgr_mgr_url, json.dumps(params.kms_ranger_plugin_repo), ambari_username_password_for_ranger)
+      else:
+        return True
+    else:
+      return False
+  else:
+    Logger.error('Ranger service is not reachable')
+    return False
+
+@safe_retry(times=5, sleep_time=8, backoff_factor=1.5, err_class=Fail, return_on_fail=False)
+def create_repo(url, data, usernamepassword):
+  try:
+    base_url = url + '/service/public/v2/api/service'
+    base64string = base64.encodestring('{0}'.format(usernamepassword)).replace('\n', '')
+    headers = {
+      'Accept': 'application/json',
+      "Content-Type": "application/json"
+    }
+    request = urllib2.Request(base_url, data, headers)
+    request.add_header("Authorization", "Basic {0}".format(base64string))
+    result = urllib2.urlopen(request, timeout=20)
+    response_code = result.getcode()
+    response = json.loads(json.JSONEncoder().encode(result.read()))
+    if response_code == 200:
+      Logger.info('Repository created Successfully')
+      return True
+    else:
+      Logger.info('Repository not created')
+      return False
+  except urllib2.URLError, e:
+    if isinstance(e, urllib2.HTTPError):
+      raise Fail("Error creating service. Http status code - {0}. \n {1}".format(e.code, e.read()))
+    else:
+      raise Fail("Error creating service. Reason - {0}.".format(e.reason))
+  except socket.timeout as e:
+    raise Fail("Error creating service. Reason - {0}".format(e))
+
+@safe_retry(times=5, sleep_time=8, backoff_factor=1.5, err_class=Fail, return_on_fail=False)
+def get_repo(url, name, usernamepassword):
+  try:
+    base_url = url + '/service/public/v2/api/service?serviceName=' + name + '&serviceType=kms&isEnabled=true'
+    request = urllib2.Request(base_url)
+    base64string = base64.encodestring(usernamepassword).replace('\n', '')
+    request.add_header("Content-Type", "application/json")
+    request.add_header("Accept", "application/json")
+    request.add_header("Authorization", "Basic {0}".format(base64string))
+    result = urllib2.urlopen(request, timeout=20)
+    response_code = result.getcode()
+    response = json.loads(result.read())
+    if response_code == 200 and len(response) > 0:
+      for repo in response:
+        if repo.get('name').lower() == name.lower() and repo.has_key('name'):
+          Logger.info('KMS repository exist')
+          return True
+        else:
+          Logger.info('KMS repository doesnot exist')
+          return False
+    else:
+      Logger.info('KMS repository doesnot exist')
+      return False
+  except urllib2.URLError, e:
+    if isinstance(e, urllib2.HTTPError):
+      raise Fail("Error getting {0} service. Http status code - {1}. \n {2}".format(name, e.code, e.read()))
+    else:
+      raise Fail("Error getting {0} service. Reason - {1}.".format(name, e.reason))
+  except socket.timeout as e:
+    raise Fail("Error creating service. Reason - {0}".format(e))
+
+def check_ranger_service_support_kerberos(user, keytab, principal):
+  import params
+
+  policymgr_mgr_url = params.policymgr_mgr_url
+  if policymgr_mgr_url.endswith('/'):
+    policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
+  ranger_adm_obj = RangeradminV2(url=policymgr_mgr_url)
+  response_code = ranger_adm_obj.check_ranger_login_curl(user, keytab, principal, policymgr_mgr_url, True)
+
+  if response_code is not None and response_code[0] == 200:
+    get_repo_name_response = ranger_adm_obj.get_repository_by_name_curl(user, keytab, principal, params.repo_name, 'kms', 'true', is_keyadmin = True)
+    if get_repo_name_response is not None:
+      Logger.info('KMS repository {0} exist'.format(get_repo_name_response['name']))
+      return True
+    else:
+      create_repo_response = ranger_adm_obj.create_repository_curl(user, keytab, principal, params.repo_name, json.dumps(params.kms_ranger_plugin_repo), None, is_keyadmin = True)
+      if create_repo_response is not None and len(create_repo_response) > 0:
+        return True
+      else:
+        return False
+  else:
+    Logger.error('Ranger service is not reachable')
+    return False
+
+def update_password_configs():
+  import params
+
+  ModifyPropertiesFile(format("{kms_home}/install.properties"),
+    properties = {'db_root_password': '_', 'db_password': '_', 'KMS_MASTER_KEY_PASSWD': '_', 'REPOSITORY_CONFIG_PASSWORD': '_'},
+    owner = params.kms_user,
+  )
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/params.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/params.py	(date 1719626239000)
@@ -0,0 +1,388 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+import os
+from resource_management.libraries.functions import conf_select
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions.version import format_stack_version
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions.stack_features import get_stack_feature_version
+from resource_management.libraries.functions import StackFeature
+from resource_management.libraries.functions.get_bare_principal import get_bare_principal
+from resource_management.libraries.functions.is_empty import is_empty
+from resource_management.libraries.functions.setup_ranger_plugin_xml import generate_ranger_service_config
+from resource_management.libraries.resources.hdfs_resource import HdfsResource
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions import get_kinit_path
+from resource_management.core.exceptions import Fail
+
+config = Script.get_config()
+tmp_dir = Script.get_tmp_dir()
+stack_root = Script.get_stack_root()
+
+stack_name = default("/clusterLevelParams/stack_name", None)
+version = default("/commandParams/version", None)
+upgrade_direction = default("/commandParams/upgrade_direction", None)
+
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+
+# get the correct version to use for checking stack features
+version_for_stack_feature_checks = get_stack_feature_version(config)
+
+stack_supports_config_versioning = check_stack_feature(StackFeature.CONFIG_VERSIONING, version_for_stack_feature_checks)
+stack_support_kms_hsm = check_stack_feature(StackFeature.RANGER_KMS_HSM_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_kerberos = check_stack_feature(StackFeature.RANGER_KERBEROS_SUPPORT,
+                                                     version_for_stack_feature_checks)
+stack_supports_pid = check_stack_feature(StackFeature.RANGER_KMS_PID_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_audit_db = check_stack_feature(StackFeature.RANGER_AUDIT_DB_SUPPORT,
+                                                     version_for_stack_feature_checks)
+stack_supports_ranger_kms_ssl = check_stack_feature(StackFeature.RANGER_KMS_SSL, version_for_stack_feature_checks)
+stack_supports_multiple_env_sh_files = check_stack_feature(StackFeature.MULTIPLE_ENV_SH_FILES_SUPPORT,
+                                                           version_for_stack_feature_checks)
+
+hadoop_conf_dir = conf_select.get_hadoop_conf_dir()
+security_enabled = config['configurations']['cluster-env']['security_enabled']
+
+# if stack_supports_config_versioning:
+kms_home = format('{stack_root}/current/ranger-kms')
+kms_conf_dir = format('{stack_root}/current/ranger-kms/conf')
+ranger_kms_setup_marker = os.path.join(kms_conf_dir, "kms_setup")
+
+kms_lib_path = format('{kms_home}/ews/lib/')
+kms_log_dir = default("/configurations/kms-env/kms_log_dir", "/var/log/ranger/kms")
+java_home = config['ambariLevelParams']['java_home']
+kms_user = default("/configurations/kms-env/kms_user", "kms")
+kms_group = default("/configurations/kms-env/kms_group", "kms")
+
+ranger_kms_audit_log_maxfilesize = default('/configurations/kms-log4j/ranger_kms_audit_log_maxfilesize', 256)
+ranger_kms_audit_log_maxbackupindex = default('/configurations/kms-log4j/ranger_kms_audit_log_maxbackupindex', 20)
+ranger_kms_log_maxfilesize = default('/configurations/kms-log4j/ranger_kms_log_maxfilesize', 256)
+ranger_kms_log_maxbackupindex = default('/configurations/kms-log4j/ranger_kms_log_maxbackupindex', 20)
+
+jdk_location = config['ambariLevelParams']['jdk_location']
+kms_log4j = config['configurations']['kms-log4j']['content']
+
+# ranger host
+ranger_admin_hosts = config['clusterHostInfo']['ranger_admin_hosts']
+has_ranger_admin = len(ranger_admin_hosts) > 0
+ranger_kms_hosts = config['clusterHostInfo']['ranger_kms_server_hosts']
+if len(ranger_kms_hosts) > 1:
+    kms_hosts = ";".join(ranger_kms_hosts)
+else:
+    kms_hosts = ranger_kms_hosts[0]
+kms_port = config['configurations']['kms-env']['kms_port']
+
+create_db_user = config['configurations']['kms-env']['create_db_user']
+
+# kms properties
+db_flavor = (config['configurations']['kms-properties']['DB_FLAVOR']).lower()
+db_host = config['configurations']['kms-properties']['db_host']
+db_name = config['configurations']['kms-properties']['db_name']
+db_user = config['configurations']['kms-properties']['db_user']
+db_password = unicode(config['configurations']['kms-properties']['db_password'])
+kms_master_key_password = unicode(config['configurations']['kms-properties']['KMS_MASTER_KEY_PASSWD'])
+credential_provider_path = config['configurations']['dbks-site']['ranger.ks.jpa.jdbc.credential.provider.path']
+jdbc_alias = config['configurations']['dbks-site']['ranger.ks.jpa.jdbc.credential.alias']
+masterkey_alias = config['configurations']['dbks-site']['ranger.ks.masterkey.credential.alias']
+repo_name = str(config['clusterName']) + '_kms'
+repo_name_value = config['configurations']['ranger-kms-security']['ranger.plugin.kms.service.name']
+if not is_empty(repo_name_value) and repo_name_value != "{{repo_name}}":
+    repo_name = repo_name_value
+cred_lib_path = os.path.join(kms_home, "cred", "lib", "*")
+cred_setup_prefix = (format('{kms_home}/ranger_credential_helper.py'), '-l', cred_lib_path)
+credential_file = format('/etc/ranger/{repo_name}/cred.jceks')
+
+# ranger kms ssl enabled config
+ranger_kms_ssl_enabled = config['configurations']['ranger-kms-site']['ranger.service.https.attrib.ssl.enabled']
+url_scheme = "http"
+if ranger_kms_ssl_enabled:
+    url_scheme = "https"
+
+if has_ranger_admin:
+    policymgr_mgr_url = config['configurations']['admin-properties']['policymgr_external_url']
+    if 'admin-properties' in config['configurations'] and 'policymgr_external_url' in config['configurations'][
+        'admin-properties'] and policymgr_mgr_url.endswith('/'):
+        policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
+    xa_audit_db_flavor = (config['configurations']['admin-properties']['DB_FLAVOR']).lower()
+    xa_audit_db_name = default('/configurations/admin-properties/audit_db_name', 'ranger_audits')
+    xa_audit_db_user = default('/configurations/admin-properties/audit_db_user', 'rangerlogger')
+    xa_audit_db_password = ''
+    if not is_empty(
+            config['configurations']['admin-properties']['audit_db_password']) and stack_supports_ranger_audit_db:
+        xa_audit_db_password = config['configurations']['admin-properties']['audit_db_password']
+    xa_db_host = config['configurations']['admin-properties']['db_host']
+
+    admin_uname = config['configurations']['ranger-env']['admin_username']
+    admin_password = config['configurations']['ranger-env']['admin_password']
+    ambari_ranger_admin = config['configurations']['ranger-env']['ranger_admin_username']
+    ambari_ranger_password = config['configurations']['ranger-env']['ranger_admin_password']
+    admin_uname_password = format("{admin_uname}:{admin_password}")
+    ranger_audit_solr_urls = config['configurations']['ranger-admin-site']['ranger.audit.solr.urls']
+
+default_connectors_map = {"mssql": "sqljdbc4.jar",
+                          "mysql": "mysql-connector-java.jar",
+                          "postgres": "postgresql-jdbc.jar",
+                          "oracle": "ojdbc.jar",
+                          "sqla": "sajdbc4.jar"}
+
+java_share_dir = '/usr/share/java'
+jdbc_jar_name = None
+previous_jdbc_jar_name = None
+if db_flavor == 'mysql':
+    jdbc_jar_name = default("/ambariLevelParams/custom_mysql_jdbc_name", None)
+    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mysql_jdbc_name", None)
+    db_jdbc_url = format('jdbc:log4jdbc:mysql://{db_host}/{db_name}')
+    db_jdbc_driver = "com.mysql.jdbc.Driver"
+    jdbc_dialect = "org.eclipse.persistence.platform.database.MySQLPlatform"
+elif db_flavor == 'oracle':
+    jdbc_jar_name = default("/ambariLevelParams/custom_oracle_jdbc_name", None)
+    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_oracle_jdbc_name", None)
+    colon_count = db_host.count(':')
+    if colon_count == 2 or colon_count == 0:
+        db_jdbc_url = format('jdbc:oracle:thin:@{db_host}')
+    else:
+        db_jdbc_url = format('jdbc:oracle:thin:@//{db_host}')
+    db_jdbc_driver = "oracle.jdbc.OracleDriver"
+    jdbc_dialect = "org.eclipse.persistence.platform.database.OraclePlatform"
+elif db_flavor == 'postgres':
+    jdbc_jar_name = default("/ambariLevelParams/custom_postgres_jdbc_name", None)
+    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_postgres_jdbc_name", None)
+    db_jdbc_url = format('jdbc:postgresql://{db_host}/{db_name}')
+    db_jdbc_driver = "org.postgresql.Driver"
+    jdbc_dialect = "org.eclipse.persistence.platform.database.PostgreSQLPlatform"
+elif db_flavor == 'mssql':
+    jdbc_jar_name = default("/ambariLevelParams/custom_mssql_jdbc_name", None)
+    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mssql_jdbc_name", None)
+    db_jdbc_url = format('jdbc:sqlserver://{db_host};databaseName={db_name}')
+    db_jdbc_driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
+    jdbc_dialect = "org.eclipse.persistence.platform.database.SQLServerPlatform"
+elif db_flavor == 'sqla':
+    jdbc_jar_name = default("/ambariLevelParams/custom_sqlanywhere_jdbc_name", None)
+    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_sqlanywhere_jdbc_name", None)
+    db_jdbc_url = format('jdbc:sqlanywhere:database={db_name};host={db_host}')
+    db_jdbc_driver = "sap.jdbc4.sqlanywhere.IDriver"
+    jdbc_dialect = "org.eclipse.persistence.platform.database.SQLAnywherePlatform"
+else:
+    raise Fail(format("'{db_flavor}' db flavor not supported."))
+
+downloaded_custom_connector = format("{tmp_dir}/{jdbc_jar_name}")
+
+driver_curl_source = format("{jdk_location}/{jdbc_jar_name}")
+driver_curl_target = format("{kms_home}/ews/webapp/lib/{jdbc_jar_name}")
+previous_jdbc_jar = format("{kms_home}/ews/webapp/lib/{previous_jdbc_jar_name}")
+ews_lib_jar_path = format("{kms_home}/ews/webapp/lib/{jdbc_jar_name}")
+
+if db_flavor == 'sqla':
+    downloaded_custom_connector = format("{tmp_dir}/sqla-client-jdbc.tar.gz")
+    jar_path_in_archive = format("{tmp_dir}/sqla-client-jdbc/java/sajdbc4.jar")
+    libs_path_in_archive = format("{tmp_dir}/sqla-client-jdbc/native/lib64/*")
+    jdbc_libs_dir = format("{kms_home}/native/lib64")
+    ld_library_path = format("{jdbc_libs_dir}")
+
+if has_ranger_admin:
+    xa_previous_jdbc_jar_name = None
+    if stack_supports_ranger_audit_db:
+        if xa_audit_db_flavor == 'mysql':
+            jdbc_jar = default("/ambariLevelParams/custom_mysql_jdbc_name", None)
+            xa_previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mysql_jdbc_name", None)
+            audit_jdbc_url = format('jdbc:mysql://{xa_db_host}/{xa_audit_db_name}')
+            jdbc_driver = "com.mysql.jdbc.Driver"
+        elif xa_audit_db_flavor == 'oracle':
+            jdbc_jar = default("/ambariLevelParams/custom_oracle_jdbc_name", None)
+            xa_previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_oracle_jdbc_name", None)
+            colon_count = xa_db_host.count(':')
+            if colon_count == 2 or colon_count == 0:
+                audit_jdbc_url = format('jdbc:oracle:thin:@{xa_db_host}')
+            else:
+                audit_jdbc_url = format('jdbc:oracle:thin:@//{xa_db_host}')
+            jdbc_driver = "oracle.jdbc.OracleDriver"
+        elif xa_audit_db_flavor == 'postgres':
+            jdbc_jar = default("/ambariLevelParams/custom_postgres_jdbc_name", None)
+            xa_previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_postgres_jdbc_name", None)
+            audit_jdbc_url = format('jdbc:postgresql://{xa_db_host}/{xa_audit_db_name}')
+            jdbc_driver = "org.postgresql.Driver"
+        elif xa_audit_db_flavor == 'mssql':
+            jdbc_jar = default("/ambariLevelParams/custom_mssql_jdbc_name", None)
+            xa_previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mssql_jdbc_name", None)
+            audit_jdbc_url = format('jdbc:sqlserver://{xa_db_host};databaseName={xa_audit_db_name}')
+            jdbc_driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
+        elif xa_audit_db_flavor == 'sqla':
+            jdbc_jar = default("/ambariLevelParams/custom_sqlanywhere_jdbc_name", None)
+            xa_previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_sqlanywhere_jdbc_name", None)
+            audit_jdbc_url = format('jdbc:sqlanywhere:database={xa_audit_db_name};host={xa_db_host}')
+            jdbc_driver = "sap.jdbc4.sqlanywhere.IDriver"
+        else:
+            raise Fail(format("'{xa_audit_db_flavor}' db flavor not supported."))
+
+    downloaded_connector_path = format("{tmp_dir}/{jdbc_jar}") if stack_supports_ranger_audit_db else None
+    driver_source = format("{jdk_location}/{jdbc_jar}") if stack_supports_ranger_audit_db else None
+    driver_target = format("{kms_home}/ews/webapp/lib/{jdbc_jar}") if stack_supports_ranger_audit_db else None
+    xa_previous_jdbc_jar = format(
+        "{kms_home}/ews/webapp/lib/{previous_jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+
+repo_config_username = config['configurations']['kms-properties']['REPOSITORY_CONFIG_USERNAME']
+repo_config_password = unicode(config['configurations']['kms-properties']['REPOSITORY_CONFIG_PASSWORD'])
+
+kms_plugin_config = {
+    'username': repo_config_username,
+    'password': repo_config_password,
+    'provider': format('kms://{url_scheme}@{kms_hosts}:{kms_port}/kms')
+}
+
+xa_audit_db_is_enabled = False
+if stack_supports_ranger_audit_db:
+    xa_audit_db_is_enabled = config['configurations']['ranger-kms-audit']['xasecure.audit.destination.db']
+ssl_keystore_password = unicode(
+    config['configurations']['ranger-kms-policymgr-ssl']['xasecure.policymgr.clientssl.keystore.password'])
+ssl_truststore_password = unicode(
+    config['configurations']['ranger-kms-policymgr-ssl']['xasecure.policymgr.clientssl.truststore.password'])
+
+# For SQLA explicitly disable audit to DB for Ranger
+if xa_audit_db_flavor == 'sqla':
+    xa_audit_db_is_enabled = False
+
+current_host = config['agentLevelParams']['hostname']
+if current_host in ranger_kms_hosts:
+    kms_host = current_host
+
+check_db_connection_jar_name = "DBConnectionVerification.jar"
+check_db_connection_jar = format("/usr/lib/ambari-agent/{check_db_connection_jar_name}")
+ranger_kms_jdbc_connection_url = config['configurations']['dbks-site']['ranger.ks.jpa.jdbc.url']
+ranger_kms_jdbc_driver = config['configurations']['dbks-site']['ranger.ks.jpa.jdbc.driver']
+
+jce_name = default("/ambariLevelParams/jce_name", None)
+jce_source_dir = format('{tmp_dir}/jce_dir')
+
+# kms hsm support
+enable_kms_hsm = default("/configurations/dbks-site/ranger.ks.hsm.enabled", False)
+hms_partition_alias = default("/configurations/dbks-site/ranger.ks.hsm.partition.password.alias",
+                              "ranger.kms.hsm.partition.password")
+hms_partition_passwd = default("/configurations/kms-env/hsm_partition_password", None)
+
+# kms kerberos from stack 2.5 onward
+rangerkms_bare_principal = 'rangerkms'
+
+if stack_supports_ranger_kerberos:
+    if security_enabled:
+        rangerkms_principal = config['configurations']['dbks-site']['ranger.ks.kerberos.principal']
+        rangerkms_keytab = config['configurations']['dbks-site']['ranger.ks.kerberos.keytab']
+        if not is_empty(rangerkms_principal) and rangerkms_principal != '':
+            rangerkms_bare_principal = get_bare_principal(rangerkms_principal)
+            rangerkms_principal = rangerkms_principal.replace('_HOST', kms_host.lower())
+    kms_plugin_config['policy.download.auth.users'] = format('keyadmin,{rangerkms_bare_principal}')
+
+custom_ranger_service_config = generate_ranger_service_config(config['configurations']['kms-properties'])
+if len(custom_ranger_service_config) > 0:
+    kms_plugin_config.update(custom_ranger_service_config)
+
+kms_ranger_plugin_repo = {
+    'isEnabled': 'true',
+    'configs': kms_plugin_config,
+    'description': 'kms repo',
+    'name': repo_name,
+    'type': 'kms'
+}
+
+# ranger kms pid
+user_group = config['configurations']['cluster-env']['user_group']
+ranger_kms_pid_dir = default("/configurations/kms-env/ranger_kms_pid_dir", "/var/run/ranger_kms")
+ranger_kms_pid_file = format('{ranger_kms_pid_dir}/rangerkms.pid')
+
+if security_enabled:
+    spengo_keytab = config['configurations']['kms-site'][
+        'hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.keytab']
+    spnego_principal = config['configurations']['kms-site'][
+        'hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.principal']
+    spnego_principal = spnego_principal.replace('_HOST', current_host.lower())
+
+plugin_audit_password_property = 'xasecure.audit.destination.db.password'
+kms_plugin_password_properties = ['xasecure.policymgr.clientssl.keystore.password',
+                                  'xasecure.policymgr.clientssl.truststore.password']
+dbks_site_password_properties = ['ranger.db.encrypt.key.password', 'ranger.ks.jpa.jdbc.password',
+                                 'ranger.ks.hsm.partition.password', 'ranger.kms.keysecure.login.password']
+ranger_kms_site_password_properties = ['ranger.service.https.attrib.keystore.pass']
+ranger_kms_cred_ssl_path = config['configurations']['ranger-kms-site']['ranger.credential.provider.path']
+ranger_kms_ssl_keystore_alias = config['configurations']['ranger-kms-site'][
+    'ranger.service.https.attrib.keystore.credential.alias']
+ranger_kms_ssl_passwd = config['configurations']['ranger-kms-site']['ranger.service.https.attrib.keystore.pass']
+
+xa_audit_hdfs_is_enabled = default("/configurations/ranger-kms-audit/xasecure.audit.destination.hdfs", False)
+namenode_host = default("/clusterHostInfo/namenode_hosts", [])
+
+# need this to capture cluster name from where ranger kms plugin is enabled
+cluster_name = config['clusterName']
+
+has_namenode = len(namenode_host) > 0
+
+hdfs_user = default("/configurations/hadoop-env/hdfs_user", None)
+hdfs_user_keytab = default("/configurations/hadoop-env/hdfs_user_keytab", None)
+hdfs_principal_name = default("/configurations/hadoop-env/hdfs_principal_name", None)
+default_fs = default("/configurations/core-site/fs.defaultFS", None)
+hdfs_site = config['configurations']['hdfs-site'] if has_namenode else None
+hadoop_bin_dir = stack_select.get_hadoop_dir("bin") if has_namenode else None
+kinit_path_local = get_kinit_path(default('/configurations/kerberos-env/executable_search_paths', None))
+dfs_type = default("/clusterLevelParams/dfs_type", "")
+
+import functools
+
+# create partial functions with common arguments for every HdfsResource call
+# to create/delete hdfs directory/file/copyfromlocal we need to call params.HdfsResource in code
+HdfsResource = functools.partial(
+    HdfsResource,
+    user=hdfs_user,
+    security_enabled=security_enabled,
+    keytab=hdfs_user_keytab,
+    kinit_path_local=kinit_path_local,
+    hadoop_bin_dir=hadoop_bin_dir,
+    hadoop_conf_dir=hadoop_conf_dir,
+    principal_name=hdfs_principal_name,
+    hdfs_site=hdfs_site,
+    default_fs=default_fs,
+    dfs_type=dfs_type,
+)
+
+local_component_list = default("/localComponents", [])
+has_hdfs_client_on_node = 'HDFS_CLIENT' in local_component_list
+
+kms_env_content = config['configurations']['kms-env']['content']
+
+# zookeeper principal
+zookeeper_principal = default("/configurations/zookeeper-env/zookeeper_principal_name", "zookeeper@EXAMPLE.COM")
+zookeeper_principal_primary = get_bare_principal(zookeeper_principal)
+
+mount_table_xml_inclusion_file_full_path = None
+mount_table_content = None
+if 'viewfs-mount-table' in config['configurations']:
+    xml_inclusion_file_name = 'viewfs-mount-table.xml'
+    mount_table = config['configurations']['viewfs-mount-table']
+
+    if 'content' in mount_table and mount_table['content'].strip():
+        mount_table_xml_inclusion_file_full_path = os.path.join(kms_conf_dir, xml_inclusion_file_name)
+        mount_table_content = mount_table['content']
+
+# Ranger KMS Service maximum heap size configurations
+ranger_kms_max_heap_size = default('/configurations/kms-env/ranger_kms_max_heap_size', '1g')
+
+# ranger kms keysecure support
+enable_kms_keysecure = default("/configurations/dbks-site/ranger.kms.keysecure.enabled", False)
+keysecure_login_password = config['configurations']['dbks-site']['ranger.kms.keysecure.login.password']
+keysecure_login_password_alias = config['configurations']['dbks-site']['ranger.kms.keysecure.login.password.alias']
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/upgrade.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/upgrade.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/upgrade.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/upgrade.py	(date 1719626239000)
@@ -0,0 +1,29 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.core.resources.system import Execute
+from resource_management.libraries.functions import conf_select
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions.format import format
+
+def prestart(env):
+  import params
+
+  if params.version and params.stack_supports_config_versioning:
+    stack_select.select_packages(params.version)
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_server.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_server.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_server.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_server.py	(date 1719626239000)
@@ -0,0 +1,141 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.core.exceptions import Fail
+from resource_management.libraries.functions.check_process_status import check_process_status
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions import upgrade_summary
+from resource_management.libraries.functions.constants import Direction
+from resource_management.libraries.script import Script
+from resource_management.core.resources.system import Execute, File
+from resource_management.core.exceptions import ComponentIsNotRunning
+from resource_management.libraries.functions.format import format
+from resource_management.core.logger import Logger
+from resource_management.core import shell
+from resource_management.libraries.functions.default import default
+from kms_service import kms_service
+import upgrade
+
+import kms
+
+class KmsServer(Script):
+
+  def install(self, env):
+    self.install_packages(env)
+    import params
+    env.set_params(params)
+
+    # taking backup of install.properties file
+    Execute(('cp', '-f', format('{kms_home}/install.properties'), format('{kms_home}/install-backup.properties')),
+      not_if = format('ls {kms_home}/install-backup.properties'),
+      only_if = format('ls {kms_home}/install.properties'),
+      sudo = True
+    )
+    Execute('python /usr/lib/bigtop-select/distro-select set ranger-kms 3.2.0')
+
+    kms.setup_kms_db()
+    self.configure(env)
+    kms.setup_java_patch()
+
+  def stop(self, env, upgrade_type=None):
+    import params
+
+    env.set_params(params)
+    kms_service(action = 'stop', upgrade_type=upgrade_type)
+    if params.stack_supports_pid:
+      File(params.ranger_kms_pid_file,
+        action = "delete"
+      )
+
+  def start(self, env, upgrade_type=None):
+    import params
+
+    env.set_params(params)
+    self.configure(env)
+    kms.enable_kms_plugin()
+    kms.setup_kms_jce()
+    kms.update_password_configs()
+    kms_service(action = 'start', upgrade_type=upgrade_type)
+
+  def status(self, env):
+    import status_params
+    env.set_params(status_params)
+
+    if status_params.stack_supports_pid:
+      check_process_status(status_params.ranger_kms_pid_file)
+      return
+
+    cmd = 'ps -ef | grep proc_rangerkms | grep -v grep'
+    code, output = shell.call(cmd, timeout=20)
+    if code != 0:
+      Logger.debug('KMS process not running')
+      raise ComponentIsNotRunning()
+    pass
+
+  def configure(self, env):
+    import params
+
+    env.set_params(params)
+    # set up ranger_admin version for current
+    Execute('python /usr/lib/bigtop-select/distro-select set ranger-kms 3.2.0')
+
+    kms.kms()
+
+  def pre_upgrade_restart(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+
+    upgrade.prestart(env)
+    kms.kms(upgrade_type=upgrade_type)
+    kms.setup_java_patch()
+
+  def post_upgrade_restart(self, env, upgrade_type = None):
+    import params
+    env.set_params(params)
+
+    if upgrade_type and params.upgrade_direction == Direction.UPGRADE and not params.stack_supports_multiple_env_sh_files:
+      files_name_list = ['ranger-kms-env-piddir.sh', 'ranger-kms-env-logdir.sh']
+      for file_name in files_name_list:
+        File(format("{kms_conf_dir}/{file_name}"),
+          action = "delete"
+        )
+
+  def setup_ranger_kms_database(self, env):
+    import params
+    env.set_params(params)
+
+    upgrade_stack = stack_select._get_upgrade_stack()
+    if upgrade_stack is None:
+      raise Fail('Unable to determine the stack and stack version')
+
+    stack_version = upgrade_stack[1]
+    target_version = upgrade_summary.get_target_version("RANGER_KMS", default_version = stack_version)
+    Logger.info(format('Setting Ranger KMS database schema, using version {target_version}'))
+    kms.setup_kms_db(stack_version = target_version)
+
+  def get_log_folder(self):
+    import params
+    return params.kms_log_dir
+
+  def get_user(self):
+    import params
+    return params.kms_user
+
+if __name__ == "__main__":
+  KmsServer().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_service.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_service.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_service.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/kms_service.py	(date 1719626239000)
@@ -0,0 +1,58 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.core.resources.system import Execute, File
+from resource_management.core import shell
+from resource_management.libraries.functions.format import format
+from resource_management.core.exceptions import ComponentIsNotRunning
+from resource_management.core.logger import Logger
+from resource_management.libraries.functions.show_logs import show_logs
+from ambari_commons.constants import UPGRADE_TYPE_NON_ROLLING, UPGRADE_TYPE_ROLLING
+from resource_management.libraries.functions.constants import Direction
+import os
+
+def kms_service(action='start', upgrade_type=None):
+  import params
+
+  env_dict = {'JAVA_HOME': params.java_home}
+  if params.db_flavor.lower() == 'sqla':
+    env_dict = {'JAVA_HOME': params.java_home, 'LD_LIBRARY_PATH': params.ld_library_path}
+
+  if action == 'start':
+    no_op_test = format('ps -ef | grep proc_rangerkms | grep -v grep')
+    cmd = format('{kms_home}/ranger-kms start')
+    try:
+      Execute(cmd, not_if=no_op_test, environment=env_dict, user=format('{kms_user}'))
+    except:
+      show_logs(params.kms_log_dir, params.kms_user)
+      raise
+  elif action == 'stop':
+    if upgrade_type == UPGRADE_TYPE_NON_ROLLING and params.upgrade_direction == Direction.UPGRADE:
+      if os.path.isfile(format('{kms_home}/ranger-kms')):
+        File(format('{kms_home}/ranger-kms'),
+          owner=params.kms_user,
+          group = params.kms_group
+        )
+    cmd = format('{kms_home}/ranger-kms stop')
+    try:
+      Execute(cmd, environment=env_dict, user=format('{kms_user}'))
+    except:
+      show_logs(params.kms_log_dir, params.kms_user)
+      raise
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/service_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/service_check.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/service_check.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/service_check.py	(date 1719626239000)
@@ -0,0 +1,41 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.script import Script
+from resource_management.core.logger import Logger
+from resource_management.core import shell
+from resource_management.core.exceptions import ComponentIsNotRunning
+
+
+class KmsServiceCheck(Script):
+  def service_check(self, env):
+    import params
+
+    env.set_params(params)
+    cmd = 'ps -ef | grep proc_rangerkms | grep -v grep'
+    code, output = shell.call(cmd, timeout=20)
+    if code == 0:
+      Logger.info('KMS process up and running')
+    else:
+      Logger.debug('KMS process not running')
+      raise ComponentIsNotRunning()
+
+if __name__ == "__main__":
+  KmsServiceCheck().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hdfs-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hdfs-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hdfs-site.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hdfs-site.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hdfs-site.xml	(date 1723883106878)
@@ -18,500 +18,547 @@
 -->
 <!-- Put site-specific property overrides in this file. -->
 <configuration supports_final="true">
-  <!-- file system properties -->
-  <property>
-    <name>dfs.namenode.name.dir</name>
-    <!-- cluster variant -->
-    <value>/hadoop/hdfs/namenode</value>
-    <display-name>NameNode directories</display-name>
-    <description>Determines where on the local filesystem the DFS name node
-      should store the name table.  If this is a comma-delimited list
-      of directories then the name table is replicated in all of the
-      directories, for redundancy. </description>
-    <final>true</final>
-    <value-attributes>
-      <type>directories</type>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.support.append</name>
-    <value>true</value>
-    <description>to enable dfs append</description>
-    <final>true</final>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.webhdfs.enabled</name>
-    <value>true</value>
-    <display-name>WebHDFS enabled</display-name>
-    <description>Whether to enable WebHDFS feature</description>
-    <final>true</final>
-    <value-attributes>
-      <type>boolean</type>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.failed.volumes.tolerated</name>
-    <value>0</value>
-    <description> Number of failed disks a DataNode would tolerate before it stops offering service</description>
-    <final>true</final>
-    <display-name>DataNode failed disk tolerance</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>2</maximum>
-      <increment-step>1</increment-step>
-    </value-attributes>
-    <depends-on>
-      <property>
-        <type>hdfs-site</type>
-        <name>dfs.datanode.data.dir</name>
-      </property>
-    </depends-on>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.data.dir</name>
-    <value>/hadoop/hdfs/data</value>
-    <display-name>DataNode directories</display-name>
-    <description>Determines where on the local filesystem an DFS data node
-      should store its blocks.  If this is a comma-delimited
-      list of directories, then data will be stored in all named
-      directories, typically on different devices.
-      Directories that do not exist are ignored.
-    </description>
-    <final>true</final>
-    <value-attributes>
-      <type>directories</type>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.hosts.exclude</name>
-    <value>/etc/hadoop/conf/dfs.exclude</value>
-    <description>Names a file that contains a list of hosts that are
-      not permitted to connect to the namenode.  The full pathname of the
-      file must be specified.  If the value is empty, no hosts are
-      excluded.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>manage.include.files</name>
-    <value>false</value>
-    <description>If true Ambari will manage include file if dfs.hosts is configured.</description>
-    <on-ambari-upgrade add="false"/>
-  </property>
-  <!--
-    <property>
-      <name>dfs.hosts</name>
-      <value>/etc/hadoop/conf/dfs.include</value>
-      <description>Names a file that contains a list of hosts that are
-      permitted to connect to the namenode. The full pathname of the file
-      must be specified.  If the value is empty, all hosts are
-      permitted.</description>
-    </property>
-  -->
-  <property>
-    <name>dfs.namenode.checkpoint.dir</name>
-    <value>/hadoop/hdfs/namesecondary</value>
-    <display-name>SecondaryNameNode Checkpoint directories</display-name>
-    <description>Determines where on the local filesystem the DFS secondary
-      name node should store the temporary images to merge.
-      If this is a comma-delimited list of directories then the image is
-      replicated in all of the directories for redundancy.
-    </description>
-    <value-attributes>
-      <type>directories</type>
-      <overridable>false</overridable>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.checkpoint.edits.dir</name>
-    <display-name>NameNode Checkpoint Edits directory</display-name>
-    <value>${dfs.namenode.checkpoint.dir}</value>
-    <description>Determines where on the local filesystem the DFS secondary
-      name node should store the temporary edits to merge.
-      If this is a comma-delimited list of directories then the edits are
-      replicated in all of the directories for redundancy.
-      Default value is same as dfs.namenode.checkpoint.dir
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.checkpoint.period</name>
-    <value>21600</value>
-    <display-name>HDFS Maximum Checkpoint Delay</display-name>
-    <description>The number of seconds between two periodic checkpoints.</description>
-    <value-attributes>
-      <type>int</type>
-      <unit>seconds</unit>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.checkpoint.txns</name>
-    <value>1000000</value>
-    <description>The Secondary NameNode or CheckpointNode will create a checkpoint
-      of the namespace every 'dfs.namenode.checkpoint.txns' transactions,
-      regardless of whether 'dfs.namenode.checkpoint.period' has expired.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.replication.max</name>
-    <value>50</value>
-    <description>Maximal block replication.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.replication</name>
-    <value>3</value>
-    <display-name>Block replication</display-name>
-    <description>Default block replication.
-    </description>
-    <value-attributes>
-      <type>int</type>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.heartbeat.interval</name>
-    <value>3</value>
-    <description>Determines datanode heartbeat interval in seconds.</description>
-    <on-ambari-upgrade add="true"/>
-    <supported-refresh-commands>
-      <refresh-command componentName="NAMENODE" command="reload_configs" />
-    </supported-refresh-commands>
-  </property>
-  <property>
-    <name>dfs.namenode.safemode.threshold-pct</name>
-    <value>0.999</value>
-    <description>
-      Specifies the percentage of blocks that should satisfy
-      the minimal replication requirement defined by dfs.namenode.replication.min.
-      Values less than or equal to 0 mean not to start in safe mode.
-      Values greater than 1 will make safe mode permanent.
-    </description>
-    <display-name>Minimum replicated blocks %</display-name>
-    <value-attributes>
-      <type>float</type>
-      <minimum>0.990</minimum>
-      <maximum>1.000</maximum>
-      <increment-step>0.001</increment-step>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.balance.bandwidthPerSec</name>
-    <value>6250000</value>
-    <description>
-      Specifies the maximum amount of bandwidth that each datanode
-      can utilize for the balancing purpose in term of
-      the number of bytes per second.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.https.port</name>
-    <value>50470</value>
-    <description>
-      This property is used by HftpFileSystem.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.address</name>
-    <value>0.0.0.0:50010</value>
-    <description>
-      The datanode server address and port for data transfer.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.http.address</name>
-    <value>0.0.0.0:50075</value>
-    <description>
-      The datanode http server address and port.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.https.address</name>
-    <value>0.0.0.0:50475</value>
-    <description>
-      The datanode https server address and port.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.blocksize</name>
-    <value>134217728</value>
-    <description>The default block size for new files.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.http-address</name>
-    <value>localhost:50070</value>
-    <description>The name of the default file system.  Either the
-      literal string "local" or a host:port for HDFS.</description>
-    <final>true</final>
-    <on-ambari-upgrade add="false"/>
-  </property>
-  <property>
-    <name>dfs.namenode.rpc-address</name>
-    <value>localhost:8020</value>
-    <description>RPC address that handles all clients requests.</description>
-    <on-ambari-upgrade add="false"/>
-  </property>
-  <property>
-    <name>dfs.datanode.du.reserved</name>
-    <!-- cluster variant -->
-    <value>1073741824</value>
-    <display-name>Reserved space for HDFS</display-name>
-    <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
-    </description>
-    <value-attributes>
-      <type>int</type>
-      <unit>bytes</unit>
-    </value-attributes>
-    <depends-on>
-      <property>
-        <type>hdfs-site</type>
-        <name>dfs.datanode.data.dir</name>
-      </property>
-    </depends-on>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.ipc.address</name>
-    <value>0.0.0.0:8010</value>
-    <description>
-      The datanode ipc server address and port.
-      If the port is 0 then the server will start on a free port.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.blockreport.initialDelay</name>
-    <value>120</value>
-    <description>Delay for first block report in seconds.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.datanode.max.transfer.threads</name>
-    <value>1024</value>
-    <description>Specifies the maximum number of threads to use for transferring data in and out of the datanode.</description>
-    <display-name>DataNode max data transfer threads</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>0</minimum>
-      <maximum>48000</maximum>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <!-- Permissions configuration -->
-  <property>
-    <name>fs.permissions.umask-mode</name>
-    <value>022</value>
-    <description>
-      The octal umask used when creating files and directories.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.permissions.enabled</name>
-    <value>true</value>
-    <description>
-      If "true", enable permission checking in HDFS.
-      If "false", permission checking is turned off,
-      but all other behavior is unchanged.
-      Switching from one parameter value to the other does not change the mode,
-      owner or group of files or directories.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.permissions.superusergroup</name>
-    <value>hdfs</value>
-    <property-type>GROUP</property-type>
-    <description>The name of the group of super-users.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.handler.count</name>
-    <value>100</value>
-    <description>Added to grow Queue size so that more client connections are allowed</description>
-    <display-name>NameNode Server threads</display-name>
-    <value-attributes>
-      <type>int</type>
-      <minimum>1</minimum>
-      <maximum>200</maximum>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.block.access.token.enable</name>
-    <value>true</value>
-    <description>
-      If "true", access tokens are used as capabilities for accessing datanodes.
-      If "false", no access tokens are checked on accessing datanodes.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <!-- cluster variant -->
-    <name>dfs.namenode.secondary.http-address</name>
-    <value>localhost:50090</value>
-    <description>Address of secondary namenode web server</description>
-    <on-ambari-upgrade add="false"/>
-  </property>
-  <property>
-    <name>dfs.namenode.https-address</name>
-    <value>localhost:50470</value>
-    <description>The https address where namenode binds</description>
-    <on-ambari-upgrade add="false"/>
-  </property>
-  <property>
-    <name>dfs.datanode.data.dir.perm</name>
-    <value>750</value>
-    <display-name>DataNode directories permission</display-name>
-    <description>The permissions that should be there on dfs.datanode.data.dir
-      directories. The datanode will not come up if the permissions are
-      different on existing dfs.datanode.data.dir directories. If the directories
-      don't exist, they will be created with this permission.</description>
-    <value-attributes>
-      <type>int</type>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.accesstime.precision</name>
-    <value>0</value>
-    <display-name>Access time precision</display-name>
-    <description>The access time for HDFS file is precise up to this value.
-      The default value is 1 hour. Setting a value of 0 disables
-      access times for HDFS.
-    </description>
-    <value-attributes>
-      <type>int</type>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.cluster.administrators</name>
-    <value> hdfs</value>
-    <description>ACL for the admins, this configuration is used to control who can access the default servlets in the namenode, etc. The value should be a comma separated list of users and groups. The user list comes first and is separated by a space followed by the group list, e.g. "user1,user2 group1,group2". Both users and groups are optional, so "user1", " group1", "", "user1 group1", "user1,user2 group1,group2" are all valid (note the leading space in " group1"). '*' grants access to all users and groups, e.g. '*', '* ' and ' *' are all valid.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.avoid.read.stale.datanode</name>
-    <value>true</value>
-    <description>
-      Indicate whether or not to avoid reading from stale datanodes whose
-      heartbeat messages have not been received by the namenode for more than a
-      specified time interval.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.avoid.write.stale.datanode</name>
-    <value>true</value>
-    <description>
-      Indicate whether or not to avoid writing to stale datanodes whose
-      heartbeat messages have not been received by the namenode for more than a
-      specified time interval.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.write.stale.datanode.ratio</name>
-    <value>1.0f</value>
-    <description>When the ratio of number stale datanodes to total datanodes marked is greater
-      than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.stale.datanode.interval</name>
-    <value>30000</value>
-    <description>Datanode is stale after not getting a heartbeat in this interval in ms</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.journalnode.http-address</name>
-    <value>0.0.0.0:8480</value>
-    <description>The address and port the JournalNode web UI listens on.
-      If the port is 0 then the server will start on a free port. </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.journalnode.https-address</name>
-    <value>0.0.0.0:8481</value>
-    <description>The address and port the JournalNode HTTPS server listens on.
-      If the port is 0 then the server will start on a free port. </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.journalnode.edits.dir</name>
-    <display-name>JournalNode Edits directory</display-name>
-    <value>/grid/0/hdfs/journal</value>
-    <description>The path where the JournalNode daemon will store its local state. </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <!-- HDFS Short-Circuit Local Reads -->
-  <property>
-    <name>dfs.client.read.shortcircuit</name>
-    <value>true</value>
-    <display-name>HDFS Short-circuit read</display-name>
-    <description>
-      This configuration parameter turns on short-circuit local reads.
-    </description>
-    <value-attributes>
-      <type>boolean</type>
-    </value-attributes>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.domain.socket.path</name>
-    <value>/var/lib/hadoop-hdfs/dn_socket</value>
-    <description>
-      This is a path to a UNIX domain socket that will be used for communication between the DataNode and local HDFS clients.
-      If the string "_PORT" is present in this path, it will be replaced by the TCP port of the DataNode.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.client.read.shortcircuit.streams.cache.size</name>
-    <value>4096</value>
-    <description>
-      The DFSClient maintains a cache of recently opened file descriptors. This
-      parameter controls the size of that cache. Setting this higher will use
-      more file descriptors, but potentially provide better performance on
-      workloads involving lots of seeks.
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.namenode.name.dir.restore</name>
-    <value>true</value>
-    <description>Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir.
-      When enabled, a recovery of any failed directory is attempted during checkpoint.</description>
-    <on-ambari-upgrade add="true"/>
-  </property>
-  <property>
-    <name>dfs.http.policy</name>
-    <value>HTTP_ONLY</value>
-    <description>
-      Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons:
-      The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY :
-      Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https
-    </description>
-    <on-ambari-upgrade add="true"/>
-  </property>
+    <!-- file system properties -->
+    <property>
+        <name>dfs.namenode.name.dir</name>
+        <!-- cluster variant -->
+        <value>/hadoop/hdfs/namenode</value>
+        <display-name>NameNode directories</display-name>
+        <description>Determines where on the local filesystem the DFS name node
+            should store the name table. If this is a comma-delimited list
+            of directories then the name table is replicated in all of the
+            directories, for redundancy.
+        </description>
+        <final>true</final>
+        <value-attributes>
+            <type>directories</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.support.append</name>
+        <value>true</value>
+        <description>to enable dfs append</description>
+        <final>true</final>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.webhdfs.enabled</name>
+        <value>true</value>
+        <display-name>WebHDFS enabled</display-name>
+        <description>Whether to enable WebHDFS feature</description>
+        <final>true</final>
+        <value-attributes>
+            <type>boolean</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.failed.volumes.tolerated</name>
+        <value>0</value>
+        <description>Number of failed disks a DataNode would tolerate before it stops offering service</description>
+        <final>true</final>
+        <display-name>DataNode failed disk tolerance</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>2</maximum>
+            <increment-step>1</increment-step>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>hdfs-site</type>
+                <name>dfs.datanode.data.dir</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.data.dir</name>
+        <value>/hadoop/hdfs/data</value>
+        <display-name>DataNode directories</display-name>
+        <description>Determines where on the local filesystem an DFS data node
+            should store its blocks. If this is a comma-delimited
+            list of directories, then data will be stored in all named
+            directories, typically on different devices.
+            Directories that do not exist are ignored.
+        </description>
+        <final>true</final>
+        <value-attributes>
+            <type>directories</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.hosts.exclude</name>
+        <value>/etc/hadoop/conf/dfs.exclude</value>
+        <description>Names a file that contains a list of hosts that are
+            not permitted to connect to the namenode. The full pathname of the
+            file must be specified. If the value is empty, no hosts are
+            excluded.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>manage.include.files</name>
+        <value>false</value>
+        <description>If true Ambari will manage include file if dfs.hosts is configured.</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <!--
+      <property>
+        <name>dfs.hosts</name>
+        <value>/etc/hadoop/conf/dfs.include</value>
+        <description>Names a file that contains a list of hosts that are
+        permitted to connect to the namenode. The full pathname of the file
+        must be specified.  If the value is empty, all hosts are
+        permitted.</description>
+      </property>
+    -->
+    <property>
+        <name>dfs.namenode.checkpoint.dir</name>
+        <value>/hadoop/hdfs/namesecondary</value>
+        <display-name>SecondaryNameNode Checkpoint directories</display-name>
+        <description>Determines where on the local filesystem the DFS secondary
+            name node should store the temporary images to merge.
+            If this is a comma-delimited list of directories then the image is
+            replicated in all of the directories for redundancy.
+        </description>
+        <value-attributes>
+            <type>directories</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.checkpoint.edits.dir</name>
+        <display-name>NameNode Checkpoint Edits directory</display-name>
+        <value>${dfs.namenode.checkpoint.dir}</value>
+        <description>Determines where on the local filesystem the DFS secondary
+            name node should store the temporary edits to merge.
+            If this is a comma-delimited list of directories then the edits are
+            replicated in all of the directories for redundancy.
+            Default value is same as dfs.namenode.checkpoint.dir
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.checkpoint.period</name>
+        <value>21600</value>
+        <display-name>HDFS Maximum Checkpoint Delay</display-name>
+        <description>The number of seconds between two periodic checkpoints.</description>
+        <value-attributes>
+            <type>int</type>
+            <unit>seconds</unit>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.checkpoint.txns</name>
+        <value>1000000</value>
+        <description>The Secondary NameNode or CheckpointNode will create a checkpoint
+            of the namespace every 'dfs.namenode.checkpoint.txns' transactions,
+            regardless of whether 'dfs.namenode.checkpoint.period' has expired.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.replication.max</name>
+        <value>50</value>
+        <description>Maximal block replication.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.replication</name>
+        <value>3</value>
+        <display-name>Block replication</display-name>
+        <description>Default block replication.
+        </description>
+        <value-attributes>
+            <type>int</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.heartbeat.interval</name>
+        <value>3</value>
+        <description>Determines datanode heartbeat interval in seconds.</description>
+        <on-ambari-upgrade add="true"/>
+        <supported-refresh-commands>
+            <refresh-command componentName="NAMENODE" command="reload_configs"/>
+        </supported-refresh-commands>
+    </property>
+    <property>
+        <name>dfs.namenode.safemode.threshold-pct</name>
+        <value>0.999</value>
+        <description>
+            Specifies the percentage of blocks that should satisfy
+            the minimal replication requirement defined by dfs.namenode.replication.min.
+            Values less than or equal to 0 mean not to start in safe mode.
+            Values greater than 1 will make safe mode permanent.
+        </description>
+        <display-name>Minimum replicated blocks %</display-name>
+        <value-attributes>
+            <type>float</type>
+            <minimum>0.990</minimum>
+            <maximum>1.000</maximum>
+            <increment-step>0.001</increment-step>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.balance.bandwidthPerSec</name>
+        <value>6250000</value>
+        <description>
+            Specifies the maximum amount of bandwidth that each datanode
+            can utilize for the balancing purpose in term of
+            the number of bytes per second.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.https.port</name>
+        <value>50470</value>
+        <description>
+            This property is used by HftpFileSystem.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.address</name>
+        <value>0.0.0.0:50010</value>
+        <description>
+            The datanode server address and port for data transfer.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.http.address</name>
+        <value>0.0.0.0:50075</value>
+        <description>
+            The datanode http server address and port.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.https.address</name>
+        <value>0.0.0.0:50475</value>
+        <description>
+            The datanode https server address and port.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.blocksize</name>
+        <value>134217728</value>
+        <description>The default block size for new files.</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.http-address</name>
+        <value>localhost:50070</value>
+        <description>The name of the default file system. Either the
+            literal string "local" or a host:port for HDFS.
+        </description>
+        <final>true</final>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>dfs.namenode.rpc-address</name>
+        <value>localhost:8020</value>
+        <description>RPC address that handles all clients requests.</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>dfs.datanode.du.reserved</name>
+        <!-- cluster variant -->
+        <value>1073741824</value>
+        <display-name>Reserved space for HDFS</display-name>
+        <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
+        </description>
+        <value-attributes>
+            <type>int</type>
+            <unit>bytes</unit>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>hdfs-site</type>
+                <name>dfs.datanode.data.dir</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.ipc.address</name>
+        <value>0.0.0.0:8010</value>
+        <description>
+            The datanode ipc server address and port.
+            If the port is 0 then the server will start on a free port.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.blockreport.initialDelay</name>
+        <value>120</value>
+        <description>Delay for first block report in seconds.</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.datanode.max.transfer.threads</name>
+        <value>1024</value>
+        <description>Specifies the maximum number of threads to use for transferring data in and out of the datanode.
+        </description>
+        <display-name>DataNode max data transfer threads</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>48000</maximum>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <!-- Permissions configuration -->
+    <property>
+        <name>fs.permissions.umask-mode</name>
+        <value>022</value>
+        <description>
+            The octal umask used when creating files and directories.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.permissions.enabled</name>
+        <value>true</value>
+        <description>
+            If "true", enable permission checking in HDFS.
+            If "false", permission checking is turned off,
+            but all other behavior is unchanged.
+            Switching from one parameter value to the other does not change the mode,
+            owner or group of files or directories.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.permissions.superusergroup</name>
+        <value>hdfs</value>
+        <property-type>GROUP</property-type>
+        <description>The name of the group of super-users.</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.handler.count</name>
+        <value>100</value>
+        <description>Added to grow Queue size so that more client connections are allowed</description>
+        <display-name>NameNode Server threads</display-name>
+        <value-attributes>
+            <type>int</type>
+            <minimum>1</minimum>
+            <maximum>200</maximum>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.block.access.token.enable</name>
+        <value>true</value>
+        <description>
+            If "true", access tokens are used as capabilities for accessing datanodes.
+            If "false", no access tokens are checked on accessing datanodes.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <!-- cluster variant -->
+        <name>dfs.namenode.secondary.http-address</name>
+        <value>localhost:50090</value>
+        <description>Address of secondary namenode web server</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>dfs.namenode.https-address</name>
+        <value>localhost:50470</value>
+        <description>The https address where namenode binds</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>dfs.datanode.data.dir.perm</name>
+        <value>750</value>
+        <display-name>DataNode directories permission</display-name>
+        <description>The permissions that should be there on dfs.datanode.data.dir
+            directories. The datanode will not come up if the permissions are
+            different on existing dfs.datanode.data.dir directories. If the directories
+            don't exist, they will be created with this permission.
+        </description>
+        <value-attributes>
+            <type>int</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.accesstime.precision</name>
+        <value>0</value>
+        <display-name>Access time precision</display-name>
+        <description>The access time for HDFS file is precise up to this value.
+            The default value is 1 hour. Setting a value of 0 disables
+            access times for HDFS.
+        </description>
+        <value-attributes>
+            <type>int</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.cluster.administrators</name>
+        <value>hdfs</value>
+        <description>ACL for the admins, this configuration is used to control who can access the default servlets in
+            the namenode, etc. The value should be a comma separated list of users and groups. The user list comes first
+            and is separated by a space followed by the group list, e.g. "user1,user2 group1,group2". Both users and
+            groups are optional, so "user1", " group1", "", "user1 group1", "user1,user2 group1,group2" are all valid
+            (note the leading space in " group1"). '*' grants access to all users and groups, e.g. '*', '* ' and ' *'
+            are all valid.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.avoid.read.stale.datanode</name>
+        <value>true</value>
+        <description>
+            Indicate whether or not to avoid reading from stale datanodes whose
+            heartbeat messages have not been received by the namenode for more than a
+            specified time interval.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.avoid.write.stale.datanode</name>
+        <value>true</value>
+        <description>
+            Indicate whether or not to avoid writing to stale datanodes whose
+            heartbeat messages have not been received by the namenode for more than a
+            specified time interval.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.write.stale.datanode.ratio</name>
+        <value>1.0f</value>
+        <description>When the ratio of number stale datanodes to total datanodes marked is greater
+            than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.stale.datanode.interval</name>
+        <value>30000</value>
+        <description>Datanode is stale after not getting a heartbeat in this interval in ms</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.journalnode.http-address</name>
+        <value>0.0.0.0:8480</value>
+        <description>The address and port the JournalNode web UI listens on.
+            If the port is 0 then the server will start on a free port.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.journalnode.https-address</name>
+        <value>0.0.0.0:8481</value>
+        <description>The address and port the JournalNode HTTPS server listens on.
+            If the port is 0 then the server will start on a free port.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.journalnode.edits.dir</name>
+        <display-name>JournalNode Edits directory</display-name>
+        <value>/grid/0/hdfs/journal</value>
+        <description>The path where the JournalNode daemon will store its local state.</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <!-- HDFS Short-Circuit Local Reads -->
+    <property>
+        <name>dfs.client.read.shortcircuit</name>
+        <value>true</value>
+        <display-name>HDFS Short-circuit read</display-name>
+        <description>
+            This configuration parameter turns on short-circuit local reads.
+        </description>
+        <value-attributes>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.domain.socket.path</name>
+        <value>/var/lib/hadoop-hdfs/dn_socket</value>
+        <description>
+            This is a path to a UNIX domain socket that will be used for communication between the DataNode and local
+            HDFS clients.
+            If the string "_PORT" is present in this path, it will be replaced by the TCP port of the DataNode.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.client.read.shortcircuit.streams.cache.size</name>
+        <value>4096</value>
+        <description>
+            The DFSClient maintains a cache of recently opened file descriptors. This
+            parameter controls the size of that cache. Setting this higher will use
+            more file descriptors, but potentially provide better performance on
+            workloads involving lots of seeks.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.namenode.name.dir.restore</name>
+        <value>true</value>
+        <description>Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir.
+            When enabled, a recovery of any failed directory is attempted during checkpoint.
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>dfs.http.policy</name>
+        <value>HTTP_ONLY</value>
+        <description>
+            Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons:
+            The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY :
+            Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https
+        </description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+
+    <!--ranger-->
+    <property>
+        <name>dfs.namenode.inode.attributes.provider.class</name>
+        <description>Enable ranger hdfs plugin</description>
+        <depends-on>
+            <property>
+                <type>ranger-hdfs-plugin-properties</type>
+                <name>ranger-hdfs-plugin-enabled</name>
+            </property>
+        </depends-on>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>dfs.permissions.ContentSummary.subAccess</name>
+        <value>false</value>
+        <depends-on>
+            <property>
+                <type>ranger-hdfs-plugin-properties</type>
+                <name>ranger-hdfs-plugin-enabled</name>
+            </property>
+        </depends-on>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+
 </configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/status_params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/status_params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/status_params.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/scripts/status_params.py	(date 1719626239000)
@@ -0,0 +1,36 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.version import format_stack_version
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions import StackFeature
+
+config  = Script.get_config()
+tmp_dir = Script.get_tmp_dir()
+
+stack_name = default("/clusterLevelParams/stack_name", None)
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+stack_supports_pid = stack_version_formatted and check_stack_feature(StackFeature.RANGER_KMS_PID_SUPPORT, stack_version_formatted)
+ranger_kms_pid_dir = default("/configurations/kms-env/ranger_kms_pid_dir", "/var/run/ranger_kms")
+ranger_kms_pid_file = format('{ranger_kms_pid_dir}/rangerkms.pid')
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/templates/input.config-ranger-kms.json.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/templates/input.config-ranger-kms.json.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/templates/input.config-ranger-kms.json.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/package/templates/input.config-ranger-kms.json.j2	(date 1719626239000)
@@ -0,0 +1,48 @@
+{#
+ # Licensed to the Apache Software Foundation (ASF) under one
+ # or more contributor license agreements.  See the NOTICE file
+ # distributed with this work for additional information
+ # regarding copyright ownership.  The ASF licenses this file
+ # to you under the Apache License, Version 2.0 (the
+ # "License"); you may not use this file except in compliance
+ # with the License.  You may obtain a copy of the License at
+ #
+ #   http://www.apache.org/licenses/LICENSE-2.0
+ #
+ # Unless required by applicable law or agreed to in writing, software
+ # distributed under the License is distributed on an "AS IS" BASIS,
+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ # See the License for the specific language governing permissions and
+ # limitations under the License.
+ #}
+{
+  "input":[
+    {
+      "type":"ranger_kms",
+      "rowtype":"service",
+      "path":"{{default('/configurations/kms-env/kms_log_dir', '/var/log/ranger/kms')}}/kms.log"
+    }
+  ],
+  "filter":[
+    {
+      "filter":"grok",
+      "conditions":{
+        "fields":{
+          "type":[
+            "ranger_kms"
+          ]
+        }
+      },
+      "log4j_format":"%d{ISO8601} %-5p %c{1} - %m%n",
+      "multiline_pattern":"^(%{TIMESTAMP_ISO8601:logtime})",
+      "message_pattern":"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}",
+      "post_map_values":{
+        "logtime":{
+          "map_date":{
+            "target_date_pattern":"yyyy-MM-dd HH:mm:ss,SSS"
+          }
+        }
+      }
+    }
+  ]
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/properties/kms-env.sh.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/properties/kms-env.sh.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/properties/kms-env.sh.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/properties/kms-env.sh.j2	(date 1719626239000)
@@ -0,0 +1,28 @@
+#!/bin/bash
+
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#  http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Set Ranger KMS specific environment variables here.
+
+export JAVA_HOME={{java_home}}
+export RANGER_KMS_LOG_DIR={{kms_log_dir}}
+export RANGER_KMS_PID_DIR_PATH={{ranger_kms_pid_dir}}
+export KMS_USER={{kms_user}}
+ranger_kms_max_heap_size={{ranger_kms_max_heap_size}}
+{% if security_enabled %}
+export JAVA_OPTS=" ${JAVA_OPTS} -Dzookeeper.sasl.client.username={{zookeeper_principal_primary}} "
+{% endif %}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/alerts.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/alerts.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/alerts.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/alerts.json	(date 1719626239000)
@@ -0,0 +1,32 @@
+{
+  "RANGER_KMS": {
+    "service": [],
+    "RANGER_KMS_SERVER": [
+      {
+        "name": "ranger_kms_server_process",
+        "label": "Ranger KMS Server Process",
+        "description": "This host-level alert is triggered if the Ranger KMS Server cannot be determined to be up.",
+        "interval": 1,
+        "scope": "HOST",
+        "source": {
+          "type": "PORT",
+          "uri": "{{kms-env/kms_port}}",
+          "default_port": 9292,
+          "reporting": {
+            "ok": {
+              "text": "TCP OK - {0:.3f}s response on port {1}"
+            },
+            "warning": {
+              "text": "TCP OK - {0:.3f}s response on port {1}",
+              "value": 1.5
+            },
+            "critical": {
+              "text": "Connection failed: {0} to {1}:{2}",
+              "value": 5.0
+            }
+          }
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml
new file mode 100644
--- /dev/null	(date 1723877089930)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/metainfo.xml	(date 1723877089930)
@@ -0,0 +1,118 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<metainfo>
+  <schemaVersion>2.0</schemaVersion>
+  <services>
+    <service>
+      <name>RANGER_KMS</name>
+      <displayName>Ranger KMS</displayName>
+      <comment>Component Ranger KMS Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env</comment>
+      <version>2.4.0</version>
+      <components>
+
+        <component>
+          <name>RANGER_KMS_SERVER</name>
+          <displayName>Ranger KMS Server</displayName>
+          <category>MASTER</category>
+          <cardinality>1+</cardinality>
+          <versionAdvertised>true</versionAdvertised>
+          <commandScript>
+            <script>scripts/kms_server.py</script>
+            <scriptType>PYTHON</scriptType>
+            <timeout>600</timeout>
+          </commandScript>
+          <logs>
+            <log>
+              <logId>ranger_kms</logId>
+              <primary>true</primary>
+            </log>
+          </logs>
+          <dependencies>
+            <dependency>
+              <name>HDFS/HDFS_CLIENT</name>
+              <scope>host</scope>
+              <auto-deploy>
+                <enabled>true</enabled>
+              </auto-deploy>
+            </dependency>
+          </dependencies>
+        </component>
+      </components>
+
+
+      <osSpecifics>
+        <osSpecific>
+          <osFamily>any</osFamily>
+          <packages>
+            <package>
+              <name>ranger_${stack_version}-kms</name>
+            </package>
+          </packages>
+        </osSpecific>
+      </osSpecifics>
+
+      <configuration-dependencies>
+        <config-type>kms-properties</config-type>
+        <config-type>kms-site</config-type>
+        <config-type>kms-log4j</config-type>
+        <config-type>dbks-site</config-type>
+        <config-type>ranger-kms-site</config-type>
+        <config-type>ranger-kms-audit</config-type>
+        <config-type>ranger-kms-policymgr-ssl</config-type>
+        <config-type>ranger-kms-security</config-type>
+        <config-type>kms-env</config-type>
+        <config-type>core-site</config-type>
+        <config-type>hdfs-site</config-type>
+      </configuration-dependencies>
+
+      <commandScript>
+        <script>scripts/service_check.py</script>
+        <scriptType>PYTHON</scriptType>
+        <timeout>300</timeout>
+      </commandScript>
+
+      <requiredServices>
+        <service>RANGER</service>
+        <service>HDFS</service>
+      </requiredServices>
+
+      <themes>
+        <theme>
+          <fileName>credentials.json</fileName>
+          <default>true</default>
+        </theme>
+        <theme>
+          <fileName>database.json</fileName>
+          <default>true</default>
+        </theme>
+        <theme>
+          <fileName>directories.json</fileName>
+          <default>true</default>
+        </theme>
+        <theme>
+          <fileName>theme_version_2.json</fileName>
+          <default>true</default>
+        </theme>
+      </themes>
+
+    </service>
+  </services>
+</metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-env.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-env.xml	(date 1719626239000)
@@ -0,0 +1,166 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="true">
+  <property>
+    <name>kms_user</name>
+    <display-name>Kms User</display-name>
+    <value>kms</value>
+    <property-type>USER</property-type>
+    <description>Kms username</description>
+    <value-attributes>
+      <type>user</type>
+      <overridable>false</overridable>
+      <user-groups>
+        <property>
+          <type>kms-env</type>
+          <name>kms_group</name>
+        </property>
+        <property>
+          <type>cluster-env</type>
+          <name>user_group</name>
+        </property>
+      </user-groups>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kms_group</name>
+    <display-name>Kms group</display-name>
+    <value>kms</value>
+    <property-type>GROUP</property-type>
+    <description>Kms group</description>
+    <value-attributes>
+      <type>user</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kms_log_dir</name>
+    <display-name>Ranger KMS Log Dir</display-name>
+    <value>/var/log/ranger/kms</value>
+    <description/>
+    <value-attributes>
+      <type>directory</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kms_port</name>
+    <value>9292</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>ranger-kms-site</type>
+        <name>ranger.service.https.port</name>
+      </property>
+      <property>
+        <type>ranger-kms-site</type>
+        <name>ranger.service.https.attrib.ssl.enabled</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>create_db_user</name>
+    <display-name>Setup Database and Database User</display-name>
+    <value>true</value>
+    <description>If set to Yes, Ambari will create and setup Ranger Database and Database User. This will require to specify Database Admin user and password</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hsm_partition_password</name>
+    <display-name>HSM partition password</display-name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <description>HSM partition password</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_kms_pid_dir</name>
+    <display-name>Ranger KMS PID Dir</display-name>
+    <value>/var/run/ranger_kms</value>
+    <description/>
+    <value-attributes>
+      <type>directory</type>
+      <overridable>false</overridable>
+      <editable-only-at-install>true</editable-only-at-install>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>kms-env template</display-name>
+    <description>This is the jinja template for Ranger KMS env</description>
+    <value/>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value-attributes>
+      <property-file-name>kms-env.sh.j2</property-file-name>
+      <property-file-type>text</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_kms_privelege_user_jdbc_url</name>
+    <display-name>JDBC connect string for root user</display-name>
+    <description>JDBC connect string - auto populated based on other values. This is to be used by root user</description>
+    <value>jdbc:mysql://localhost</value>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>kms-properties</type>
+        <name>DB_FLAVOR</name>
+      </property>
+      <property>
+        <type>kms-properties</type>
+        <name>db_host</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_kms_max_heap_size</name>
+    <value>1g</value>
+    <display-name>Ranger KMS max heap size.</display-name>
+    <description>Ranger KMS maximum heap size limit.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-site.xml	(date 1719626239000)
@@ -0,0 +1,107 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>hadoop.kms.key.provider.uri</name>
+    <value>dbks://http@localhost:9292/kms</value>
+    <description>URI of the backing KeyProvider for the KMS.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.security.keystore.JavaKeyStoreProvider.password</name>
+    <value>none</value>
+    <description>If using the JavaKeyStoreProvider, the password for the keystore file.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.cache.enable</name>
+    <value>true</value>
+    <description>Whether the KMS will act as a cache for the backing KeyProvider. When the cache is enabled, operations like getKeyVersion, getMetadata, and getCurrentKey will sometimes return cached data without consulting the backing KeyProvider. Cached values are flushed when keys are deleted or modified.
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.cache.timeout.ms</name>
+    <value>600000</value>
+    <description>Expiry time for the KMS key version and key metadata cache, in milliseconds. This affects getKeyVersion and getMetadata.
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.current.key.cache.timeout.ms</name>
+    <value>30000</value>
+    <description>Expiry time for the KMS current key cache, in milliseconds. This affects getCurrentKey operations.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.audit.aggregation.window.ms</name>
+    <value>10000</value>
+    <description>Duplicate audit log events within the aggregation window (specified in ms) are quashed to reduce log traffic. A single message for aggregated events is printed at the end of the window, along with a count of the number of aggregated events.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.authentication.type</name>
+    <value>simple</value>
+    <description>Authentication type for the KMS. Can be either "simple" or "kerberos".
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.authentication.kerberos.name.rules</name>
+    <value>DEFAULT</value>
+    <description>Rules used to resolve Kerberos principal names.</description>
+    <value-attributes>
+      <type>multiLine</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.authentication.signer.secret.provider</name>
+    <value>random</value>
+    <description>Indicates how the secret to sign the authentication cookies will be stored. Options are 'random' (default), 'file' and 'zookeeper'. If using a setup with multiple KMS instances, 'zookeeper' should be used.
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.authentication.signer.secret.provider.zookeeper.path</name>
+    <value>/hadoop-kms/hadoop-auth-signature-secret</value>
+    <description>The Zookeeper ZNode path where the KMS instances will store and retrieve the secret from.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.authentication.signer.secret.provider.zookeeper.connection.string</name>
+    <value>#HOSTNAME#:#PORT#,...</value>
+    <description>The Zookeeper connection string, a list of hostnames and port comma separated.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.authentication.signer.secret.provider.zookeeper.auth.type</name>
+    <value>none</value>
+    <description>The Zookeeper authentication type, 'none' or 'sasl'. The 'none' is used for authentication using 'Client' loginContext &amp; 'sasl' is used for authentication using 'ZKSignerSecretProviderClient' loginContext.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.kms.security.authorization.manager</name>
+    <value>org.apache.ranger.authorization.kms.authorizer.RangerKmsAuthorizer</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/dbks-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/dbks-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/dbks-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/dbks-site.xml	(date 1719626239000)
@@ -0,0 +1,315 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>hadoop.kms.blacklist.DECRYPT_EEK</name>
+    <value>hdfs</value>
+    <description>Blacklist for decrypt EncryptedKey CryptoExtension operations</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.db.encrypt.key.password</name>
+    <value>_</value>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <description>Password used for encrypting Master Key</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.url</name>
+    <display-name>JDBC connect string</display-name>
+    <value>jdbc:mysql://localhost</value>
+    <description>URL for Database</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>kms-properties</type>
+        <name>DB_FLAVOR</name>
+      </property>
+      <property>
+        <type>kms-properties</type>
+        <name>db_host</name>
+      </property>
+      <property>
+        <type>kms-properties</type>
+        <name>db_name</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.user</name>
+    <value>{{db_user}}</value>
+    <description>Database username used for operation</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.password</name>
+    <value>_</value>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <description>Database user's password</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.credential.provider.path</name>
+    <value>/etc/ranger/kms/rangerkms.jceks</value>
+    <description>Credential provider path</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.credential.alias</name>
+    <value>ranger.ks.jdbc.password</value>
+    <description>Credential alias used for password</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.masterkey.credential.alias</name>
+    <value>ranger.ks.masterkey.password</value>
+    <description>Credential alias used for masterkey</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.dialect</name>
+    <value>{{jdbc_dialect}}</value>
+    <description>Dialect used for database</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jpa.jdbc.driver</name>
+    <display-name>Driver class name for a JDBC Ranger KMS database</display-name>
+    <value>com.mysql.jdbc.Driver</value>
+    <description>Driver used for database</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>kms-properties</type>
+        <name>DB_FLAVOR</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.jdbc.sqlconnectorjar</name>
+    <value>{{ews_lib_jar_path}}</value>
+    <description>Driver used for database</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.hsm.type</name>
+    <display-name>HSM Type</display-name>
+    <value>LunaProvider</value>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>LunaProvider</value>
+          <label>Luna Provider</label>
+        </entry>
+      </entries>
+    </value-attributes>
+    <description>HSM type</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.hsm.enabled</name>
+    <display-name>HSM Enabled</display-name>
+    <value>false</value>
+    <description>Enable HSM ?</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.hsm.partition.name</name>
+    <display-name>HSM partition name. In case of HSM HA enter the group name</display-name>
+    <value>par19</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.hsm.partition.password</name>
+    <value>_</value>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <description>HSM partition password</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.hsm.partition.password.alias</name>
+    <display-name>HSM partition password alias</display-name>
+    <value>ranger.kms.hsm.partition.password</value>
+    <description>HSM partition password alias</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.kerberos.principal</name>
+    <value/>
+    <description/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.ks.kerberos.keytab</name>
+    <value/>
+    <description/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+
+
+   <property>
+    <name>ranger.kms.keysecure.enabled</name>
+    <display-name>Keysecure Enabled</display-name>
+    <value>false</value>
+    <description>Enable Keysecure ?</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.UserPassword.Authentication</name>
+    <display-name>Enable Keysecure User Password Authentication</display-name>
+    <value>true</value>
+    <description>Enable Keysecure User Password Authentication ?</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.masterkey.name</name>
+    <display-name>Keysecure MasterKey Name</display-name>
+    <value></value>
+    <description>Enter Keysecure MasterKey Name</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.login.username</name>
+    <display-name>Keysecure Login Username</display-name>
+    <value></value>
+    <description>Enter Keysecure Login Username</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.login.password</name>
+    <display-name>Keysecure Login Password</display-name>
+    <value></value>
+    <property-type>PASSWORD</property-type>
+    <description>Keysecure Login Password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.login.password.alias</name>
+    <display-name>Keysecure Login Password Alias</display-name>
+    <value>ranger.ks.login.password</value>
+    <description>Enter Keysecure Login Password Alias</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.hostname</name>
+    <display-name>Keysecure Hostname</display-name>
+    <value></value>
+    <description>Enter Keysecure Hostname</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.masterkey.size</name>
+    <display-name>Keysecure Masterkey Size</display-name>
+    <value>256</value>
+    <description>Enter Keysecure Masterkey Size</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.kms.keysecure.sunpkcs11.cfg.filepath</name>
+    <display-name>Keysecure sunpkcs11 cfg filepath</display-name>
+    <value></value>
+    <description>Enter Keysecure sunpkcs11 cfg filepath</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-log4j.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-log4j.xml	(date 1719626239000)
@@ -0,0 +1,118 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="false">
+   <property>
+    <name>ranger_kms_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Ranger-kms Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+   </property>
+   <property>
+    <name>ranger_kms_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Ranger-kms Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_kms_audit_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Ranger-kms Audit Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+   </property>
+   <property>
+    <name>ranger_kms_audit_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Ranger-kms Audit Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>kms-log4j template</display-name>
+    <description>kms-log4j.properties</description>
+    <value>
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License. See accompanying LICENSE file.
+#
+
+# If the Java System property 'kms.log.dir' is not defined at KMS start up time
+# Setup sets its value to '${kms.home}/logs'
+
+log4j.appender.kms=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.kms.DatePattern='.'yyyy-MM-dd
+log4j.appender.kms.File=${kms.log.dir}/kms.log
+log4j.appender.kms.Append=true
+log4j.appender.kms.layout=org.apache.log4j.PatternLayout
+log4j.appender.kms.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
+log4j.appender.kms.MaxFileSize={{ranger_kms_log_maxfilesize}}MB
+
+log4j.appender.kms-audit=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.kms-audit.DatePattern='.'yyyy-MM-dd
+log4j.appender.kms-audit.File=${kms.log.dir}/kms-audit.log
+log4j.appender.kms-audit.Append=true
+log4j.appender.kms-audit.layout=org.apache.log4j.PatternLayout
+log4j.appender.kms-audit.layout.ConversionPattern=%d{ISO8601} %m%n
+log4j.appender.kms-audit.MaxFileSize={{ranger_kms_audit_log_maxfilesize}}MB
+
+log4j.logger.kms-audit=INFO, kms-audit
+log4j.additivity.kms-audit=false
+
+log4j.logger=INFO, kms
+log4j.additivity.kms=false
+log4j.rootLogger=INFO, kms
+log4j.logger.org.apache.hadoop.conf=ERROR
+log4j.logger.org.apache.hadoop=INFO
+log4j.logger.com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator=OFF
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/kms-properties.xml	(date 1719626239000)
@@ -0,0 +1,166 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>REPOSITORY_CONFIG_USERNAME</name>
+    <display-name>Repository config username</display-name>
+    <value>keyadmin</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_PASSWORD</name>
+    <display-name>Repository config password</display-name>
+    <value>keyadmin</value>
+    <property-type>PASSWORD</property-type>
+    <description/>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>DB_FLAVOR</name>
+    <display-name>DB FLAVOR</display-name>
+    <value>MYSQL</value>
+    <description>The database type to be used</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>MYSQL</value>
+          <label>MYSQL</label>
+        </entry>
+        <entry>
+          <value>ORACLE</value>
+          <label>ORACLE</label>
+        </entry>
+        <entry>
+          <value>POSTGRES</value>
+          <label>POSTGRES</label>
+        </entry>
+        <entry>
+          <value>MSSQL</value>
+          <label>MSSQL</label>
+        </entry>
+        <entry>
+          <value>SQLA</value>
+          <label>SQL Anywhere</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>SQL_CONNECTOR_JAR</name>
+    <display-name>SQL connector jar</display-name>
+    <value>{{driver_curl_target}}</value>
+    <description>Location of DB client library (please check the location of the jar file)</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>kms-properties</type>
+        <name>DB_FLAVOR</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false" update="false"/>
+  </property>
+  <property>
+    <name>db_root_user</name>
+    <display-name>Database Administrator (DBA) username</display-name>
+    <value>root</value>
+    <description>Database admin user. This user should have DBA permission to create the Ranger Database and Ranger Database User</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_root_password</name>
+    <display-name>Database Administrator (DBA) password</display-name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <description>Database password for the database admin username</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_host</name>
+    <display-name>Ranger KMS DB host</display-name>
+    <value/>
+    <description>Database host</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_name</name>
+    <display-name>Ranger KMS DB name</display-name>
+    <value>rangerkms</value>
+    <description>Database name</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_user</name>
+    <display-name>Ranger KMS DB username</display-name>
+    <value>rangerkms</value>
+    <description>Database username used for the Ranger KMS schema</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_password</name>
+    <display-name>Ranger KMS DB password</display-name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <description>Database password for the Ranger KMS schema</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>KMS_MASTER_KEY_PASSWD</name>
+    <display-name>KMS master key password</display-name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <description/>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-site.xml	(date 1719626239000)
@@ -0,0 +1,110 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>ranger.service.host</name>
+    <value>{{kms_host}}</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.service.http.port</name>
+    <value>{{kms_port}}</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.service.https.port</name>
+    <value>9393</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.service.shutdown.port</name>
+    <value>7085</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.contextName</name>
+    <value>/kms</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xa.webapp.dir</name>
+    <value>./webapp</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.service.https.attrib.ssl.enabled</name>
+    <value>false</value>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.service.https.attrib.keystore.file</name>
+    <value>/etc/security/serverKeys/ranger-kms-keystore.jks</value>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+  </property>
+  <property>
+    <name>ranger.service.https.attrib.client.auth</name>
+    <value>want</value>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+  </property>
+  <property>
+    <name>ranger.service.https.attrib.keystore.keyalias</name>
+    <value>rangerkms</value>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+  </property>
+  <property>
+    <name>ranger.service.https.attrib.keystore.pass</name>
+    <value>rangerkms</value>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+  </property>
+  <property>
+    <name>ranger.credential.provider.path</name>
+    <value>/etc/ranger/kms/rangerkms.jceks</value>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+  </property>
+  <property>
+    <name>ranger.service.https.attrib.keystore.credential.alias</name>
+    <value>keyStoreCredentialAlias</value>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+  </property>
+  <property>
+    <name>ajp.enabled</name>
+    <value>false</value>
+    <on-ambari-upgrade add="false"/>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/alerts.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/alerts.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/alerts.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/alerts.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/alerts.json	(date 1719626239000)
@@ -29,4 +29,4 @@
       }
     ]
   }
-}
\ No newline at end of file
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-audit.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-audit.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-audit.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-audit.xml	(date 1719626239000)
@@ -0,0 +1,118 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.audit.is.enabled</name>
+    <value>true</value>
+    <description>Is Audit enabled?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs</name>
+    <value>true</value>
+    <display-name>Audit to HDFS</display-name>
+    <description>Is Audit to HDFS enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.dir</name>
+    <value>hdfs://NAMENODE_HOSTNAME:8020/ranger/audit</value>
+    <description>HDFS folder to write audit to, make sure the service user has requried permissions</description>
+    <depends-on>
+      <property>
+        <type>core-site</type>
+        <name>fs.defaultFS</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.batch.filespool.dir</name>
+    <value>/var/log/ranger/kms/audit/hdfs/spool</value>
+    <description>/var/log/ranger/kms/audit/hdfs/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr</name>
+    <value>true</value>
+    <display-name>Audit to SOLR</display-name>
+    <description>Is Solr audit enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.batch.filespool.dir</name>
+    <value>/var/log/ranger/kms/audit/solr/spool</value>
+    <description>/var/log/ranger/kms/audit/solr/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.provider.summary.enabled</name>
+    <value>false</value>
+    <display-name>Audit provider summary enabled</display-name>
+    <description>Enable Summary audit?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.urls</name>
+    <value>{{ranger_audit_solr_urls}}</value>
+    <description>Solr URL</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.urls</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.zookeepers</name>
+    <value>none</value>
+    <description>Solr Zookeeper string</description>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.zookeepers</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kms.ambari.cluster.name</name>
+    <value>{{cluster_name}}</value>
+    <description>Capture cluster name from where Ranger kms plugin is enabled.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metainfo.xml	(date 1719626239000)
@@ -16,99 +16,105 @@
    limitations under the License.
 -->
 <metainfo>
-  <schemaVersion>2.0</schemaVersion>
-  <services>
-    <service>
-      <name>KAFKA</name>
-      <displayName>Kafka</displayName>
-      <comment>A high-throughput distributed messaging system</comment>
-      <version>2.8.1-2</version>
-
-      <components>
-        <component>
-          <name>KAFKA_BROKER</name>
-          <displayName>Kafka Broker</displayName>
-          <category>MASTER</category>
-          <cardinality>1+</cardinality>
-          <versionAdvertised>true</versionAdvertised>
-          <rollingRestartSupported>true</rollingRestartSupported>
-          <timelineAppid>kafka_broker</timelineAppid>
-          <dependencies>
-            <dependency>
-              <name>ZOOKEEPER/ZOOKEEPER_SERVER</name>
-              <scope>cluster</scope>
-              <auto-deploy>
-                <enabled>true</enabled>
-              </auto-deploy>
-            </dependency>
-          </dependencies>
-          <commandScript>
-            <script>scripts/kafka_broker.py</script>
-            <scriptType>PYTHON</scriptType>
-            <timeout>1200</timeout>
-          </commandScript>
-          <logs>
-            <log>
-              <logId>kafka_server</logId>
-              <primary>true</primary>
-            </log>
-            <log>
-              <logId>kafka_controller</logId>
-            </log>
-            <log>
-              <logId>kafka_request</logId>
-            </log>
-            <log>
-              <logId>kafka_logcleaner</logId>
-            </log>
-            <log>
-              <logId>kafka_statechange</logId>
-            </log>
-          </logs>
-        </component>
-      </components>
-
-      <commandScript>
-        <script>scripts/service_check.py</script>
-        <scriptType>PYTHON</scriptType>
-        <timeout>300</timeout>
-      </commandScript>
-
-      <requiredServices>
-        <service>ZOOKEEPER</service>
-      </requiredServices>
-
-      <configuration-dependencies>
-        <config-type>kafka-broker</config-type>
-        <config-type>kafka-env</config-type>
-        <config-type>kafka-log4j</config-type>
-        <config-type>zookeeper-env</config-type>
-        <config-type>zoo.cfg</config-type>
-        <config-type>kafka_jaas_conf</config-type>
-        <config-type>kafka_client_jaas_conf</config-type>
-      </configuration-dependencies>
-
-      <osSpecifics>
-        <osSpecific>
-          <osFamily>redhat7</osFamily>
-          <packages>
-            <package>
-              <name>kafka_${stack_version}</name>
-            </package>
-          </packages>
-        </osSpecific>
-      </osSpecifics>
-
-      <restartRequiredAfterChange>true</restartRequiredAfterChange>
-      <restartRequiredAfterRackChange>true</restartRequiredAfterRackChange>
+    <schemaVersion>2.0</schemaVersion>
+    <services>
+        <service>
+            <name>KAFKA</name>
+            <displayName>Kafka</displayName>
+            <comment>A high-throughput distributed messaging system</comment>
+            <version>1.0.0.3.0</version>
+            <components>
+                <component>
+                    <name>KAFKA_BROKER</name>
+                    <displayName>Kafka Broker</displayName>
+                    <category>MASTER</category>
+                    <cardinality>1+</cardinality>
+                    <versionAdvertised>true</versionAdvertised>
+                    <rollingRestartSupported>true</rollingRestartSupported>
+                    <timelineAppid>kafka_broker</timelineAppid>
+                    <dependencies>
+                        <dependency>
+                            <name>ZOOKEEPER/ZOOKEEPER_SERVER</name>
+                            <scope>cluster</scope>
+                            <auto-deploy>
+                                <enabled>true</enabled>
+                            </auto-deploy>
+                        </dependency>
+                    </dependencies>
+                    <commandScript>
+                        <script>scripts/kafka_broker.py</script>
+                        <scriptType>PYTHON</scriptType>
+                        <timeout>1200</timeout>
+                    </commandScript>
+                    <logs>
+                        <log>
+                            <logId>kafka_server</logId>
+                            <primary>true</primary>
+                        </log>
+                        <log>
+                            <logId>kafka_controller</logId>
+                        </log>
+                        <log>
+                            <logId>kafka_request</logId>
+                        </log>
+                        <log>
+                            <logId>kafka_logcleaner</logId>
+                        </log>
+                        <log>
+                            <logId>kafka_statechange</logId>
+                        </log>
+                    </logs>
+                </component>
+            </components>
+            <commandScript>
+                <script>scripts/service_check.py</script>
+                <scriptType>PYTHON</scriptType>
+                <timeout>300</timeout>
+            </commandScript>
+            <requiredServices>
+                <service>ZOOKEEPER</service>
+            </requiredServices>
+            <configuration-dependencies>
+                <config-type>kafka-broker</config-type>
+                <config-type>kafka-env</config-type>
+                <config-type>kafka-log4j</config-type>
+                <config-type>ranger-kafka-plugin-properties</config-type>
+                <config-type>ranger-kafka-audit</config-type>
+                <config-type>ranger-kafka-policymgr-ssl</config-type>
+                <config-type>ranger-kafka-security</config-type>
+                <config-type>zookeeper-env</config-type>
+                <config-type>zoo.cfg</config-type>
+                <config-type>kafka_jaas_conf</config-type>
+                <config-type>kafka_client_jaas_conf</config-type>
+            </configuration-dependencies>
+            <osSpecifics>
+                <osSpecific>
+                    <osFamily>redhat9,redhat8,redhat7,amazonlinux2,redhat6,suse11,suse12</osFamily>
+                    <packages>
+                        <package>
+                            <name>kafka_${stack_version}</name>
+                        </package>
+                    </packages>
+                </osSpecific>
+                <osSpecific>
+                    <osFamily>debian7,debian9,ubuntu12,ubuntu14,ubuntu16,ubuntu18</osFamily>
+                    <packages>
+                        <package>
+                            <name>kafka-${stack_version}</name>
+                        </package>
+                    </packages>
+                </osSpecific>
+            </osSpecifics>
+            <restartRequiredAfterChange>true</restartRequiredAfterChange>
+            <restartRequiredAfterRackChange>true</restartRequiredAfterRackChange>
 
-      <themes>
-        <theme>
-          <fileName>directories.json</fileName>
-          <default>true</default>
-        </theme>
-      </themes>
+            <themes>
+                <theme>
+                    <fileName>directories.json</fileName>
+                    <default>true</default>
+                </theme>
+            </themes>
 
-    </service>
-  </services>
-</metainfo>
\ No newline at end of file
+        </service>
+    </services>
+</metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-security.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-security.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-security.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-security.xml	(date 1719626239000)
@@ -0,0 +1,64 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>ranger.plugin.kms.service.name</name>
+    <value>{{repo_name}}</value>
+    <description>Name of the Ranger service containing policies for this kms instance</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kms.policy.source.impl</name>
+    <value>org.apache.ranger.admin.client.RangerAdminRESTClient</value>
+    <description>Class to retrieve policies from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kms.policy.rest.url</name>
+    <value>{{policymgr_mgr_url}}</value>
+    <description>URL to Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>policymgr_external_url</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>ranger.plugin.kms.policy.rest.ssl.config.file</name>
+    <value>/etc/ranger/kms/conf/ranger-policymgr-ssl.xml</value>
+    <description>Path to the file containing SSL details to contact Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kms.policy.pollIntervalMs</name>
+    <value>30000</value>
+    <description>How often to poll for changes in policies?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kms.policy.cache.dir</name>
+    <value>/etc/ranger/{{repo_name}}/policycache</value>
+    <description>Directory where Ranger policies are cached after successful retrieval from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metrics.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metrics.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metrics.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metrics.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/metrics.json	(date 1719626239000)
@@ -200,6 +200,7 @@
               "pointInTime": true,
               "temporal": true
             },
+
             "metrics/kafka/server/ReplicaFetcherManager/Replica-MaxLag": {
               "metric": "kafka.server.ReplicaFetcherManager.MaxLag.clientId.Replica",
               "pointInTime": true,
@@ -235,4 +236,4 @@
       }
     ]
   }
-}
\ No newline at end of file
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-policymgr-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-policymgr-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-policymgr-ssl.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/configuration/ranger-kms-policymgr-ssl.xml	(date 1719626239000)
@@ -0,0 +1,74 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <description>password for keystore</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <description>java truststore password</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/widgets.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/widgets.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/widgets.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/widgets.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/widgets.json	(date 1719626239000)
@@ -175,7 +175,8 @@
             "time_range": "1"
           }
         }
+
       ]
     }
   ]
-}
\ No newline at end of file
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/kerberos.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/kerberos.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/kerberos.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/kerberos.json	(date 1719626239000)
@@ -0,0 +1,81 @@
+{
+  "services": [
+    {
+      "name": "RANGER_KMS",
+      "identities": [
+        {
+          "name": "ranger_kms_spnego",
+          "reference": "/spnego",
+          "keytab": {
+            "configuration": "kms-site/hadoop.kms.authentication.kerberos.keytab"
+          }
+        }
+      ],
+      "auth_to_local_properties" : [
+        "kms-site/hadoop.kms.authentication.kerberos.name.rules"
+      ],
+      "configurations": [
+        {
+          "kms-site": {
+            "hadoop.kms.authentication.type": "kerberos",
+            "hadoop.kms.authentication.kerberos.principal": "*"
+          }
+        },
+        {
+          "ranger-kms-audit": {
+            "xasecure.audit.jaas.Client.loginModuleName": "com.sun.security.auth.module.Krb5LoginModule",
+            "xasecure.audit.jaas.Client.loginModuleControlFlag": "required",
+            "xasecure.audit.jaas.Client.option.useKeyTab": "true",
+            "xasecure.audit.jaas.Client.option.storeKey": "false",
+            "xasecure.audit.jaas.Client.option.serviceName": "solr",
+            "xasecure.audit.destination.solr.force.use.inmemory.jaas.config": "true"
+          }
+        }
+      ],
+      "components": [
+        {
+          "name": "RANGER_KMS_SERVER",
+          "identities": [
+            {
+              "name": "ranger_kms_ranger_kms_server_spnego",
+              "reference": "/spnego",
+              "principal": {
+                "configuration": "kms-site/hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.principal"
+              },
+              "keytab": {
+                "configuration": "kms-site/hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.keytab"
+              }
+            },
+            {
+              "name": "rangerkms",
+              "principal": {
+                "value": "rangerkms/_HOST@${realm}",
+                "type" : "service",
+                "configuration": "dbks-site/ranger.ks.kerberos.principal",
+                "local_username" : "keyadmin"
+              },
+              "keytab": {
+                "file": "${keytab_dir}/rangerkms.service.keytab",
+                "owner": {
+                  "name": "${kms-env/kms_user}",
+                  "access": "r"
+                },
+                "configuration": "dbks-site/ranger.ks.kerberos.keytab"
+              }
+            },
+            {
+              "name": "ranger_kms_ranger_kms_server_rangerkms",
+              "reference": "/RANGER_KMS/RANGER_KMS_SERVER/rangerkms",
+              "principal": {
+                "configuration": "ranger-kms-audit/xasecure.audit.jaas.Client.option.principal"
+              },
+              "keytab": {
+                "configuration": "ranger-kms-audit/xasecure.audit.jaas.Client.option.keyTab"
+              }
+            }
+          ]
+        }
+      ]
+    }
+  ]
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/kerberos.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/kerberos.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/kerberos.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/kerberos.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/kerberos.json	(date 1719626239000)
@@ -11,15 +11,25 @@
       "configurations": [
         {
           "kafka-broker": {
-            "authorizer.class.name": "kafka.security.auth.SimpleAclAuthorizer",
-            "principal.to.local.class": "kafka.security.auth.KerberosPrincipalToLocal",
-            "super.users": "user:${kafka-env/kafka_user}",
-            "security.inter.broker.protocol": "SASL_PLAINTEXT",
-            "zookeeper.set.acl": "true"
+              "authorizer.class.name": "kafka.security.auth.SimpleAclAuthorizer",
+              "principal.to.local.class":"kafka.security.auth.KerberosPrincipalToLocal",
+              "super.users": "user:${kafka-env/kafka_user}",
+              "security.inter.broker.protocol": "SASL_PLAINTEXT",
+              "zookeeper.set.acl": "true"
           }
+        },
+        {
+          "ranger-kafka-audit": {
+            "xasecure.audit.jaas.Client.loginModuleName": "com.sun.security.auth.module.Krb5LoginModule",
+            "xasecure.audit.jaas.Client.loginModuleControlFlag": "required",
+            "xasecure.audit.jaas.Client.option.useKeyTab": "true",
+            "xasecure.audit.jaas.Client.option.storeKey": "false",
+            "xasecure.audit.jaas.Client.option.serviceName": "solr",
+            "xasecure.audit.destination.solr.force.use.inmemory.jaas.config": "true"
+          }
         }
       ],
-      "auth_to_local_properties": [
+      "auth_to_local_properties" : [
         "kafka-broker/sasl.kerberos.principal.to.local.rules|comma"
       ],
       "components": [
@@ -47,13 +57,20 @@
               }
             },
             {
+              "name": "kafka_kafka_broker_kafka_broker",
+              "reference": "/KAFKA/KAFKA_BROKER/kafka_broker",
+              "principal": {
+                "configuration": "ranger-kafka-audit/xasecure.audit.jaas.Client.option.principal"
+              },
+              "keytab": {
+                "configuration": "ranger-kafka-audit/xasecure.audit.jaas.Client.option.keyTab"
+              }
+            },
+            {
               "name": "kafka_kafka_broker_hdfs",
               "reference": "/HDFS/NAMENODE/hdfs",
-              "when": {
-                "contains": [
-                  "services",
-                  "HDFS"
-                ]
+              "when" : {
+                "contains" : ["services", "HDFS"]
               }
             }
           ]
@@ -61,4 +78,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/service_advisor.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/service_advisor.py	(date 1719626239000)
@@ -0,0 +1,407 @@
+#!/usr/bin/env ambari-python-wrap
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+# Python imports
+import imp
+import os
+import traceback
+import re
+import socket
+import fnmatch
+
+
+from resource_management.core.logger import Logger
+
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../../../stacks/')
+PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+
+try:
+  if "BASE_SERVICE_ADVISOR" in os.environ:
+    PARENT_FILE = os.environ["BASE_SERVICE_ADVISOR"]
+  with open(PARENT_FILE, 'rb') as fp:
+    service_advisor = imp.load_module('service_advisor', fp, PARENT_FILE, ('.py', 'rb', imp.PY_SOURCE))
+except Exception as e:
+  traceback.print_exc()
+  print "Failed to load parent"
+
+DB_TYPE_DEFAULT_PORT_MAP = {"MYSQL":"3306", "ORACLE":"1521", "POSTGRES":"5432", "MSSQL":"1433", "SQLA":"2638"}
+
+class Ranger_KMSServiceAdvisor(service_advisor.ServiceAdvisor):
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(Ranger_KMSServiceAdvisor, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    # Always call these methods
+    self.modifyMastersWithMultipleInstances()
+    self.modifyCardinalitiesDict()
+    self.modifyHeapSizeProperties()
+    self.modifyNotValuableComponents()
+    self.modifyComponentsNotPreferableOnServer()
+    self.modifyComponentLayoutSchemes()
+
+  def modifyMastersWithMultipleInstances(self):
+    """
+    Modify the set of masters with multiple instances.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyCardinalitiesDict(self):
+    """
+    Modify the dictionary of cardinalities.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyHeapSizeProperties(self):
+    """
+    Modify the dictionary of heap size properties.
+    Must be overriden in child class.
+    """
+    pass
+
+  def modifyNotValuableComponents(self):
+    """
+    Modify the set of components whose host assignment is based on other services.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentsNotPreferableOnServer(self):
+    """
+    Modify the set of components that are not preferable on the server.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentLayoutSchemes(self):
+    """
+    Modify layout scheme dictionaries for components.
+    The scheme dictionary basically maps the number of hosts to
+    host index where component should exist.
+    Must be overriden in child class.
+    """
+
+    self.componentLayoutSchemes.update({'RANGER_KMS_SERVER': {3: 0, 6: 1, 31: 2, "else": 2}})
+
+  def getServiceComponentLayoutValidations(self, services, hosts):
+    """
+    Get a list of errors.
+    Must be overriden in child class.
+    """
+
+    return self.getServiceComponentCardinalityValidations(services, hosts, "RANGER_KMS")
+
+  def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overriden in child class.
+    """
+    # Logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    recommender = RangerKMSRecommender()
+    recommender.recommendRangerKMSConfigurationsFromHDP23(configurations, clusterData, services, hosts)
+    recommender.recommendRangerKMSConfigurationsFromHDP25(configurations, clusterData, services, hosts)
+    recommender.recommendRangerKMSConfigurationsFromHDP26(configurations, clusterData, services, hosts)
+    recommender.recommendRangerKMSConfigurationsFromHDP30(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+    """
+    Entry point.
+    Validate configurations for the service. Return a list of errors.
+    The code for this function should be the same for each Service Advisor.
+    """
+    # Logger.info("Class: %s, Method: %s. Validating Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    validator = RangerKMSValidator()
+    # Calls the methods of the validator using arguments,
+    # method(siteProperties, siteRecommendations, configurations, services, hosts)
+    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
+
+  @staticmethod
+  def isKerberosEnabled(services, configurations):
+    """
+    Determines if security is enabled by testing the value of kms-site/hadoop.kms.authentication.type enabled.
+    If the property exists and is equal to "kerberos", then is it enabled; otherwise is it assumed to be
+    disabled.
+
+    :type services: dict
+    :param services: the dictionary containing the existing configuration values
+    :type configurations: dict
+    :param configurations: the dictionary containing the updated configuration values
+    :rtype: bool
+    :return: True or False
+    """
+    if configurations and "kms-site" in configurations and \
+            "hadoop.kms.authentication.type" in configurations["kms-site"]["properties"]:
+      return configurations["kms-site"]["properties"]["hadoop.kms.authentication.type"].lower() == "kerberos"
+    elif services and "kms-site" in services["configurations"] and \
+            "hadoop.kms.authentication.type" in services["configurations"]["kms-site"]["properties"]:
+      return services["configurations"]["kms-site"]["properties"]["hadoop.kms.authentication.type"].lower() == "kerberos"
+    else:
+      return False
+
+
+class RangerKMSRecommender(service_advisor.ServiceAdvisor):
+  """
+  RangerKMS Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(RangerKMSRecommender, self)
+    self.as_super.__init__(*args, **kwargs)
+
+  def recommendRangerKMSConfigurationsFromHDP23(self, configurations, clusterData, services, hosts):
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    putRangerKmsDbksProperty = self.putProperty(configurations, "dbks-site", services)
+    putRangerKmsProperty = self.putProperty(configurations, "kms-properties", services)
+    kmsEnvProperties = self.getSiteProperties(services['configurations'], 'kms-env')
+    putCoreSiteProperty = self.putProperty(configurations, "core-site", services)
+    putCoreSitePropertyAttribute = self.putPropertyAttribute(configurations, "core-site")
+    putRangerKmsAuditProperty = self.putProperty(configurations, "ranger-kms-audit", services)
+    security_enabled = Ranger_KMSServiceAdvisor.isKerberosEnabled(services, configurations)
+    putRangerKmsSiteProperty = self.putProperty(configurations, "kms-site", services)
+    putRangerKmsSitePropertyAttribute = self.putPropertyAttribute(configurations, "kms-site")
+
+    if 'kms-properties' in services['configurations'] and ('DB_FLAVOR' in services['configurations']['kms-properties']['properties']):
+
+      rangerKmsDbFlavor = services['configurations']["kms-properties"]["properties"]["DB_FLAVOR"]
+
+      if ('db_host' in services['configurations']['kms-properties']['properties']) and ('db_name' in services['configurations']['kms-properties']['properties']):
+
+        rangerKmsDbHost =   services['configurations']["kms-properties"]["properties"]["db_host"]
+        rangerKmsDbName =   services['configurations']["kms-properties"]["properties"]["db_name"]
+
+        ranger_kms_db_url_dict = {
+          'MYSQL': {'ranger.ks.jpa.jdbc.driver': 'com.mysql.jdbc.Driver',
+                    'ranger.ks.jpa.jdbc.url': 'jdbc:mysql://' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + '/' + rangerKmsDbName},
+          'ORACLE': {'ranger.ks.jpa.jdbc.driver': 'oracle.jdbc.driver.OracleDriver',
+                     'ranger.ks.jpa.jdbc.url': 'jdbc:oracle:thin:@' + self.getOracleDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost, rangerKmsDbName)},
+          'POSTGRES': {'ranger.ks.jpa.jdbc.driver': 'org.postgresql.Driver',
+                       'ranger.ks.jpa.jdbc.url': 'jdbc:postgresql://' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + '/' + rangerKmsDbName},
+          'MSSQL': {'ranger.ks.jpa.jdbc.driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver',
+                    'ranger.ks.jpa.jdbc.url': 'jdbc:sqlserver://' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + ';databaseName=' + rangerKmsDbName},
+          'SQLA': {'ranger.ks.jpa.jdbc.driver': 'sap.jdbc4.sqlanywhere.IDriver',
+                   'ranger.ks.jpa.jdbc.url': 'jdbc:sqlanywhere:host=' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + ';database=' + rangerKmsDbName}
+        }
+
+        rangerKmsDbProperties = ranger_kms_db_url_dict.get(rangerKmsDbFlavor, ranger_kms_db_url_dict['MYSQL'])
+        for key in rangerKmsDbProperties:
+          putRangerKmsDbksProperty(key, rangerKmsDbProperties.get(key))
+
+    if kmsEnvProperties and self.checkSiteProperties(kmsEnvProperties, 'kms_user') and 'KERBEROS' in servicesList:
+      kmsUser = kmsEnvProperties['kms_user']
+      kmsUserOld = self.getOldValue(services, 'kms-env', 'kms_user')
+      self.put_proxyuser_value(kmsUser, '*', is_groups=True, services=services, configurations=configurations, put_function=putCoreSiteProperty)
+      if kmsUserOld is not None and kmsUser != kmsUserOld:
+        putCoreSitePropertyAttribute("hadoop.proxyuser.{0}.groups".format(kmsUserOld), 'delete', 'true')
+        services["forced-configurations"].append({"type" : "core-site", "name" : "hadoop.proxyuser.{0}.groups".format(kmsUserOld)})
+        services["forced-configurations"].append({"type" : "core-site", "name" : "hadoop.proxyuser.{0}.groups".format(kmsUser)})
+
+    if "HDFS" in servicesList:
+      if 'core-site' in services['configurations'] and ('fs.defaultFS' in services['configurations']['core-site']['properties']):
+        default_fs = services['configurations']['core-site']['properties']['fs.defaultFS']
+        putRangerKmsAuditProperty('xasecure.audit.destination.hdfs.dir', '{0}/{1}/{2}'.format(default_fs,'ranger','audit'))
+
+    required_services = [{'service' : 'YARN', 'config-type': 'yarn-env', 'property-name': 'yarn_user', 'proxy-category': ['hosts', 'users', 'groups']},
+                         {'service' : 'SPARK', 'config-type': 'livy-env', 'property-name': 'livy_user', 'proxy-category': ['hosts', 'users', 'groups']}]
+
+    required_services_for_secure = [{'service' : 'HIVE', 'config-type': 'hive-env', 'property-name': 'hive_user', 'proxy-category': ['hosts', 'users']},
+                                    {'service' : 'OOZIE', 'config-type': 'oozie-env', 'property-name': 'oozie_user', 'proxy-category': ['hosts', 'users']}]
+
+    if security_enabled:
+      required_services.extend(required_services_for_secure)
+
+    # recommendations for kms proxy related properties
+    self.recommendKMSProxyUsers(configurations, services, hosts, required_services)
+
+    ambari_user = self.getAmbariUser(services)
+    if security_enabled:
+      # adding for ambari user
+      putRangerKmsSiteProperty('hadoop.kms.proxyuser.{0}.users'.format(ambari_user), '*')
+      putRangerKmsSiteProperty('hadoop.kms.proxyuser.{0}.hosts'.format(ambari_user), '*')
+      # adding for HTTP
+      putRangerKmsSiteProperty('hadoop.kms.proxyuser.HTTP.users', '*')
+      putRangerKmsSiteProperty('hadoop.kms.proxyuser.HTTP.hosts', '*')
+    else:
+      self.deleteKMSProxyUsers(configurations, services, hosts, required_services_for_secure)
+      # deleting ambari user proxy properties
+      putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.hosts'.format(ambari_user), 'delete', 'true')
+      putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.users'.format(ambari_user), 'delete', 'true')
+      # deleting HTTP proxy properties
+      putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.HTTP.hosts', 'delete', 'true')
+      putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.HTTP.users', 'delete', 'true')
+
+  def recommendRangerKMSConfigurationsFromHDP25(self, configurations, clusterData, services, hosts):
+
+    security_enabled = Ranger_KMSServiceAdvisor.isKerberosEnabled(services, configurations)
+    required_services = [{'service' : 'RANGER', 'config-type': 'ranger-env', 'property-name': 'ranger_user', 'proxy-category': ['hosts', 'users', 'groups']},
+                        {'service' : 'SPARK2', 'config-type': 'livy2-env', 'property-name': 'livy2_user', 'proxy-category': ['hosts', 'users', 'groups']}]
+
+    if security_enabled:
+      # recommendations for kms proxy related properties
+      self.recommendKMSProxyUsers(configurations, services, hosts, required_services)
+    else:
+      self.deleteKMSProxyUsers(configurations, services, hosts, required_services)
+
+  def recommendRangerKMSConfigurationsFromHDP26(self, configurations, clusterData, services, hosts):
+    putRangerKmsEnvProperty = self.putProperty(configurations, "kms-env", services)
+
+    ranger_kms_ssl_enabled = False
+    ranger_kms_ssl_port = "9393"
+    if 'ranger-kms-site' in services['configurations'] and 'ranger.service.https.attrib.ssl.enabled' in services['configurations']['ranger-kms-site']['properties']:
+      ranger_kms_ssl_enabled = services['configurations']['ranger-kms-site']['properties']['ranger.service.https.attrib.ssl.enabled'].lower() == "true"
+
+    if 'ranger-kms-site' in services['configurations'] and 'ranger.service.https.port' in services['configurations']['ranger-kms-site']['properties']:
+      ranger_kms_ssl_port = services['configurations']['ranger-kms-site']['properties']['ranger.service.https.port']
+
+    if ranger_kms_ssl_enabled:
+      putRangerKmsEnvProperty("kms_port", ranger_kms_ssl_port)
+    else:
+      putRangerKmsEnvProperty("kms_port", "9292")
+
+  def recommendRangerKMSConfigurationsFromHDP30(self, configurations, clusterData, services, hosts):
+    putRangerKmsEnvProperty = self.putProperty(configurations, "kms-env", services)
+
+    if 'kms-properties' in services['configurations'] and ('DB_FLAVOR' in services['configurations']['kms-properties']['properties']) \
+      and ('db_host' in services['configurations']['kms-properties']['properties']):
+
+      rangerKmsDbFlavor = services['configurations']["kms-properties"]["properties"]["DB_FLAVOR"]
+      rangerKmsDbHost =   services['configurations']["kms-properties"]["properties"]["db_host"]
+
+      ranger_kms_db_privelege_url_dict = {
+        'MYSQL': {'ranger_kms_privelege_user_jdbc_url': 'jdbc:mysql://' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost)},
+        'ORACLE': {'ranger_kms_privelege_user_jdbc_url': 'jdbc:oracle:thin:@' + self.getOracleDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost, None)},
+        'POSTGRES': {'ranger_kms_privelege_user_jdbc_url': 'jdbc:postgresql://' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + '/postgres'},
+        'MSSQL': {'ranger_kms_privelege_user_jdbc_url': 'jdbc:sqlserver://' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + ';'},
+        'SQLA': {'ranger_kms_privelege_user_jdbc_url': 'jdbc:sqlanywhere:host=' + self.getDBConnectionHostPort(rangerKmsDbFlavor, rangerKmsDbHost) + ';'}
+      }
+
+      rangerKmsPrivelegeDbProperties = ranger_kms_db_privelege_url_dict.get(rangerKmsDbFlavor, ranger_kms_db_privelege_url_dict['MYSQL'])
+      for key in rangerKmsPrivelegeDbProperties:
+        putRangerKmsEnvProperty(key, rangerKmsPrivelegeDbProperties.get(key))
+
+  def getDBConnectionHostPort(self, db_type, db_host):
+    connection_string = ""
+    if db_type is None or db_type == "":
+      return connection_string
+    else:
+      colon_count = db_host.count(':')
+      if colon_count == 0:
+        if DB_TYPE_DEFAULT_PORT_MAP.has_key(db_type):
+          connection_string = db_host + ":" + DB_TYPE_DEFAULT_PORT_MAP[db_type]
+        else:
+          connection_string = db_host
+      elif colon_count == 1:
+        connection_string = db_host
+      elif colon_count == 2:
+        connection_string = db_host
+
+    return connection_string
+
+  def getOracleDBConnectionHostPort(self, db_type, db_host, rangerDbName):
+    connection_string = self.getDBConnectionHostPort(db_type, db_host)
+    colon_count = db_host.count(':')
+    if colon_count == 1 and '/' in db_host:
+      connection_string = "//" + connection_string
+    elif colon_count == 0 or colon_count == 1:
+      connection_string = "//" + connection_string + "/" + rangerDbName if rangerDbName else "//" + connection_string
+
+    return connection_string
+
+  def recommendKMSProxyUsers(self, configurations, services, hosts, requiredServices):
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    putRangerKmsSiteProperty = self.putProperty(configurations, "kms-site", services)
+    putRangerKmsSitePropertyAttribute = self.putPropertyAttribute(configurations, "kms-site")
+
+    if 'forced-configurations' not in services:
+      services["forced-configurations"] = []
+
+    for index in range(len(requiredServices)):
+      service = requiredServices[index]['service']
+      config_type = requiredServices[index]['config-type']
+      property_name = requiredServices[index]['property-name']
+      proxy_category = requiredServices[index]['proxy-category']
+
+      if service in servicesList:
+        if config_type in services['configurations'] and property_name in services['configurations'][config_type]['properties']:
+          service_user = services['configurations'][config_type]['properties'][property_name]
+          service_old_user = self.getOldValue(services, config_type, property_name)
+
+          if 'groups' in proxy_category:
+            putRangerKmsSiteProperty('hadoop.kms.proxyuser.{0}.groups'.format(service_user), '*')
+          if 'hosts' in proxy_category:
+            putRangerKmsSiteProperty('hadoop.kms.proxyuser.{0}.hosts'.format(service_user), '*')
+          if 'users' in proxy_category:
+            putRangerKmsSiteProperty('hadoop.kms.proxyuser.{0}.users'.format(service_user), '*')
+
+          if service_old_user is not None and service_user != service_old_user:
+            if 'groups' in proxy_category:
+              putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.groups'.format(service_old_user), 'delete', 'true')
+              services["forced-configurations"].append({"type" : "kms-site", "name" : "hadoop.kms.proxyuser.{0}.groups".format(service_old_user)})
+              services["forced-configurations"].append({"type" : "kms-site", "name" : "hadoop.kms.proxyuser.{0}.groups".format(service_user)})
+            if 'hosts' in proxy_category:
+              putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.hosts'.format(service_old_user), 'delete', 'true')
+              services["forced-configurations"].append({"type" : "kms-site", "name" : "hadoop.kms.proxyuser.{0}.hosts".format(service_old_user)})
+              services["forced-configurations"].append({"type" : "kms-site", "name" : "hadoop.kms.proxyuser.{0}.hosts".format(service_user)})
+            if 'users' in proxy_category:
+              putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.users'.format(service_old_user), 'delete', 'true')
+              services["forced-configurations"].append({"type" : "kms-site", "name" : "hadoop.kms.proxyuser.{0}.users".format(service_old_user)})
+              services["forced-configurations"].append({"type" : "kms-site", "name" : "hadoop.kms.proxyuser.{0}.users".format(service_user)})
+
+  def deleteKMSProxyUsers(self, configurations, services, hosts, requiredServices):
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    putRangerKmsSitePropertyAttribute = self.putPropertyAttribute(configurations, "kms-site")
+
+    for index in range(len(requiredServices)):
+      service = requiredServices[index]['service']
+      config_type = requiredServices[index]['config-type']
+      property_name = requiredServices[index]['property-name']
+      proxy_category = requiredServices[index]['proxy-category']
+
+      if service in servicesList:
+        if config_type in services['configurations'] and property_name in services['configurations'][config_type]['properties']:
+          service_user = services['configurations'][config_type]['properties'][property_name]
+
+          if 'groups' in proxy_category:
+            putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.groups'.format(service_user), 'delete', 'true')
+          if 'hosts' in proxy_category:
+            putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.hosts'.format(service_user), 'delete', 'true')
+          if 'users' in proxy_category:
+            putRangerKmsSitePropertyAttribute('hadoop.kms.proxyuser.{0}.users'.format(service_user), 'delete', 'true')
+
+class RangerKMSValidator(service_advisor.ServiceAdvisor):
+  """
+  RangerKMS Validator checks the correctness of properties whenever the service is first added or the user attempts to
+  change configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(RangerKMSValidator, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    self.validators = []
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/service_advisor.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/service_advisor.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/service_advisor.py	(date 1719626239000)
@@ -22,6 +22,9 @@
 import os
 import traceback
 import re
+import socket
+import fnmatch
+
 
 from resource_management.core.logger import Logger
 
@@ -263,6 +266,10 @@
     else:
       putKafkaBrokerAttributes('authorizer.class.name', 'delete', 'true')
 
+    #If AMS is part of Services, use the KafkaTimelineMetricsReporter for metric reporting. Default is ''.
+    if "AMBARI_METRICS" in servicesList:
+      putKafkaBrokerProperty('kafka.metrics.reporters', 'org.apache.hadoop.metrics2.sink.kafka.KafkaTimelineMetricsReporter')
+
     if ranger_plugin_enabled:
       kafkaLog4jRangerLines = [{
                                  "name": "log4j.appender.rangerAppender",
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/role_command_order.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/role_command_order.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/role_command_order.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER_KMS/role_command_order.json	(date 1719626239000)
@@ -0,0 +1,7 @@
+{
+  "general_deps" : {
+    "_comment" : "dependencies for RANGER-KMS",
+    "RANGER_KMS_SERVER-START" : ["RANGER_ADMIN-START", "NAMENODE-START"],
+    "RANGER_KMS_SERVICE_CHECK-SERVICE_CHECK" : ["RANGER_KMS_SERVER-START"]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/role_command_order.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/role_command_order.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/role_command_order.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/role_command_order.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/role_command_order.json	(date 1719626239000)
@@ -1,7 +1,7 @@
 {
   "general_deps" : {
     "_comment" : "dependencies for KAFKA",
-    "KAFKA_BROKER-START" : ["ZOOKEEPER_SERVER-START"],
+    "KAFKA_BROKER-START" : ["ZOOKEEPER_SERVER-START", "RANGER_USERSYNC-START", "NAMENODE-START"],
     "KAFKA_SERVICE_CHECK-SERVICE_CHECK": ["KAFKA_BROKER-START"]
   }
-}
\ No newline at end of file
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-site.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-site.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-site.xml	(date 1723877052648)
@@ -519,11 +519,24 @@
     <name>yarn.acl.enable</name>
     <value>false</value>
     <description> Are acls enabled. </description>
+    <depends-on>
+      <property>
+        <type>ranger-yarn-plugin-properties</type>
+        <name>ranger-yarn-plugin-enabled</name>
+      </property>
+    </depends-on>
     <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>yarn.authorization-provider</name>
+    <value/>
     <description> Yarn authorization provider class. </description>
+    <depends-on>
+      <property>
+        <type>ranger-yarn-plugin-properties</type>
+        <name>ranger-yarn-plugin-enabled</name>
+      </property>
+    </depends-on>
     <on-ambari-upgrade add="true"/>
   </property>
   <property>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/metainfo.xml	(date 1719626239000)
@@ -81,6 +81,15 @@
           <versionAdvertised>true</versionAdvertised>
           <decommissionAllowed>true</decommissionAllowed>
           <timelineAppid>hbase</timelineAppid>
+          <dependencies>
+            <dependency>
+              <name>HDFS/HDFS_CLIENT</name>
+              <scope>host</scope>
+              <auto-deploy>
+                <enabled>true</enabled>
+              </auto-deploy>
+            </dependency>
+          </dependencies>
           <commandScript>
             <script>scripts/hbase_regionserver.py</script>
             <scriptType>PYTHON</scriptType>
@@ -128,10 +137,49 @@
               <type>env</type>
               <fileName>log4j.properties</fileName>
               <dictionaryName>hbase-log4j</dictionaryName>
-            </configFile>
+            </configFile>            
           </configFiles>
         </component>
 
+        <component>
+          <name>PHOENIX_QUERY_SERVER</name>
+          <displayName>Phoenix Query Server</displayName>
+          <category>SLAVE</category>
+          <cardinality>0+</cardinality>
+          <versionAdvertised>true</versionAdvertised>
+          <commandScript>
+            <script>scripts/phoenix_queryserver.py</script>
+            <scriptType>PYTHON</scriptType>
+          </commandScript>
+          <logs>
+            <log>
+              <logId>hbase_phoenix_server</logId>
+              <primary>true</primary>
+            </log>
+          </logs>
+        </component>
+		
+        <component>
+          <name>HBASE_THRIFTSERVER</name>
+          <displayName>HBase ThriftServer</displayName>
+          <category>SLAVE</category>
+          <cardinality>0+</cardinality>
+          <versionAdvertised>true</versionAdvertised>
+          <dependencies>
+              <dependency>
+                  <name>HBASE/HBASE_CLIENT</name>
+                  <scope>host</scope>
+                  <auto-deploy>
+                      <enabled>true</enabled>
+                  </auto-deploy>
+              </dependency>
+          </dependencies>
+          <commandScript>
+              <script>scripts/hbase_thriftserver.py</script>
+              <scriptType>PYTHON</scriptType>
+          </commandScript>
+        </component>
+		
       </components>
 
       <commandScript>
@@ -139,7 +187,7 @@
         <scriptType>PYTHON</scriptType>
         <timeout>300</timeout>
       </commandScript>
-
+      
       <requiredServices>
         <service>ZOOKEEPER</service>
         <service>HDFS</service>
@@ -156,6 +204,7 @@
         <config-type>ranger-hbase-audit</config-type>
         <config-type>ranger-hbase-policymgr-ssl</config-type>
         <config-type>ranger-hbase-security</config-type>
+        <config-type>hbase-atlas-application-properties</config-type>
       </configuration-dependencies>
 
       <quickLinksConfigurations>
@@ -171,6 +220,10 @@
           <packages>
             <package>
               <name>hbase_${stack_version}</name>
+            </package>
+            <package>
+              <name>phoenix_${stack_version}</name>
+              <condition>should_install_phoenix</condition>
             </package>
           </packages>
         </osSpecific>
@@ -189,4 +242,4 @@
 
     </service>
   </services>
-</metainfo>
\ No newline at end of file
+</metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/kerberos.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/kerberos.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/kerberos.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/kerberos.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/kerberos.json	(date 1719626239000)
@@ -105,6 +105,25 @@
               "keytab": {
                 "configuration": "ranger-hbase-audit/xasecure.audit.jaas.Client.option.keyTab"
               }
+            },
+            {
+              "name": "hbase_atlas_hook_kafka",
+              "reference": "/HBASE/HBASE_MASTER/hbase_master_hbase",
+              "principal":{
+                "configuration": "hbase-atlas-application-properties/atlas.jaas.KafkaClient.option.principal"
+              },
+              "keytab":{
+                "configuration": "hbase-atlas-application-properties/atlas.jaas.KafkaClient.option.keyTab"
+              }
+            }
+          ],
+          "configurations":[
+            {
+              "hbase-atlas-application-properties":{
+                "atlas.jaas.ticketBased-KafkaClient.loginModuleControlFlag" : "required",
+                "atlas.jaas.ticketBased-KafkaClient.loginModuleName" : "com.sun.security.auth.module.Krb5LoginModule",
+                "atlas.jaas.ticketBased-KafkaClient.option.useTicketCache" : "true"
+              }
             }
           ]
         },
@@ -143,6 +162,57 @@
               }
             }
           ]
+        },
+        {
+          "name": "PHOENIX_QUERY_SERVER",
+          "identities": [
+            {
+              "name": "phoenix_spnego",
+              "reference": "/spnego",
+              "principal": {
+                "configuration": "hbase-site/phoenix.queryserver.kerberos.principal"
+              },
+              "keytab": {
+                "configuration": "hbase-site/phoenix.queryserver.keytab.file"
+              }
+            }
+          ]
+        },
+        {
+          "name": "HBASE_THRIFTSERVER",
+          "identities": [
+            {
+              "name": "hbase_thrift_hbase",
+              "principal": {
+                "value": "hbase/_HOST@${realm}",
+                "type" : "service",
+                "configuration": "hbase-site/hbase.thrift.kerberos.principal",
+                "local_username": "${hbase-env/hbase_user}"
+              },
+              "keytab": {
+                "file": "${keytab_dir}/hbase.service.keytab",
+                "owner": {
+                  "name": "${hbase-env/hbase_user}",
+                  "access": "r"
+                },
+                "group": {
+                  "name": "${cluster-env/user_group}",
+                  "access": ""
+                },
+                "configuration": "hbase-site/hbase.thrift.keytab.file"
+              }
+            },
+            {
+              "name": "hbase_thrift_spnego",
+              "reference": "/spnego",
+              "principal": {
+                "configuration": "hbase-site/hbase.thrift.spnego.principal"
+              },
+              "keytab": {
+                "configuration": "hbase-site/hbase.thrift.spnego.keytab.file"
+              }
+            }
+          ]
         }
       ]
     }
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/service_advisor.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/service_advisor.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/service_advisor.py	(date 1719626239000)
@@ -127,6 +127,7 @@
     recommender.recommendHBASEConfigurationsFromHDP22(configurations, clusterData, services, hosts)
     recommender.recommendHBASEConfigurationsFromHDP23(configurations, clusterData, services, hosts)
     recommender.recommendHBASEConfigurationsFromHDP26(configurations, clusterData, services, hosts)
+    recommender.recommendHBASEConfigurationsFromHDP30(configurations, clusterData, services, hosts)
     recommender.recommendHBASEConfigurationsFromHDP301(configurations, clusterData, services, hosts)
     recommender.recommendHBASEConfigurationsForKerberos(configurations, clusterData, services, hosts)
 
@@ -153,8 +154,8 @@
     # method(siteProperties, siteRecommendations, configurations, services, hosts)
     return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
 
-  # def isComponentUsingCardinalityForLayout(self, componentName):
-  #   return componentName == 'PHOENIX_QUERY_SERVER'
+  def isComponentUsingCardinalityForLayout(self, componentName):
+    return componentName == 'PHOENIX_QUERY_SERVER'
 
 
 class HBASERecommender(service_advisor.ServiceAdvisor):
@@ -245,11 +246,19 @@
     putHbaseSitePropertyAttributes = self.putPropertyAttribute(configurations, "hbase-site")
     putHbaseSiteProperty("hbase.regionserver.global.memstore.size", '0.4')
 
-    putHbaseSiteProperty("hbase.regionserver.wal.codec", 'org.apache.hadoop.hbase.regionserver.wal.WALCellCodec')
-    if ('hbase.rpc.controllerfactory.class' in configurations["hbase-site"]["properties"]) or \
-            ('hbase-site' in services['configurations'] and 'hbase.rpc.controllerfactory.class' in services['configurations']["hbase-site"]["properties"]):
-      putHbaseSitePropertyAttributes('hbase.rpc.controllerfactory.class', 'delete', 'true')
-      
+    if 'hbase-env' in services['configurations'] and 'phoenix_sql_enabled' in services['configurations']['hbase-env']['properties'] and \
+                    'true' == services['configurations']['hbase-env']['properties']['phoenix_sql_enabled'].lower():
+      putHbaseSiteProperty("hbase.regionserver.wal.codec", 'org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec')
+      putHbaseSiteProperty("phoenix.functions.allowUserDefinedFunctions", 'true')
+    else:
+      putHbaseSiteProperty("hbase.regionserver.wal.codec", 'org.apache.hadoop.hbase.regionserver.wal.WALCellCodec')
+      if ('hbase.rpc.controllerfactory.class' in configurations["hbase-site"]["properties"]) or \
+              ('hbase-site' in services['configurations'] and 'hbase.rpc.controllerfactory.class' in services['configurations']["hbase-site"]["properties"]):
+        putHbaseSitePropertyAttributes('hbase.rpc.controllerfactory.class', 'delete', 'true')
+      if ('phoenix.functions.allowUserDefinedFunctions' in configurations["hbase-site"]["properties"]) or \
+              ('hbase-site' in services['configurations'] and 'phoenix.functions.allowUserDefinedFunctions' in services['configurations']["hbase-site"]["properties"]):
+        putHbaseSitePropertyAttributes('phoenix.functions.allowUserDefinedFunctions', 'delete', 'true')
+
     if "ranger-env" in services["configurations"] and "ranger-hbase-plugin-properties" in services["configurations"] and \
                     "ranger-hbase-plugin-enabled" in services["configurations"]["ranger-env"]["properties"]:
       putHbaseRangerPluginProperty = self.putProperty(configurations, "ranger-hbase-plugin-properties", services)
@@ -342,18 +351,18 @@
 
       putHbaseEnvPropertyAttributes('hbase_max_direct_memory_size', 'delete', 'true')
 
-    # if 'hbase-env' in services['configurations'] and 'phoenix_sql_enabled' in services['configurations']['hbase-env']['properties'] and \
-    #                 'true' == services['configurations']['hbase-env']['properties']['phoenix_sql_enabled'].lower():
-    #   if 'hbase.rpc.controllerfactory.class' in services['configurations']['hbase-site']['properties'] and \
-    #                   services['configurations']['hbase-site']['properties']['hbase.rpc.controllerfactory.class'] == \
-    #                   'org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory':
-    #     putHbaseSitePropertyAttributes('hbase.rpc.controllerfactory.class', 'delete', 'true')
+    if 'hbase-env' in services['configurations'] and 'phoenix_sql_enabled' in services['configurations']['hbase-env']['properties'] and \
+                    'true' == services['configurations']['hbase-env']['properties']['phoenix_sql_enabled'].lower():
+      if 'hbase.rpc.controllerfactory.class' in services['configurations']['hbase-site']['properties'] and \
+                      services['configurations']['hbase-site']['properties']['hbase.rpc.controllerfactory.class'] == \
+                      'org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory':
+        putHbaseSitePropertyAttributes('hbase.rpc.controllerfactory.class', 'delete', 'true')
 
-    #   putHbaseSiteProperty("hbase.region.server.rpc.scheduler.factory.class", "org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory")
-    # else:
-    #   putHbaseSitePropertyAttributes('hbase.region.server.rpc.scheduler.factory.class', 'delete', 'true')
+      putHbaseSiteProperty("hbase.region.server.rpc.scheduler.factory.class", "org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory")
+    else:
       putHbaseSitePropertyAttributes('hbase.region.server.rpc.scheduler.factory.class', 'delete', 'true')
 
+
   def recommendHBASEConfigurationsFromHDP26(self, configurations, clusterData, services, hosts):
     if 'hbase-env' in services['configurations'] and 'hbase_user' in services['configurations']['hbase-env']['properties']:
       hbase_user = services['configurations']['hbase-env']['properties']['hbase_user']
@@ -369,6 +378,56 @@
     else:
       self.logger.info("Not setting Hbase Repo user for Ranger.")
 
+
+  def recommendHBASEConfigurationsFromHDP30(self, configurations, clusterData, services, hosts):
+    # Hbase-hook configurations for Atlas
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    hbase_atlas_hook_property = 'hbase.coprocessor.master.classes'
+    hbase_atlas_hook_impl_class = 'org.apache.atlas.hbase.hook.HBaseAtlasCoprocessor'
+    if hbase_atlas_hook_property in configurations['hbase-site']['properties']:
+      hbase_master_coprocessor_value = configurations['hbase-site']['properties']['hbase.coprocessor.master.classes']
+    else:
+      hbase_master_coprocessor_value = ''
+
+    hbase_master_coprocessor_list = [coprocessor_class.strip(' ') for coprocessor_class in hbase_master_coprocessor_value.split(',')]
+    hbase_master_coprocessor_list = [coprocessor_class for coprocessor_class in hbase_master_coprocessor_list if coprocessor_class != '' ]
+
+    is_atlas_present_in_cluster = 'ATLAS' in servicesList
+    putHbaseEnvProperty = self.putProperty(configurations, "hbase-env", services)
+    putHbaseSiteProperty = self.putProperty(configurations, "hbase-site", services)
+    putHBaseAtlasHookProperty = self.putProperty(configurations, "hbase-atlas-application-properties", services)
+    putHBaseAtlasHookPropertyAttribute = self.putPropertyAttribute(configurations,"hbase-atlas-application-properties")
+    if 'hbase-atlas-application-properties' in services['configurations'] and 'enable.external.atlas.for.hbase' in services['configurations']['hbase-atlas-application-properties']['properties']:
+      enable_external_hook_for_hbase = services['configurations']['hbase-atlas-application-properties']['properties']['enable.external.atlas.for.hbase'].lower() == 'true'
+    else:
+      enable_external_hook_for_hbase = False
+
+    if is_atlas_present_in_cluster:
+      putHbaseEnvProperty('hbase.atlas.hook','true')
+    elif enable_external_hook_for_hbase:
+      putHbaseEnvProperty('hbase.atlas.hook','true')
+    else:
+      putHbaseEnvProperty('hbase.atlas.hook','false')
+
+    if 'hbase-env' in configurations and 'hbase.atlas.hook' in configurations['hbase-env']['properties']:
+      enable_hbase_atlas_hook = configurations['hbase-env']['properties']['hbase.atlas.hook'] == 'true'
+    elif 'hbase-env' in services['configurations'] and 'hbase.atlas.hook' in services['configurations']['hbase-env']['properties']:
+      enable_hbase_atlas_hook = services['configurations']['hbase-env']['properties']['hbase.atlas.hook'] == 'true'
+    else:
+      enable_hbase_atlas_hook = False
+
+    if enable_hbase_atlas_hook:
+      is_hbase_atlas_hook_in_config = hbase_atlas_hook_impl_class in hbase_master_coprocessor_list
+      if not is_hbase_atlas_hook_in_config:
+        hbase_master_coprocessor_list.append(hbase_atlas_hook_impl_class)
+      else:
+        self.logger.info('hbase-atlas hook is already present in configuration.')
+    else:
+      hbase_master_coprocessor_list = [hbase_master_coprocessor for hbase_master_coprocessor in hbase_master_coprocessor_list if hbase_master_coprocessor != hbase_atlas_hook_impl_class]
+
+    hbase_master_coprocessor_value = '' if len(hbase_master_coprocessor_list) == 0 else ",".join(hbase_master_coprocessor_list)
+    putHbaseSiteProperty(hbase_atlas_hook_property,hbase_master_coprocessor_value)
+
   def recommendHBASEConfigurationsFromHDP301(self, configurations, clusterData, services, hosts):
     # Setters
     putHbaseSiteProperty = self.putProperty(configurations, "hbase-site", services)
@@ -407,6 +466,16 @@
             {'mem_limit':8192, 'cpu_limit':8, 'handlers': 60},
             {'mem_limit':12288, 'cpu_limit':16, 'handlers': 90},
             {'mem_limit':16384, 'cpu_limit':24, 'handlers': 120} ]
+    phoenix_recommendations = [ {'mem_limit':2048, 'cpu_limit':4, 'handlers': 20, 'index_handlers': 10},
+            {'mem_limit':8192, 'cpu_limit':8, 'handlers': 50, 'index_handlers': 15},
+            {'mem_limit':12288, 'cpu_limit':16, 'handlers': 70, 'index_handlers': 20},
+            {'mem_limit':16384, 'cpu_limit':24, 'handlers': 100, 'index_handlers': 30} ]
+
+    # Is Phoenix enabled?
+    phoenix_enabled = 'hbase-env' in services['configurations'] and 'phoenix_sql_enabled' in services['configurations']['hbase-env']['properties'] and \
+                'true' == services['configurations']['hbase-env']['properties']['phoenix_sql_enabled'].lower()
+    recommendations = phoenix_recommendations if phoenix_enabled else hbase_recommendations
+    self.logger.info("phoenix_enabled=" + str(phoenix_enabled))
 
     # Determine the limit
     handlers = None
@@ -414,16 +483,19 @@
     for level in recommendations:
       if level['mem_limit'] > hbaseRamInMB or level['cpu_limit'] > cores:
         handlers = level['handlers']
+        if phoenix_enabled:
+          index_handlers = level['index_handlers']
         break
 
     # For lots of RAM+CPU, we may have exceeded the final level's limits. Just use the last one.
     if handlers is None:
       level = recommendations[-1]
       handlers = level['handlers']
+      if phoenix_enabled:
+        index_handlers = level['index_handlers']
 
     self.logger.info("Setting HBase handlers to %d" % (handlers))
     putHbaseSiteProperty('hbase.regionserver.handler.count', handlers)
-    phoenix_enabled = False
     if phoenix_enabled:
       self.logger.info("Setting Phoenix index handlers to %d" % (index_handlers))
       putHbaseSiteProperty('phoenix.rpc.index.handler.count', index_handlers)
@@ -440,16 +512,16 @@
       # Set the master's UI to readonly
       putHbaseSiteProperty('hbase.master.ui.readonly', 'true')
 
-      # phoenix_query_server_hosts = self.getPhoenixQueryServerHosts(services, hosts)
-      # self.logger.debug("Calculated Phoenix Query Server hosts: %s" % str(phoenix_query_server_hosts))
-      # if phoenix_query_server_hosts:
-      #   self.logger.debug("Attempting to update hadoop.proxyuser.HTTP.hosts with %s" % str(phoenix_query_server_hosts))
-      #   # The PQS hosts we want to ensure are set
-      #   new_value = ','.join(phoenix_query_server_hosts)
-      #   # Update the proxyuser setting, deferring to out callback to merge results together
-      #   self.put_proxyuser_value("HTTP", new_value, services=services, configurations=configurations, put_function=putCoreSiteProperty)
-      # else:
-      self.logger.debug("No phoenix query server hosts to update")
+      phoenix_query_server_hosts = self.getPhoenixQueryServerHosts(services, hosts)
+      self.logger.debug("Calculated Phoenix Query Server hosts: %s" % str(phoenix_query_server_hosts))
+      if phoenix_query_server_hosts:
+        self.logger.debug("Attempting to update hadoop.proxyuser.HTTP.hosts with %s" % str(phoenix_query_server_hosts))
+        # The PQS hosts we want to ensure are set
+        new_value = ','.join(phoenix_query_server_hosts)
+        # Update the proxyuser setting, deferring to out callback to merge results together
+        self.put_proxyuser_value("HTTP", new_value, services=services, configurations=configurations, put_function=putCoreSiteProperty)
+      else:
+        self.logger.debug("No phoenix query server hosts to update")
     else:
       putHbaseSiteProperty('hbase.master.ui.readonly', 'false')
 
@@ -584,15 +656,14 @@
 
 
   def getPhoenixQueryServerHosts(self, services, hosts):
-    return None
-    # """
-    # Returns the list of Phoenix Query Server host names, or None.
-    # """
-    # if len(hosts['items']) > 0:
-    #   phoenix_query_server_hosts = self.getHostsWithComponent("HBASE", "PHOENIX_QUERY_SERVER", services, hosts)
-    #   if phoenix_query_server_hosts is None:
-    #     return []
-    #   return [host['Hosts']['host_name'] for host in phoenix_query_server_hosts]
+    """
+    Returns the list of Phoenix Query Server host names, or None.
+    """
+    if len(hosts['items']) > 0:
+      phoenix_query_server_hosts = self.getHostsWithComponent("HBASE", "PHOENIX_QUERY_SERVER", services, hosts)
+      if phoenix_query_server_hosts is None:
+        return []
+      return [host['Hosts']['host_name'] for host in phoenix_query_server_hosts]
 
 
   def isRangerPluginEnabled(self, configurations, services):
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/role_command_order.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/role_command_order.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/role_command_order.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/role_command_order.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/role_command_order.json	(date 1719626239000)
@@ -2,8 +2,10 @@
   "general_deps" : {
     "_comment" : "dependencies for HBase",
     "HBASE_REGIONSERVER-START": ["HBASE_MASTER-START"],
-    "HBASE_SERVICE_CHECK-SERVICE_CHECK": ["HBASE_MASTER-START", "HBASE_REGIONSERVER-START"],
+	"HBASE_THRIFTSERVER-START": ["HBASE_MASTER-START", "HBASE_REGIONSERVER-START"],
+    "HBASE_SERVICE_CHECK-SERVICE_CHECK": ["HBASE_MASTER-START", "HBASE_REGIONSERVER-START", "HBASE_THRIFTSERVER-START","HBASE_THRIFTSERVER-START"],
     "HBASE_MASTER-STOP": ["HBASE_REGIONSERVER-STOP"],
-    "HBASE_MASTER-START": ["NAMENODE-START", "DATANODE-START", "ZOOKEEPER_SERVER-START", "RANGER_USERSYNC-START", "RANGER_KMS_SERVER-START"]
+    "HBASE_MASTER-START": ["NAMENODE-START", "DATANODE-START", "ZOOKEEPER_SERVER-START", "RANGER_USERSYNC-START", "RANGER_KMS_SERVER-START"],
+    "PHOENIX_QUERY_SERVER-START": ["HBASE_MASTER-START"]
   }
 }
Index: ambari-common/src/main/python/resource_management/libraries/functions/constants.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-common/src/main/python/resource_management/libraries/functions/constants.py b/ambari-common/src/main/python/resource_management/libraries/functions/constants.py
--- a/ambari-common/src/main/python/resource_management/libraries/functions/constants.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-common/src/main/python/resource_management/libraries/functions/constants.py	(date 1719626239000)
@@ -85,6 +85,7 @@
   HIVE_ENV_HEAPSIZE = "hive_env_heapsize"
   RANGER_KMS_HSM_SUPPORT = "ranger_kms_hsm_support"
   RANGER_LOG4J_SUPPORT = "ranger_log4j_support"
+  RANGER_LOGBACK_SUPPORT = "ranger_logback_support"
   RANGER_KERBEROS_SUPPORT = "ranger_kerberos_support"
   HIVE_METASTORE_SITE_SUPPORT = "hive_metastore_site_support"
   RANGER_USERSYNC_PASSWORD_JCEKS = "ranger_usersync_password_jceks"
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/themes/theme.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/themes/theme.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/themes/theme.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/themes/theme.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/themes/theme.json	(date 1719626239000)
@@ -170,6 +170,10 @@
           "subsection-name": "subsection-hbase-memory-col1"
         },
         {
+          "config": "hbase-site/hbase.regionserver.global.memstore.size",
+          "subsection-name": "subsection-hbase-memory-col1"
+        },
+        {
           "config": "hbase-site/hbase.hregion.memstore.flush.size",
           "subsection-name": "subsection-hbase-memory-col2"
         },
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-env.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-env.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-env.xml	(date 1719626239000)
@@ -186,6 +186,8 @@
 # Extra Java CLASSPATH elements. Optional.
 export HBASE_CLASSPATH=${HBASE_CLASSPATH}
 
+# Add Phoenix client JAR to HBase classpath
+export HBASE_CLASSPATH=$HBASE_CLASSPATH:/usr/bigtop/current/phoenix-client/phoenix-client.jar
 
 # The maximum amount of heap to use, in MB. Default is 1000.
 # export HBASE_HEAPSIZE=1000
@@ -241,12 +243,14 @@
 export HBASE_OPTS="$HBASE_OPTS -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:-ResizePLAB -XX:ErrorFile={{log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{java_io_tmpdir}}"
 export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx{{master_heapsize}} -XX:ParallelGCThreads={{parallel_gc_threads}} $JDK_DEPENDED_OPTS "
 export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}} -XX:ParallelGCThreads={{parallel_gc_threads}} $JDK_DEPENDED_OPTS"
+export PHOENIX_QUERYSERVER_OPTS="$PHOENIX_QUERYSERVER_OPTS -XX:ParallelGCThreads={{parallel_gc_threads}} $JDK_DEPENDED_OPTS"
 
 # Add Kerberos authentication-related configuration
 {% if security_enabled %}
 export HBASE_OPTS="$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}} {{zk_security_opts}}"
 export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}} -Djavax.security.auth.useSubjectCredsOnly=false"
 export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}} -Djavax.security.auth.useSubjectCredsOnly=false"
+export PHOENIX_QUERYSERVER_OPTS="$PHOENIX_QUERYSERVER_OPTS -Djava.security.auth.login.config={{queryserver_jaas_config_file}}"
 export HBASE_SERVER_JAAS_OPTS="-Djava.security.auth.login.config={{client_jaas_config_file}}"
 {% endif %}
 
@@ -273,7 +277,7 @@
   </property>
   <property>
     <name>phoenix_sql_enabled</name>
-    <value>false</value>
+    <value>true</value>
     <description>Enable Phoenix SQL</description>
     <display-name>Enable Phoenix</display-name>
     <value-attributes>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/role_command_order.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/role_command_order.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/role_command_order.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/role_command_order.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/role_command_order.json	(date 1723875997448)
@@ -1,76 +1,103 @@
 {
-  "_comment" : "Record format:",
-  "_comment" : "blockedRole-blockedCommand: [blockerRole1-blockerCommand1, blockerRole2-blockerCommand2, ...]",
-  "general_deps" : {
-    "_comment" : "dependencies for all cases",
-    "HBASE_MASTER-START": ["ZOOKEEPER_SERVER-START"],
+  "_comment": "Record format:",
+  "_comment": "blockedRole-blockedCommand: [blockerRole1-blockerCommand1, blockerRole2-blockerCommand2, ...]",
+  "general_deps": {
+    "_comment": "dependencies for all cases",
+    "APP_TIMELINE_SERVER-START": ["NAMENODE-START", "DATANODE-START"],
+    "DATANODE-START" : ["RANGER_USERSYNC-START"],
+    "DATANODE-STOP": ["RESOURCEMANAGER-STOP", "NODEMANAGER-STOP", "HISTORYSERVER-STOP", "HBASE_MASTER-STOP"],
+    "FLUME_HANDLER-START": ["OOZIE_SERVER-START"],
+    "FLUME_SERVICE_CHECK-SERVICE_CHECK": ["FLUME_HANDLER-START"],
+    "HBASE_MASTER-START": ["ZOOKEEPER_SERVER-START","RANGER_USERSYNC-START"],
+    "HBASE_MASTER-STOP": ["HBASE_REGIONSERVER-STOP"],
     "HBASE_REGIONSERVER-START": ["HBASE_MASTER-START"],
-    "APP_TIMELINE_SERVER-START": ["NAMENODE-START", "DATANODE-START"],
-    "OOZIE_SERVER-START": ["NODEMANAGER-START", "RESOURCEMANAGER-START"],
-    "WEBHCAT_SERVER-START": ["NODEMANAGER-START", "HIVE_SERVER-START"],
-    "WEBHCAT_SERVER-RESTART": ["NODEMANAGER-RESTART", "HIVE_SERVER-RESTART"],
-    "HIVE_METASTORE-START": ["MYSQL_SERVER-START", "NAMENODE-START"],
-    "HIVE_METASTORE-RESTART": ["MYSQL_SERVER-RESTART", "NAMENODE-RESTART"],
-    "HIVE_SERVER-START": ["NODEMANAGER-START", "MYSQL_SERVER-START"],
+    "HBASE_SERVICE_CHECK-SERVICE_CHECK": ["HBASE_MASTER-START", "HBASE_REGIONSERVER-START"],
+    "HIVE_METASTORE-RESTART": ["MYSQL_SERVER-RESTART", "NAMENODE-RESTART","DATANODE-RESTART", "RESOURCEMANAGER-RESTART"],
+    "HIVE_METASTORE-START": ["MYSQL_SERVER-START", "NAMENODE-START","DATANODE-START", "RESOURCEMANAGER-START"],
+    "HIVE_SERVER_INTERACTIVE-START": ["RESOURCEMANAGER-START", "NODEMANAGER-START", "MYSQL_SERVER-START", "HIVE_METASTORE-START"],
+    "HIVE_SERVER_INTERACTIVE-RESTART": ["NODEMANAGER-RESTART", "MYSQL_SERVER-RESTART"],
     "HIVE_SERVER-RESTART": ["NODEMANAGER-RESTART", "MYSQL_SERVER-RESTART", "ZOOKEEPER_SERVER-RESTART"],
+    "HIVE_SERVER-START": ["NODEMANAGER-START", "MYSQL_SERVER-START", "RANGER_USERSYNC-START", "ZOOKEEPER_SERVER-START", "HIVE_METASTORE-START"],
+    "HIVE_SERVICE_CHECK-SERVICE_CHECK": ["HIVE_SERVER-START", "HIVE_METASTORE-START", "WEBHCAT_SERVER-START","HIVE_SERVER_INTERACTIVE-START"],
     "HUE_SERVER-START": ["HIVE_SERVER-START", "HCAT-START", "OOZIE_SERVER-START"],
-    "FLUME_HANDLER-START": ["OOZIE_SERVER-START"],
     "MAPREDUCE_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START", "RESOURCEMANAGER-START"],
+    "OOZIE_SERVER-START": ["NODEMANAGER-START", "RESOURCEMANAGER-START"],
     "OOZIE_SERVICE_CHECK-SERVICE_CHECK": ["OOZIE_SERVER-START", "MAPREDUCE2_SERVICE_CHECK-SERVICE_CHECK"],
-    "HBASE_SERVICE_CHECK-SERVICE_CHECK": ["HBASE_MASTER-START", "HBASE_REGIONSERVER-START"],
-    "HIVE_SERVICE_CHECK-SERVICE_CHECK": ["HIVE_SERVER-START", "HIVE_METASTORE-START", "WEBHCAT_SERVER-START"],
     "PIG_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START", "RESOURCEMANAGER-START"],
+    "RANGER_ADMIN-START": ["ZOOKEEPER_SERVER-START", "INFRA_SOLR-START"],
+    "RANGER_KMS_SERVER-START": ["RANGER_ADMIN-START"],
+    "RANGER_KMS_SERVICE_CHECK-SERVICE_CHECK": ["RANGER_KMS_SERVER-START"],
+    "RANGER_SERVICE_CHECK-SERVICE_CHECK": ["RANGER_USERSYNC-START", "RANGER_ADMIN-START"],
+    "RANGER_USERSYNC-START": ["RANGER_ADMIN-START"],
+    "RESOURCEMANAGER-START": ["RANGER_USERSYNC-START"],
+    "RESOURCEMANAGER-STOP": ["HIVE_SERVER_INTERACTIVE-STOP", "SPARK_THRIFTSERVER-STOP", "SPARK2_THRIFTSERVER-STOP"],
+    "SPARK_SERVICE_CHECK-SERVICE_CHECK" : ["SPARK_JOBHISTORYSERVER-START", "LIVY_SERVER-START", "SPARK_THRIFTSERVER-START"],
+    "SPARK2_SERVICE_CHECK-SERVICE_CHECK" : ["SPARK2_JOBHISTORYSERVER-START", "LIVY2_SERVER-START", "SPARK2_THRIFTSERVER-START"],
+    "SPARK_THRIFTSERVER-START" : ["HIVE_METASTORE-START", "HIVE_SERVER-START"],
+    "SPARK2_JOBHISTORYSERVER-START" : ["NAMENODE-START", "DATANODE-START", "HIVE_METASTORE-START"],
+    "SPARK2_THRIFTSERVER-START" : ["NAMENODE-START", "DATANODE-START", "HIVE_SERVER-START", "SPARK2_JOBHISTORYSERVER-START"],
+    "SOLR_SERVER-START": ["ZOOKEEPER_SERVER-START"],
     "SQOOP_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START", "RESOURCEMANAGER-START"],
-    "ZOOKEEPER_SERVICE_CHECK-SERVICE_CHECK": ["ZOOKEEPER_SERVER-START"],
+    "WEBHCAT_SERVER-RESTART": ["NODEMANAGER-RESTART", "HIVE_SERVER-RESTART"],
+    "WEBHCAT_SERVER-START": ["NODEMANAGER-START", "HIVE_SERVER-START"],
     "ZOOKEEPER_QUORUM_SERVICE_CHECK-SERVICE_CHECK": ["ZOOKEEPER_SERVER-START"],
-    "ZOOKEEPER_SERVER-STOP" : ["HBASE_MASTER-STOP", "HBASE_REGIONSERVER-STOP", "METRICS_COLLECTOR-STOP", "SOLR_SERVER-STOP"],
-    "HBASE_MASTER-STOP": ["HBASE_REGIONSERVER-STOP"],
-    "SOLR_SERVER-START": ["ZOOKEEPER_SERVER-START"]
+    "ZOOKEEPER_SERVER-STOP": ["HBASE_MASTER-STOP", "HBASE_REGIONSERVER-STOP", "METRICS_COLLECTOR-STOP", "SOLR_SERVER-STOP"],
+    "ZOOKEEPER_SERVICE_CHECK-SERVICE_CHECK": ["ZOOKEEPER_SERVER-START"]
+
   },
-  "_comment" : "GLUSTERFS-specific dependencies",
+  "namenode_optional_ha": {
+    "_comment": "Dependencies that are used in HA NameNode cluster",
+    "JOURNALNODE-STOP": ["NAMENODE-STOP"],
+    "NAMENODE-START": ["ZKFC-START", "JOURNALNODE-START", "ZOOKEEPER_SERVER-START"],
+    "ZKFC-START": ["ZOOKEEPER_SERVER-START"],
+    "ZKFC-STOP": ["NAMENODE-STOP"]
+  },
   "optional_glusterfs": {
-    "HBASE_MASTER-START": ["PEERSTATUS-START"],
-    "GLUSTERFS_SERVICE_CHECK-SERVICE_CHECK": ["PEERSTATUS-START"]
+    "_comment": "GLUSTERFS-specific dependencies",
+    "GLUSTERFS_SERVICE_CHECK-SERVICE_CHECK": ["PEERSTATUS-START"],
+    "HBASE_MASTER-START": ["PEERSTATUS-START"]
   },
-  "_comment" : "Dependencies that are used when GLUSTERFS is not present in cluster",
   "optional_no_glusterfs": {
-    "METRICS_COLLECTOR-START": ["NAMENODE-START", "DATANODE-START", "SECONDARY_NAMENODE-START", "ZOOKEEPER_SERVER-START"],
+    "_comment": "Dependencies that are used when GLUSTERFS is not present in cluster",
     "AMBARI_METRICS_SERVICE_CHECK-SERVICE_CHECK": ["METRICS_COLLECTOR-START", "HDFS_SERVICE_CHECK-SERVICE_CHECK"],
-    "SECONDARY_NAMENODE-START": ["NAMENODE-START"],
-    "SECONDARY_NAMENODE-RESTART": ["NAMENODE-RESTART"],
-    "RESOURCEMANAGER-START": ["NAMENODE-START", "DATANODE-START"],
-    "NODEMANAGER-START": ["NAMENODE-START", "DATANODE-START", "RESOURCEMANAGER-START"],
+
+    "DATANODE-START" : ["RANGER_USERSYNC-START"],
+    "DATANODE-STOP": ["RESOURCEMANAGER-STOP", "NODEMANAGER-STOP", "HISTORYSERVER-STOP", "HBASE_MASTER-STOP"],
+
+
+    "HBASE_MASTER-START": ["NAMENODE-START", "DATANODE-START"],
+    "HDFS_SERVICE_CHECK-SERVICE_CHECK": ["NAMENODE-START", "DATANODE-START", "SECONDARY_NAMENODE-START"],
     "HISTORYSERVER-START": ["NAMENODE-START", "DATANODE-START"],
-    "HBASE_MASTER-START": ["NAMENODE-START", "DATANODE-START"],
+    "HISTORYSERVER-RESTART": ["NAMENODE-RESTART"],
     "HIVE_SERVER-START": ["DATANODE-START"],
-    "WEBHCAT_SERVER-START": ["DATANODE-START"],
-    "HISTORYSERVER-RESTART": ["NAMENODE-RESTART"],
-    "RESOURCEMANAGER-RESTART": ["NAMENODE-RESTART"],
+
+
+    "MAPREDUCE2_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START", "RESOURCEMANAGER-START", "HISTORYSERVER-START", "YARN_SERVICE_CHECK-SERVICE_CHECK"],
+    "METRICS_COLLECTOR-START": ["NAMENODE-START", "DATANODE-START", "SECONDARY_NAMENODE-START", "ZOOKEEPER_SERVER-START"],
+    "METRICS_COLLECTOR-STOP": ["METRICS_GRAFANA-STOP"],
+    "METRICS_GRAFANA-START": ["METRICS_COLLECTOR-START"],
+
+    "NAMENODE-STOP": ["RESOURCEMANAGER-STOP", "NODEMANAGER-STOP", "HISTORYSERVER-STOP", "HBASE_MASTER-STOP", "METRICS_COLLECTOR-STOP"],
     "NODEMANAGER-RESTART": ["NAMENODE-RESTART"],
+    "NODEMANAGER-START": ["NAMENODE-START", "DATANODE-START", "RESOURCEMANAGER-START"],
+
     "OOZIE_SERVER-RESTART": ["NAMENODE-RESTART"],
-    "HDFS_SERVICE_CHECK-SERVICE_CHECK": ["NAMENODE-START", "DATANODE-START",
-        "SECONDARY_NAMENODE-START"],
-    "MAPREDUCE2_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START",
-        "RESOURCEMANAGER-START", "HISTORYSERVER-START", "YARN_SERVICE_CHECK-SERVICE_CHECK"],
-    "YARN_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START", "RESOURCEMANAGER-START"],
+
+    "PIG_SERVICE_CHECK-SERVICE_CHECK": ["RESOURCEMANAGER-START", "NODEMANAGER-START"],
+
+    "RESOURCEMANAGER-RESTART": ["NAMENODE-RESTART"],
+    "RESOURCEMANAGER-START": ["NAMENODE-START", "DATANODE-START"],
     "RESOURCEMANAGER_SERVICE_CHECK-SERVICE_CHECK": ["RESOURCEMANAGER-START"],
-    "PIG_SERVICE_CHECK-SERVICE_CHECK": ["RESOURCEMANAGER-START", "NODEMANAGER-START"],
-    "NAMENODE-STOP": ["RESOURCEMANAGER-STOP", "NODEMANAGER-STOP",
-        "HISTORYSERVER-STOP", "HBASE_MASTER-STOP", "METRICS_COLLECTOR-STOP"],
-    "DATANODE-STOP": ["RESOURCEMANAGER-STOP", "NODEMANAGER-STOP",
-        "HISTORYSERVER-STOP", "HBASE_MASTER-STOP"],
-    "METRICS_GRAFANA-START": ["METRICS_COLLECTOR-START"],
-    "METRICS_COLLECTOR-STOP": ["METRICS_GRAFANA-STOP"]
+
+    "SECONDARY_NAMENODE-RESTART": ["NAMENODE-RESTART"],
+    "SECONDARY_NAMENODE-START": ["NAMENODE-START"],
+
+    "WEBHCAT_SERVER-START": ["DATANODE-START"],
+
+    "YARN_SERVICE_CHECK-SERVICE_CHECK": ["NODEMANAGER-START", "RESOURCEMANAGER-START"]
   },
-  "_comment" : "Dependencies that are used in HA NameNode cluster",
-  "namenode_optional_ha": {
-    "NAMENODE-START": ["ZKFC-START", "JOURNALNODE-START", "ZOOKEEPER_SERVER-START"],
-    "ZKFC-START": ["ZOOKEEPER_SERVER-START"],
-    "ZKFC-STOP": ["NAMENODE-STOP"],
-    "JOURNALNODE-STOP": ["NAMENODE-STOP"]
-  },
-  "_comment" : "Dependencies that are used in ResourceManager HA cluster",
-  "resourcemanager_optional_ha" : {
+  "resourcemanager_optional_ha": {
+    "_comment": "Dependencies that are used in ResourceManager HA cluster",
     "RESOURCEMANAGER-START": ["ZOOKEEPER_SERVER-START"]
   }
-}
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-site.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-site.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-site.xml	(date 1719626239000)
@@ -805,4 +805,18 @@
     </description>
     <on-ambari-upgrade add="false"/>
   </property>
+    <property>
+    <name>hbase.regionserver.thrift.http</name>
+    <value>true</value>
+    <description>doAs is supported only in Thrift 1. Enable doAs support 
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.thrift.support.proxyuser</name>
+    <value>true</value>
+    <description>doAs is supported only in Thrift 1. Enable doAs support 
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>  
 </configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_features.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_features.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_features.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_features.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_features.json	(date 1723876423934)
@@ -55,6 +55,152 @@
         "name": "kafka_listeners",
         "description": "Kafka listeners (AMBARI-10984)",
         "min_version": "3.2.0"
+      },
+      {
+        "name": "atlas_ranger_plugin_support",
+        "description": "Atlas Ranger plugin support",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "core_site_for_ranger_plugins",
+        "description": "Adding core-site.xml in when Ranger plugin is enabled for Storm, Kafka, and Knox.",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "kafka_env_include_ranger_script",
+        "description": "Need to include Ranger Kafka plugin script if stack version is lower than 3.1.0.0",
+        "max_version": "3.2.0"
+      },
+      {
+        "name": "kafka_ranger_plugin_support",
+        "description": "Ambari stack changes for Ranger Kafka Plugin (AMBARI-11299)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "multiple_env_sh_files_support",
+        "description": "This feature is supported by RANGER and RANGER_KMS service to remove multiple env sh files during upgrade to stack 3.0",
+        "max_version": "3.2.0"
+      },
+      {
+        "name": "ranger",
+        "description": "Ranger Service support",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_admin_password_change",
+        "description": "Allow ranger admin credentials to be specified during cluster creation (AMBARI-17000)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_all_admin_change_default_password",
+        "description": "Supports Ranger All admin's password change in one go instead of multiple calls",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_audit_db_support",
+        "description": "Ranger Audit to DB support",
+        "min_version": "3.2.0",
+        "max_version": "3.2.0"
+      },
+      {
+        "name": "ranger_hive_plugin_jdbc_url",
+        "description": "Handle Ranger hive repo config jdbc url change for stack 2.5 (AMBARI-18386)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_install_infra_client",
+        "description": "Ambari Infra Service support",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_kerberos_support",
+        "description": "Ranger Kerberos support",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_kms_hsm_support",
+        "description": "Ranger KMS HSM support (AMBARI-15752)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_kms_pid_support",
+        "description": "Ranger KMS Service support pid generation",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_kms_ssl",
+        "description": "Ranger KMS SSL properties in ambari stack",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_log4j_support",
+        "description": "Ranger supporting log-4j properties (AMBARI-15681)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_logback_support",
+        "description": "Ranger supporting logback properties(RANGER2.3.0)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_pid_support",
+        "description": "Ranger Service support pid generation AMBARI-16756",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_setup_db_on_start",
+        "description": "Allows setup of ranger db and java patches to be called multiple times on each START",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_solr_config_support",
+        "description": "Showing Ranger solrconfig.xml on UI",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_support_security_zone_feature",
+        "description": "Need to add zoneName field in Ranger Audit Solr Collection",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_tagsync_component",
+        "description": "Ranger Tagsync component support (AMBARI-14383)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_tagsync_ssl_xml_support",
+        "description": "Ranger Tagsync ssl xml support.",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_usersync_non_root",
+        "description": "Ranger Usersync as non-root user (AMBARI-10416)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_usersync_password_jceks",
+        "description": "Saving Ranger Usersync credentials in jceks",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "ranger_xml_configuration",
+        "description": "Ranger code base support xml configurations",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "remove_ranger_hdfs_plugin_env",
+        "description": "HDFS removes Ranger env files (AMBARI-14299)",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "secure_ranger_ssl_password",
+        "description": "Securing Ranger Admin and Usersync SSL and Trustore related passwords in jceks",
+        "min_version": "3.2.0"
+      },
+      {
+        "name": "yarn_ranger_plugin_support",
+        "description": "Implement Stack changes for Ranger Yarn Plugin integration (AMBARI-10866)",
+        "min_version": "3.2.0"
       }
     ]
   }
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/properties/stack_packages.json	(date 1723876483629)
@@ -20,7 +20,8 @@
           "STACK-SELECT-PACKAGE": "hbase-master",
           "INSTALL": [
             "hbase-master",
-            "hbase-client"
+            "hbase-client",
+            "phoenix-client"
           ],
           "PATCH": [
             "hbase-master"
@@ -33,7 +34,8 @@
           "STACK-SELECT-PACKAGE": "hbase-regionserver",
           "INSTALL": [
             "hbase-regionserver",
-            "hbase-client"
+            "hbase-client",
+            "phoenix-client"
           ],
           "PATCH": [
             "hbase-regionserver"
@@ -420,6 +422,58 @@
             "sqoop-client"
           ]
         }
+      },
+      "RANGER": {
+        "RANGER_ADMIN": {
+          "STACK-SELECT-PACKAGE": "ranger-admin",
+          "INSTALL": [
+            "ranger-admin"
+          ],
+          "PATCH": [
+            "ranger-admin"
+          ],
+          "STANDARD": [
+            "ranger-admin"
+          ]
+        },
+        "RANGER_TAGSYNC": {
+          "STACK-SELECT-PACKAGE": "ranger-tagsync",
+          "INSTALL": [
+            "ranger-tagsync"
+          ],
+          "PATCH": [
+            "ranger-tagsync"
+          ],
+          "STANDARD": [
+            "ranger-tagsync"
+          ]
+        },
+        "RANGER_USERSYNC": {
+          "STACK-SELECT-PACKAGE": "ranger-usersync",
+          "INSTALL": [
+            "ranger-usersync"
+          ],
+          "PATCH": [
+            "ranger-usersync"
+          ],
+          "STANDARD": [
+            "ranger-usersync"
+          ]
+        }
+      },
+      "RANGER_KMS": {
+        "RANGER_KMS_SERVER": {
+          "STACK-SELECT-PACKAGE": "ranger-kms",
+          "INSTALL": [
+            "ranger-kms"
+          ],
+          "PATCH": [
+            "ranger-kms"
+          ],
+          "STANDARD": [
+            "ranger-kms"
+          ]
+        }
       }
     },
     "conf-select": {
@@ -520,6 +574,34 @@
           "current_dir": "{0}/current/sqoop-client/conf",
           "component": "sqoop-client"
         }
+      ],
+      "ranger-admin": [
+        {
+          "conf_dir": "/etc/ranger/admin/conf",
+          "current_dir": "{0}/current/ranger-admin/conf",
+          "component": "ranger-admin"
+        }
+      ],
+      "ranger-kms": [
+        {
+          "conf_dir": "/etc/ranger/kms/conf",
+          "current_dir": "{0}/current/ranger-kms/conf",
+          "component": "ranger-kms"
+        }
+      ],
+      "ranger-tagsync": [
+        {
+          "conf_dir": "/etc/ranger/tagsync/conf",
+          "current_dir": "{0}/current/ranger-tagsync/conf",
+          "component": "ranger-tagsync"
+        }
+      ],
+      "ranger-usersync": [
+        {
+          "conf_dir": "/etc/ranger/usersync/conf",
+          "current_dir": "{0}/current/ranger-usersync/conf",
+          "component": "ranger-usersync"
+        }
       ]
     },
     "conf-select-patching": {
@@ -582,6 +664,18 @@
         "packages": [
           "sqoop"
         ]
+      },
+      "RANGER": {
+        "packages": [
+          "ranger-admin",
+          "ranger-usersync",
+          "ranger-tagsync"
+        ]
+      },
+      "RANGER_KMS": {
+        "packages": [
+          "ranger-kms"
+        ]
       }
     },
     "upgrade-dependencies": {
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/multinode-default.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/multinode-default.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/multinode-default.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/multinode-default.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/multinode-default.json	(date 1719626239000)
@@ -95,6 +95,9 @@
                 },
                 {
                     "name" : "MAPREDUCE2_CLIENT"
+                },
+                {
+                    "name" : "SQOOP"
                 }
             ],
             "cardinality" : "1"
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/singlenode-default.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/singlenode-default.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/singlenode-default.json
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/singlenode-default.json	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/blueprints/singlenode-default.json	(date 1719626239000)
@@ -1,65 +1,68 @@
 {
-    "configurations" : [
-    ],
-    "host_groups" : [
-        {
-            "name" : "host_group_1",
-            "components" : [
-                {
-                    "name" : "HISTORYSERVER"
-                },
-                {
-                    "name" : "NAMENODE"
-                },
-                {
-                    "name" : "SUPERVISOR"
-                },
-                {
-                    "name" : "AMBARI_SERVER"
-                },
-                {
-                    "name" : "APP_TIMELINE_SERVER"
-                },
-                {
-                    "name" : "HDFS_CLIENT"
-                },
-                {
-                    "name" : "NODEMANAGER"
-                },
-                {
-                    "name" : "DATANODE"
-                },
-                {
-                    "name" : "RESOURCEMANAGER"
-                },
-                {
-                    "name" : "ZOOKEEPER_SERVER"
-                },
-                {
-                    "name" : "ZOOKEEPER_CLIENT"
-                },
-                {
-                    "name" : "SECONDARY_NAMENODE"
-                },
-                {
-                    "name" : "YARN_CLIENT"
-                },
-                {
-                    "name" : "MAPREDUCE2_CLIENT"
-                },
-                {
-                    "name" : "POSTGRESQL_SERVER"
-                },
-                {
-                    "name" : "DRPC_SERVER"
-                }
-            ],
-            "cardinality" : "1"
-        }
-    ],
-    "Blueprints" : {
-        "blueprint_name" : "blueprint-singlenode-default",
-        "stack_name" : "BIGTOP",
-        "stack_version" : "3.2.0"
-    }
+  "configurations" : [
+  ],
+  "host_groups" : [
+    {
+      "name" : "host_group_1",
+      "components" : [
+        {
+          "name" : "HISTORYSERVER"
+        },
+        {
+          "name" : "NAMENODE"
+        },
+        {
+          "name" : "SUPERVISOR"
+        },
+        {
+          "name" : "AMBARI_SERVER"
+        },
+        {
+          "name" : "APP_TIMELINE_SERVER"
+        },
+        {
+          "name" : "HDFS_CLIENT"
+        },
+        {
+          "name" : "NODEMANAGER"
+        },
+        {
+          "name" : "DATANODE"
+        },
+        {
+          "name" : "RESOURCEMANAGER"
+        },
+        {
+          "name" : "ZOOKEEPER_SERVER"
+        },
+        {
+          "name" : "ZOOKEEPER_CLIENT"
+        },
+        {
+          "name" : "SECONDARY_NAMENODE"
+        },
+        {
+          "name" : "YARN_CLIENT"
+        },
+        {
+          "name" : "MAPREDUCE2_CLIENT"
+        },
+        {
+          "name" : "POSTGRESQL_SERVER"
+        },
+        {
+          "name": "DRPC_SERVER"
+        },
+        {
+          "name": "SQOOP"
+        }
+      ],
+      "cardinality" : "1"
+    }
+  ],
+  "Blueprints" : {
+    "blueprint_name" : "blueprint-singlenode-default",
+    "stack_name" : "BIGTOP",
+    "stack_version" : "3.2.0"
+  }
 }
Index: ambari-common/src/main/python/resource_management/libraries/functions/list_ambari_managed_repos.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-common/src/main/python/resource_management/libraries/functions/list_ambari_managed_repos.py b/ambari-common/src/main/python/resource_management/libraries/functions/list_ambari_managed_repos.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-common/src/main/python/resource_management/libraries/functions/list_ambari_managed_repos.py	(date 1719626239000)
@@ -0,0 +1,57 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Ambari Agent
+
+"""
+
+__all__ = ["list_ambari_managed_repos"]
+import os
+import glob
+from ambari_commons.os_check import OSCheck
+from resource_management.core.exceptions import Fail
+
+
+def list_ambari_managed_repos(stack_name):
+  """
+  Lists all repositories that are present at host
+  """
+  stack_name = stack_name.upper()
+  # TODO : get it dynamically from the server
+  repository_names = [stack_name, stack_name + "-UTILS" ]
+  if OSCheck.is_ubuntu_family():
+    repo_dir = '/etc/apt/sources.list.d/'
+  elif OSCheck.is_redhat_family():  # Centos/RHEL 5/6
+    repo_dir = '/etc/yum.repos.d/'
+  elif OSCheck.is_suse_family():
+    repo_dir = '/etc/zypp/repos.d/'
+  else:
+    raise Fail('Can not dermine repo dir')
+  repos = []
+  for name in repository_names:
+    # List all files that match pattern
+    files = glob.glob(os.path.join(repo_dir, name) + '*')
+    for f in files:
+      filename = os.path.basename(f)
+      # leave out extension
+      reponame = os.path.splitext(filename)[0]
+      repos.append(reponame)
+  # get uniq strings
+  seen = set()
+  uniq = [s for s in repos if not (s in seen or seen.add(s))]
+  return uniq
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-audit.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-audit.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-audit.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-audit.xml	(date 1719626239000)
@@ -0,0 +1,132 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <!-- These configs were inherited from HDP 2.3 -->
+  <property>
+    <name>xasecure.audit.is.enabled</name>
+    <value>true</value>
+    <description>Is Audit enabled?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs</name>
+    <value>true</value>
+    <display-name>Audit to HDFS</display-name>
+    <description>Is Audit to HDFS enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.dir</name>
+    <value>hdfs://NAMENODE_HOSTNAME:8020/ranger/audit</value>
+    <description>HDFS folder to write audit to, make sure the service user has requried permissions</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs.dir</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.batch.filespool.dir</name>
+    <value>/var/log/hadoop/hdfs/audit/hdfs/spool</value>
+    <description>/var/log/hadoop/hdfs/audit/hdfs/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr</name>
+    <value>false</value>
+    <display-name>Audit to SOLR</display-name>
+    <description>Is Solr audit enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.solr</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.urls</name>
+    <value/>
+    <description>Solr URL</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.urls</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.zookeepers</name>
+    <value>NONE</value>
+    <description>Solr Zookeeper string</description>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.zookeepers</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.batch.filespool.dir</name>
+    <value>/var/log/hadoop/hdfs/audit/solr/spool</value>
+    <description>/var/log/hadoop/hdfs/audit/solr/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.provider.summary.enabled</name>
+    <value>false</value>
+    <display-name>Audit provider summary enabled</display-name>
+    <description>Enable Summary audit?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.hdfs.ambari.cluster.name</name>
+    <value>{{cluster_name}}</value>
+    <description>Capture cluster name from where Ranger hdfs plugin is enabled.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/viewfs-mount-table.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/viewfs-mount-table.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/viewfs-mount-table.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/viewfs-mount-table.xml	(date 1719626239000)
@@ -0,0 +1,41 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="false">
+    <!-- These configs were inherited from HDP 2.2 -->
+    <property>
+        <name>content</name>
+        <display-name>viewFS mountTable</display-name>
+        <description>The mount tables can be described in core-site.xml but it is better
+                     to use indirection in core-site.xml to reference a separate configuration file.
+                     The content of this property will be serialized into separate configuration
+                     file and referenced from the core-site using xi:include tag
+        </description>
+        <value>
+        </value>
+        <value-attributes>
+            <type>content</type>
+            <show-property-name>false</show-property-name>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-security.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-security.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-security.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-security.xml	(date 1719626239000)
@@ -0,0 +1,71 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <!-- These configs were inherited from HDP 2.3 -->
+  <property>
+    <name>ranger.plugin.hdfs.service.name</name>
+    <value>{{repo_name}}</value>
+    <description>Name of the Ranger service containing Hdfs policies</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.hdfs.policy.source.impl</name>
+    <value>org.apache.ranger.admin.client.RangerAdminRESTClient</value>
+    <description>Class to retrieve policies from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.hdfs.policy.rest.url</name>
+    <value>{{policymgr_mgr_url}}</value>
+    <description>URL to Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>policymgr_external_url</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>ranger.plugin.hdfs.policy.rest.ssl.config.file</name>
+    <value>/etc/hadoop/conf/ranger-policymgr-ssl.xml</value>
+    <description>Path to the file containing SSL details to contact Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.hdfs.policy.pollIntervalMs</name>
+    <value>30000</value>
+    <description>How often to poll for changes in policies?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.hdfs.policy.cache.dir</name>
+    <value>/etc/ranger/{{repo_name}}/policycache</value>
+    <description>Directory where Ranger policies are cached after successful retrieval from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.add-hadoop-authorization</name>
+    <value>true</value>
+    <description>Enable/Disable the default hadoop authorization (based on rwxrwxrwx permission on the resource) if Ranger Authorization fails.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-policymgr-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-policymgr-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-policymgr-ssl.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-policymgr-ssl.xml	(date 1719626239000)
@@ -0,0 +1,73 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <!-- These configs were inherited from HDP 2.3 -->
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <description>password for keystore</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>java truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-metrics2.properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-metrics2.properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-metrics2.properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/hadoop-metrics2.properties.xml	(date 1719626239000)
@@ -0,0 +1,115 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+
+<configuration>
+  <!-- hadoop-metrics2.properties -->
+  <property>
+    <name>content</name>
+    <display-name>hadoop-metrics2.properties template</display-name>
+    <description>This is the jinja template for hadoop-metrics2.properties file</description>
+    <value>
+{% if has_ganglia_server %}
+*.period=60
+
+*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
+*.sink.ganglia.period=10
+
+# default for supportsparse is false
+*.sink.ganglia.supportsparse=true
+
+.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both
+.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40
+
+# Hook up to the server
+namenode.sink.ganglia.servers={{ganglia_server_host}}:8661
+datanode.sink.ganglia.servers={{ganglia_server_host}}:8659
+jobtracker.sink.ganglia.servers={{ganglia_server_host}}:8662
+tasktracker.sink.ganglia.servers={{ganglia_server_host}}:8658
+maptask.sink.ganglia.servers={{ganglia_server_host}}:8660
+reducetask.sink.ganglia.servers={{ganglia_server_host}}:8660
+resourcemanager.sink.ganglia.servers={{ganglia_server_host}}:8664
+nodemanager.sink.ganglia.servers={{ganglia_server_host}}:8657
+historyserver.sink.ganglia.servers={{ganglia_server_host}}:8666
+journalnode.sink.ganglia.servers={{ganglia_server_host}}:8654
+nimbus.sink.ganglia.servers={{ganglia_server_host}}:8649
+supervisor.sink.ganglia.servers={{ganglia_server_host}}:8650
+
+resourcemanager.sink.ganglia.tagsForPrefix.yarn=Queue
+
+{% endif %}
+
+{% if has_metric_collector %}
+
+*.period={{metrics_collection_period}}
+*.sink.timeline.plugin.urls=file:///usr/lib/ambari-metrics-hadoop-sink/ambari-metrics-hadoop-sink.jar
+*.sink.timeline.class=org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink
+*.sink.timeline.period={{metrics_collection_period}}
+*.sink.timeline.sendInterval={{metrics_report_interval}}000
+*.sink.timeline.slave.host.name={{hostname}}
+*.sink.timeline.zookeeper.quorum={{zookeeper_quorum}}
+*.sink.timeline.protocol={{metric_collector_protocol}}
+*.sink.timeline.port={{metric_collector_port}}
+*.sink.timeline.instanceId = {{cluster_name}}
+*.sink.timeline.set.instanceId = {{set_instanceId}}
+*.sink.timeline.host_in_memory_aggregation = {{host_in_memory_aggregation}}
+*.sink.timeline.host_in_memory_aggregation_port = {{host_in_memory_aggregation_port}}
+{% if is_aggregation_https_enabled %}
+*.sink.timeline.host_in_memory_aggregation_protocol = {{host_in_memory_aggregation_protocol}}
+{% endif %}
+
+# HTTPS properties
+*.sink.timeline.truststore.path = {{metric_truststore_path}}
+*.sink.timeline.truststore.type = {{metric_truststore_type}}
+*.sink.timeline.truststore.password = {{metric_truststore_password}}
+
+datanode.sink.timeline.collector.hosts={{ams_collector_hosts}}
+namenode.sink.timeline.collector.hosts={{ams_collector_hosts}}
+resourcemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}
+nodemanager.sink.timeline.collector.hosts={{ams_collector_hosts}}
+jobhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}
+journalnode.sink.timeline.collector.hosts={{ams_collector_hosts}}
+maptask.sink.timeline.collector.hosts={{ams_collector_hosts}}
+reducetask.sink.timeline.collector.hosts={{ams_collector_hosts}}
+applicationhistoryserver.sink.timeline.collector.hosts={{ams_collector_hosts}}
+
+resourcemanager.sink.timeline.tagsForPrefix.yarn=Queue
+
+{% if is_nn_client_port_configured %}
+# Namenode rpc ports customization
+namenode.sink.timeline.metric.rpc.client.port={{nn_rpc_client_port}}
+{% endif %}
+{% if is_nn_dn_port_configured %}
+namenode.sink.timeline.metric.rpc.datanode.port={{nn_rpc_dn_port}}
+{% endif %}
+{% if is_nn_healthcheck_port_configured %}
+namenode.sink.timeline.metric.rpc.healthcheck.port={{nn_rpc_healthcheck_port}}
+{% endif %}
+
+{% endif %}
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-plugin-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-plugin-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-plugin-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/configuration/ranger-hdfs-plugin-properties.xml	(date 1719626239000)
@@ -0,0 +1,144 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+  <!-- These configs were inherited from HDP 2.2 -->
+  <property>
+    <name>policy_user</name>
+    <value>ambari-qa</value>
+    <display-name>Policy user for HDFS</display-name>
+    <description>This user must be system user and also present at Ranger
+      admin portal</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.rpc.protection</name>
+    <value>authentication</value>
+    <description>Used for repository creation on ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false" />
+  </property>
+  <property>
+    <name>common.name.for.certificate</name>
+    <value/>
+    <description>Common name for certificate, this value should match what is specified in repo within ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-hdfs-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Enable Ranger for HDFS</display-name>
+    <description>Enable ranger hdfs plugin</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>ranger-hdfs-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <value-attributes>
+      <type>boolean</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_USERNAME</name>
+    <value>hadoop</value>
+    <display-name>Ranger repository config user</display-name>
+    <description>Used for repository creation on ranger admin
+    </description>
+    <depends-on>
+      <property>
+        <type>ranger-hdfs-plugin-properties</type>
+        <name>ranger-hdfs-plugin-enabled</name>
+      </property>
+      <property>
+        <type>hadoop-env</type>
+        <name>hdfs_user</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_PASSWORD</name>
+    <value>hadoop</value>
+    <display-name>Ranger repository config password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Used for repository creation on ranger admin
+    </description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_admin_username</name>
+    <value></value>
+    <display-name>External Ranger admin username</display-name>
+    <description>Add ranger default admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_admin_password</name>
+    <value></value>
+    <display-name>External Ranger admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_username</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin username</display-name>
+    <description>Add ranger default ambari admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_password</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default ambari admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/service_advisor.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/service_advisor.py	(date 1719626239000)
@@ -0,0 +1,991 @@
+#!/usr/bin/env ambari-python-wrap
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+disass HDFSRecommender(service_advisor.ServiceAdvisributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+# Python imports
+import imp
+import os
+import traceback
+import inspect
+import re
+
+# Local imports
+from resource_management.libraries.functions.mounted_dirs_helper import get_mounts_with_multiple_data_dirs
+
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../../../stacks/')
+PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+MEMORY_SIZE_PATTERN = re.compile("([0-9]+)([kmg]?)")
+
+try:
+    if "BASE_SERVICE_ADVISOR" in os.environ:
+        PARENT_FILE = os.environ["BASE_SERVICE_ADVISOR"]
+    with open(PARENT_FILE, 'rb') as fp:
+        service_advisor = imp.load_module('service_advisor', fp, PARENT_FILE, ('.py', 'rb', imp.PY_SOURCE))
+except Exception as e:
+    traceback.print_exc()
+    print "Failed to load parent."
+
+
+def convertToMegabytes(value):
+    """
+    Using this method the given value will be converted to megabytes based on the following rules:
+      1024  -> 1024
+      1024k -> 1
+      1024m -> 1024
+      1g    -> 1024
+      1024[a-Z] -> 1024
+    """
+    modifier = {'k': float(1) / 1024, 'm': 1, 'g': 1024, '': 1}
+    pattern = MEMORY_SIZE_PATTERN.match(str(value))
+    return long(long(pattern.group(1)) * modifier[pattern.group(2)])
+
+
+class HDFSServiceAdvisor(service_advisor.ServiceAdvisor):
+
+    def __init__(self, *args, **kwargs):
+        self.as_super = super(HDFSServiceAdvisor, self)
+        self.as_super.__init__(*args, **kwargs)
+
+        self.initialize_logger("HDFSServiceAdvisor")
+
+        # Always call these methods
+        self.modifyMastersWithMultipleInstances()
+        self.modifyCardinalitiesDict()
+        self.modifyHeapSizeProperties()
+        self.modifyNotValuableComponents()
+        self.modifyComponentsNotPreferableOnServer()
+        self.modifyComponentLayoutSchemes()
+
+    def modifyMastersWithMultipleInstances(self):
+        """
+        Modify the set of masters with multiple instances.
+        Must be overridden in child class.
+        """
+        # Nothing to do
+        pass
+
+    def modifyCardinalitiesDict(self):
+        """
+        Modify the dictionary of cardinalities.
+        Must be overridden in child class.
+        """
+        # Nothing to do
+        pass
+
+    def modifyHeapSizeProperties(self):
+        """
+        Modify the dictionary of heap size properties.
+        Must be overridden in child class.
+        """
+        self.heap_size_properties = {"NAMENODE":
+                                         [{"config-name": "hadoop-env",
+                                           "property": "namenode_heapsize",
+                                           "default": "1024m"}],
+                                     "SECONDARY_NAMENODE":
+                                         [{"config-name": "hadoop-env",
+                                           "property": "namenode_heapsize",
+                                           "default": "1024m"}],
+                                     "DATANODE":
+                                         [{"config-name": "hadoop-env",
+                                           "property": "dtnode_heapsize",
+                                           "default": "1024m"}]}
+
+    def modifyNotValuableComponents(self):
+        """
+        Modify the set of components whose host assignment is based on other services.
+        Must be overridden in child class.
+        """
+        self.notValuableComponents |= set(['JOURNALNODE', 'ZKFC'])
+
+    def modifyComponentsNotPreferableOnServer(self):
+        """
+        Modify the set of components that are not preferable on the server.
+        Must be overridden in child class.
+        """
+        # Nothing to do
+        pass
+
+    def modifyComponentLayoutSchemes(self):
+        """
+        Modify layout scheme dictionaries for components.
+        The scheme dictionary basically maps the number of hosts to
+        host index where component should exist.
+        Must be overridden in child class.
+        """
+        self.componentLayoutSchemes.update({
+            'NAMENODE': {"else": 0},
+            'SECONDARY_NAMENODE': {"else": 1}
+        })
+
+    def getServiceComponentLayoutValidations(self, services, hosts):
+        """
+        Get a list of errors.
+        Must be overridden in child class.
+        """
+        self.logger.info("Class: %s, Method: %s. Validating Service Component Layout." %
+                         (self.__class__.__name__, inspect.stack()[0][3]))
+
+        # HDFS allows NameNode and Secondary NameNode to be on the same host.
+        return self.getServiceComponentCardinalityValidations(services, hosts, "HDFS")
+
+    def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+        """
+        Entry point.
+        Must be overridden in child class.
+        """
+        self.logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+                         (self.__class__.__name__, inspect.stack()[0][3]))
+
+        # Due to the existing stack inheritance, make it clear where each calculation came from.
+        recommender = HDFSRecommender()
+        recommender.recommendConfigurationsFromHDP206(configurations, clusterData, services, hosts)
+        recommender.recommendConfigurationsFromHDP22(configurations, clusterData, services, hosts)
+        recommender.recommendConfigurationsFromHDP23(configurations, clusterData, services, hosts)
+        recommender.recommendConfigurationsFromHDP26(configurations, clusterData, services, hosts)
+        recommender.recommendHDFSConfigurationsFromHDP31(configurations, clusterData, services, hosts)
+        recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+    def getServiceConfigurationRecommendationsForSSO(self, configurations, clusterData, services, hosts):
+        """
+        Entry point.
+        Must be overridden in child class.
+        """
+        recommender = HDFSRecommender()
+        recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+    def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+        """
+        Entry point.
+        Validate configurations for the service. Return a list of errors.
+        The code for this function should be the same for each Service Advisor.
+        """
+        self.logger.info("Class: %s, Method: %s. Validating Configurations." %
+                         (self.__class__.__name__, inspect.stack()[0][3]))
+
+        validator = HDFSValidator()
+        # Calls the methods of the validator using arguments,
+        # method(siteProperties, siteRecommendations, configurations, services, hosts)
+        return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts,
+                                                         validator.validators)
+
+    def isComponentUsingCardinalityForLayout(self, componentName):
+        return componentName == 'NFS_GATEWAY'
+
+
+class HDFSRecommender(service_advisor.ServiceAdvisor):
+    """
+    HDFS Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+    """
+
+    def __init__(self, *args, **kwargs):
+        self.as_super = super(HDFSRecommender, self)
+        self.as_super.__init__(*args, **kwargs)
+
+    def recommendConfigurationsFromHDP206(self, configurations, clusterData, services, hosts):
+        """
+        Recommend configurations for this service based on HDP 2.0.6.
+        """
+        self.logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+                         (self.__class__.__name__, inspect.stack()[0][3]))
+
+        putHDFSProperty = self.putProperty(configurations, "hadoop-env", services)
+        putHDFSSiteProperty = self.putProperty(configurations, "hdfs-site", services)
+        putHDFSSitePropertyAttributes = self.putPropertyAttribute(configurations, "hdfs-site")
+
+        totalAvailableRam = clusterData['totalAvailableRam']
+        self.logger.info("Class: %s, Method: %s. Total Available Ram: %s" % (
+        self.__class__.__name__, inspect.stack()[0][3], str(totalAvailableRam)))
+        putHDFSProperty('namenode_heapsize', max(convertToMegabytes(totalAvailableRam / 2), 1024))
+        putHDFSProperty = self.putProperty(configurations, "hadoop-env", services)
+        putHDFSProperty('namenode_opt_newsize', max(convertToMegabytes(totalAvailableRam / 8), 128))
+        putHDFSProperty = self.putProperty(configurations, "hadoop-env", services)
+        putHDFSProperty('namenode_opt_maxnewsize', max(convertToMegabytes(totalAvailableRam / 8), 256))
+
+        # Check if NN HA is enabled and recommend removing dfs.namenode.rpc-address
+        hdfsSiteProperties = self.getServicesSiteProperties(services, "hdfs-site")
+        nameServices = None
+        if hdfsSiteProperties and 'dfs.internal.nameservices' in hdfsSiteProperties:
+            nameServices = hdfsSiteProperties['dfs.internal.nameservices']
+        if nameServices is None and hdfsSiteProperties and 'dfs.nameservices' in hdfsSiteProperties:
+            nameServices = hdfsSiteProperties['dfs.nameservices']
+        if nameServices and "dfs.ha.namenodes.%s" % nameServices in hdfsSiteProperties:
+            namenodes = hdfsSiteProperties["dfs.ha.namenodes.%s" % nameServices]
+            if len(namenodes.split(',')) > 1:
+                putHDFSSitePropertyAttributes("dfs.namenode.rpc-address", "delete", "true")
+
+        self.logger.info("Class: %s, Method: %s. HDFS nameservices: %s" %
+                         (self.__class__.__name__, inspect.stack()[0][3], str(nameServices)))
+
+        # hdfs_mount_properties = [
+        #   ("dfs.datanode.data.dir", "DATANODE", "/hadoop/hdfs/data", "multi"),
+        #   ("dfs.namenode.name.dir", "DATANODE", "/hadoop/hdfs/namenode", "multi"),
+        #   ("dfs.namenode.checkpoint.dir", "SECONDARY_NAMENODE", "/hadoop/hdfs/namesecondary", "single")
+        # ]
+
+        # self.logger.info("Class: %s, Method: %s. Updating HDFS mount properties." %
+        #             (self.__class__.__name__, inspect.stack()[0][3]))
+        # self.updateMountProperties("hdfs-site", hdfs_mount_properties, configurations, services, hosts)
+
+        dataDirs = []
+        if configurations and "hdfs-site" in configurations and \
+                "dfs.datanode.data.dir" in configurations["hdfs-site"]["properties"] and \
+                configurations["hdfs-site"]["properties"]["dfs.datanode.data.dir"] is not None:
+            dataDirs = configurations["hdfs-site"]["properties"]["dfs.datanode.data.dir"].split(",")
+
+        elif hdfsSiteProperties and "dfs.datanode.data.dir" in hdfsSiteProperties and \
+                hdfsSiteProperties["dfs.datanode.data.dir"] is not None:
+            dataDirs = hdfsSiteProperties["dfs.datanode.data.dir"].split(",")
+
+        self.logger.info("Class: %s, Method: %s. HDFS Data Dirs: %s" %
+                         (self.__class__.__name__, inspect.stack()[0][3], str(dataDirs)))
+
+        # dfs.datanode.du.reserved should be set to 10-15% of volume size
+        # For each host selects maximum size of the volume. Then gets minimum for all hosts.
+        # This ensures that each host will have at least one data dir with available space.
+        reservedSizeRecommendation = 0l  # kBytes
+        for host in hosts["items"]:
+            mountPoints = []
+            mountPointDiskAvailableSpace = []  # kBytes
+            for diskInfo in host["Hosts"]["disk_info"]:
+                mountPoints.append(diskInfo["mountpoint"])
+                mountPointDiskAvailableSpace.append(long(diskInfo["size"]))
+
+            maxFreeVolumeSizeForHost = 0l  # kBytes
+            for dataDir in dataDirs:
+                mp = HDFSRecommender.getMountPointForDir(dataDir, mountPoints)
+                for i in range(len(mountPoints)):
+                    if mp == mountPoints[i]:
+                        if mountPointDiskAvailableSpace[i] > maxFreeVolumeSizeForHost:
+                            maxFreeVolumeSizeForHost = mountPointDiskAvailableSpace[i]
+
+            if (not reservedSizeRecommendation) or (
+                    maxFreeVolumeSizeForHost and maxFreeVolumeSizeForHost < reservedSizeRecommendation):
+                reservedSizeRecommendation = maxFreeVolumeSizeForHost
+
+        self.logger.info("Class: %s, Method: %s. HDFS Datanode recommended reserved size: %d" %
+                         (self.__class__.__name__, inspect.stack()[0][3], reservedSizeRecommendation))
+
+        if reservedSizeRecommendation:
+            reservedSizeRecommendation = max(reservedSizeRecommendation * 1024 / 8,
+                                             1073741824)  # At least 1Gb is reserved
+            putHDFSSiteProperty('dfs.datanode.du.reserved', reservedSizeRecommendation)  # Bytes
+
+        # recommendations for "hadoop.proxyuser.*.hosts", "hadoop.proxyuser.*.groups" properties in core-site
+        self.recommendHadoopProxyUsers(configurations, services, hosts)
+
+    def recommendConfigurationsFromHDP22(self, configurations, clusterData, services, hosts):
+        """
+        Recommend configurations for this service based on HDP 2.2
+        """
+        putHdfsSiteProperty = self.putProperty(configurations, "hdfs-site", services)
+        putCoreSiteProperty = self.putProperty(configurations, "core-site", services)
+        putHdfsSitePropertyAttribute = self.putPropertyAttribute(configurations, "hdfs-site")
+        putHdfsSiteProperty("dfs.datanode.max.transfer.threads", 16384 if clusterData["hBaseInstalled"] else 4096)
+
+        dataDirsCount = 1
+        # Use users 'dfs.datanode.data.dir' first
+        if "hdfs-site" in services["configurations"] and "dfs.datanode.data.dir" in \
+                services["configurations"]["hdfs-site"][
+                    "properties"]:
+            dataDirsCount = len(
+                str(services["configurations"]["hdfs-site"]["properties"]["dfs.datanode.data.dir"]).split(","))
+        elif "dfs.datanode.data.dir" in configurations["hdfs-site"]["properties"]:
+            dataDirsCount = len(str(configurations["hdfs-site"]["properties"]["dfs.datanode.data.dir"]).split(","))
+        if dataDirsCount <= 2:
+            failedVolumesTolerated = 0
+        elif dataDirsCount <= 4:
+            failedVolumesTolerated = 1
+        else:
+            failedVolumesTolerated = 2
+        putHdfsSiteProperty("dfs.datanode.failed.volumes.tolerated", failedVolumesTolerated)
+
+        namenodeHosts = self.getHostsWithComponent("HDFS", "NAMENODE", services, hosts)
+
+        # 25 * # of cores on NameNode
+        nameNodeCores = 4
+        if namenodeHosts is not None and len(namenodeHosts):
+            nameNodeCores = int(namenodeHosts[0]['Hosts']['cpu_count'])
+        putHdfsSiteProperty("dfs.namenode.handler.count", 25 * nameNodeCores)
+        if 25 * nameNodeCores > 200:
+            putHdfsSitePropertyAttribute("dfs.namenode.handler.count", "maximum", 25 * nameNodeCores)
+
+        servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+        if ('ranger-hdfs-plugin-properties' in services['configurations']) and (
+                'ranger-hdfs-plugin-enabled' in services['configurations']['ranger-hdfs-plugin-properties'][
+            'properties']):
+            rangerPluginEnabled = services['configurations']['ranger-hdfs-plugin-properties']['properties'][
+                'ranger-hdfs-plugin-enabled']
+            if ("RANGER" in servicesList) and (rangerPluginEnabled.lower() == 'Yes'.lower()):
+                putHdfsSiteProperty("dfs.permissions.enabled", 'true')
+
+        putHdfsSiteProperty("dfs.namenode.safemode.threshold-pct", "0.999" if len(namenodeHosts) > 1 else "1.000")
+
+        putHdfsEnvProperty = self.putProperty(configurations, "hadoop-env", services)
+        putHdfsEnvPropertyAttribute = self.putPropertyAttribute(configurations, "hadoop-env")
+
+        putHdfsEnvProperty('namenode_heapsize', max(convertToMegabytes(clusterData['totalAvailableRam'] / 2), 1024))
+
+        nn_heapsize_limit = None
+        if (namenodeHosts is not None and len(namenodeHosts) > 0):
+            if len(namenodeHosts) > 1:
+                nn_max_heapsize = min(convertToMegabytes(namenodeHosts[0]["Hosts"]["total_mem"]),
+                                      convertToMegabytes(namenodeHosts[1]["Hosts"]["total_mem"])) / 1024
+                masters_at_host = max(
+                    self.getHostComponentsByCategories(namenodeHosts[0]["Hosts"]["host_name"], ["MASTER"], services,
+                                                       hosts),
+                    self.getHostComponentsByCategories(namenodeHosts[1]["Hosts"]["host_name"], ["MASTER"], services,
+                                                       hosts))
+            else:
+                nn_max_heapsize = convertToMegabytes(namenodeHosts[0]["Hosts"]["total_mem"] / 1024)  # total_mem in kb
+                masters_at_host = self.getHostComponentsByCategories(namenodeHosts[0]["Hosts"]["host_name"], ["MASTER"],
+                                                                     services, hosts)
+
+            putHdfsEnvPropertyAttribute('namenode_heapsize', 'maximum', max(nn_max_heapsize, 1024))
+
+            nn_heapsize_limit = nn_max_heapsize
+            nn_heapsize_limit -= clusterData["reservedRam"]
+            if len(masters_at_host) > 1:
+                nn_heapsize_limit = convertToMegabytes(nn_heapsize_limit / 2)
+
+            putHdfsEnvProperty('namenode_heapsize', max(nn_heapsize_limit, 1024))
+
+        datanodeHosts = self.getHostsWithComponent("HDFS", "DATANODE", services, hosts)
+        if datanodeHosts is not None and len(datanodeHosts) > 0:
+            min_datanode_ram_kb = 1073741824  # 1 TB
+            for datanode in datanodeHosts:
+                ram_kb = datanode['Hosts']['total_mem']
+                min_datanode_ram_kb = min(min_datanode_ram_kb, ram_kb)
+
+            datanodeFilesM = len(datanodeHosts) * dataDirsCount / 10  # in millions, # of files = # of disks * 100'000
+            nn_memory_configs = [
+                {'nn_heap': 1024, 'nn_opt': 128},
+                {'nn_heap': 3072, 'nn_opt': 512},
+                {'nn_heap': 5376, 'nn_opt': 768},
+                {'nn_heap': 9984, 'nn_opt': 1280},
+                {'nn_heap': 14848, 'nn_opt': 2048},
+                {'nn_heap': 19456, 'nn_opt': 2560},
+                {'nn_heap': 24320, 'nn_opt': 3072},
+                {'nn_heap': 33536, 'nn_opt': 4352},
+                {'nn_heap': 47872, 'nn_opt': 6144},
+                {'nn_heap': 59648, 'nn_opt': 7680},
+                {'nn_heap': 71424, 'nn_opt': 8960},
+                {'nn_heap': 94976, 'nn_opt': 8960}
+            ]
+            index = {
+                datanodeFilesM < 1: 0,
+                1 <= datanodeFilesM < 5: 1,
+                5 <= datanodeFilesM < 10: 2,
+                10 <= datanodeFilesM < 20: 3,
+                20 <= datanodeFilesM < 30: 4,
+                30 <= datanodeFilesM < 40: 5,
+                40 <= datanodeFilesM < 50: 6,
+                50 <= datanodeFilesM < 70: 7,
+                70 <= datanodeFilesM < 100: 8,
+                100 <= datanodeFilesM < 125: 9,
+                125 <= datanodeFilesM < 150: 10,
+                150 <= datanodeFilesM: 11
+            }[1]
+
+            nn_memory_config = nn_memory_configs[index]
+
+            # override with new values if applicable
+            if nn_heapsize_limit is not None and nn_memory_config['nn_heap'] <= nn_heapsize_limit:
+                putHdfsEnvProperty('namenode_heapsize', nn_memory_config['nn_heap'])
+
+            putHdfsEnvPropertyAttribute('dtnode_heapsize', 'maximum', int(min_datanode_ram_kb / 1024))
+
+        nn_heapsize = convertToMegabytes(configurations["hadoop-env"]["properties"]["namenode_heapsize"])
+        putHdfsEnvProperty('namenode_opt_newsize', max(convertToMegabytes(nn_heapsize / 8), 128))
+        putHdfsEnvProperty('namenode_opt_maxnewsize', max(convertToMegabytes(nn_heapsize / 8), 128))
+
+        putHdfsSitePropertyAttribute = self.putPropertyAttribute(configurations, "hdfs-site")
+        putHdfsSitePropertyAttribute('dfs.datanode.failed.volumes.tolerated', 'maximum', dataDirsCount)
+
+        keyserverHostsString = None
+        keyserverPortString = None
+        if "hadoop-env" in services["configurations"] and "keyserver_host" in services["configurations"]["hadoop-env"][
+            "properties"] and "keyserver_port" in services["configurations"]["hadoop-env"]["properties"]:
+            keyserverHostsString = services["configurations"]["hadoop-env"]["properties"]["keyserver_host"]
+            keyserverPortString = services["configurations"]["hadoop-env"]["properties"]["keyserver_port"]
+
+        # Irrespective of what hadoop-env has, if Ranger-KMS is installed, we use its values.
+        rangerKMSServerHosts = self.getHostsWithComponent("RANGER_KMS", "RANGER_KMS_SERVER", services, hosts)
+        if rangerKMSServerHosts is not None and len(rangerKMSServerHosts) > 0:
+            rangerKMSServerHostsArray = []
+            for rangeKMSServerHost in rangerKMSServerHosts:
+                rangerKMSServerHostsArray.append(rangeKMSServerHost["Hosts"]["host_name"])
+            keyserverHostsString = ";".join(rangerKMSServerHostsArray)
+            if "kms-env" in services["configurations"] and "kms_port" in services["configurations"]["kms-env"][
+                "properties"]:
+                keyserverPortString = services["configurations"]["kms-env"]["properties"]["kms_port"]
+
+        if keyserverHostsString is not None and len(keyserverHostsString.strip()) > 0:
+            urlScheme = "http"
+            if "ranger-kms-site" in services["configurations"] and \
+                    "ranger.service.https.attrib.ssl.enabled" in services["configurations"]["ranger-kms-site"][
+                "properties"] and \
+                    services["configurations"]["ranger-kms-site"]["properties"][
+                        "ranger.service.https.attrib.ssl.enabled"].lower() == "true":
+                urlScheme = "https"
+
+            if keyserverPortString is None or len(keyserverPortString.strip()) < 1:
+                keyserverPortString = ":9292"
+            else:
+                keyserverPortString = ":" + keyserverPortString.strip()
+
+            kmsPath = "kms://" + urlScheme + "@" + keyserverHostsString.strip() + keyserverPortString + "/kms"
+            putCoreSiteProperty("hadoop.security.key.provider.path", kmsPath)
+            putHdfsSiteProperty("dfs.encryption.key.provider.uri", kmsPath)
+
+        putHdfsSitePropertyAttribute = self.putPropertyAttribute(configurations, "hdfs-site")
+        putCoreSitePropertyAttribute = self.putPropertyAttribute(configurations, "core-site")
+        if not "RANGER_KMS" in servicesList:
+            putCoreSitePropertyAttribute('hadoop.security.key.provider.path', 'delete', 'true')
+            putHdfsSitePropertyAttribute('dfs.encryption.key.provider.uri', 'delete', 'true')
+
+        if "ranger-env" in services["configurations"] and "ranger-hdfs-plugin-properties" in services[
+            "configurations"] and \
+                "ranger-hdfs-plugin-enabled" in services["configurations"]["ranger-env"]["properties"]:
+            putHdfsRangerPluginProperty = self.putProperty(configurations, "ranger-hdfs-plugin-properties", services)
+            rangerEnvHdfsPluginProperty = services["configurations"]["ranger-env"]["properties"][
+                "ranger-hdfs-plugin-enabled"]
+            putHdfsRangerPluginProperty("ranger-hdfs-plugin-enabled", rangerEnvHdfsPluginProperty)
+
+    def recommendConfigurationsFromHDP23(self, configurations, clusterData, services, hosts):
+        """
+        Recommend configurations for this service based on HDP 2.3.
+        """
+        putHdfsSiteProperty = self.putProperty(configurations, "hdfs-site", services)
+        putHdfsSitePropertyAttribute = self.putPropertyAttribute(configurations, "hdfs-site")
+
+        if ('ranger-hdfs-plugin-properties' in services['configurations']) and (
+                'ranger-hdfs-plugin-enabled' in services['configurations']['ranger-hdfs-plugin-properties'][
+            'properties']):
+            rangerPluginEnabled = ''
+            if 'ranger-hdfs-plugin-properties' in configurations and 'ranger-hdfs-plugin-enabled' in \
+                    configurations['ranger-hdfs-plugin-properties']['properties']:
+                rangerPluginEnabled = configurations['ranger-hdfs-plugin-properties']['properties'][
+                    'ranger-hdfs-plugin-enabled']
+            elif 'ranger-hdfs-plugin-properties' in services['configurations'] and 'ranger-hdfs-plugin-enabled' in \
+                    services['configurations']['ranger-hdfs-plugin-properties']['properties']:
+                rangerPluginEnabled = services['configurations']['ranger-hdfs-plugin-properties']['properties'][
+                    'ranger-hdfs-plugin-enabled']
+
+            if rangerPluginEnabled and (rangerPluginEnabled.lower() == 'Yes'.lower()):
+                putHdfsSiteProperty("dfs.namenode.inode.attributes.provider.class",
+                                    'org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer')
+            else:
+                putHdfsSitePropertyAttribute('dfs.namenode.inode.attributes.provider.class', 'delete', 'true')
+        else:
+            putHdfsSitePropertyAttribute('dfs.namenode.inode.attributes.provider.class', 'delete', 'true')
+
+    def recommendConfigurationsFromHDP26(self, configurations, clusterData, services, hosts):
+        """
+        Recommend configurations for this service based on HDP 2.6
+        """
+        if 'hadoop-env' in services['configurations'] and 'hdfs_user' in services['configurations']['hadoop-env'][
+            'properties']:
+            hdfs_user = services['configurations']['hadoop-env']['properties']['hdfs_user']
+        else:
+            hdfs_user = 'hadoop'
+
+        if 'ranger-hdfs-plugin-properties' in configurations and 'ranger-hdfs-plugin-enabled' in \
+                configurations['ranger-hdfs-plugin-properties']['properties']:
+            ranger_hdfs_plugin_enabled = (configurations['ranger-hdfs-plugin-properties']['properties'][
+                                              'ranger-hdfs-plugin-enabled'].lower() == 'Yes'.lower())
+        elif 'ranger-hdfs-plugin-properties' in services['configurations'] and 'ranger-hdfs-plugin-enabled' in \
+                services['configurations']['ranger-hdfs-plugin-properties']['properties']:
+            ranger_hdfs_plugin_enabled = (services['configurations']['ranger-hdfs-plugin-properties']['properties'][
+                                              'ranger-hdfs-plugin-enabled'].lower() == 'Yes'.lower())
+        else:
+            ranger_hdfs_plugin_enabled = False
+
+        if ranger_hdfs_plugin_enabled and 'ranger-hdfs-plugin-properties' in services[
+            'configurations'] and 'REPOSITORY_CONFIG_USERNAME' in \
+                services['configurations']['ranger-hdfs-plugin-properties']['properties']:
+            self.logger.info("Setting HDFS Repo user for Ranger.")
+            putRangerHDFSPluginProperty = self.putProperty(configurations, "ranger-hdfs-plugin-properties", services)
+            putRangerHDFSPluginProperty("REPOSITORY_CONFIG_USERNAME", hdfs_user)
+        else:
+            self.logger.info("Not setting HDFS Repo user for Ranger.")
+
+    def recommendHDFSConfigurationsFromHDP31(self, configurations, clusterData, services, hosts):
+        putHdfsSiteProperty = self.putProperty(configurations, "hdfs-site", services)
+
+        ranger_hdfs_plugin_enabled = False
+
+        if 'ranger-hdfs-plugin-properties' in configurations and 'ranger-hdfs-plugin-enabled' in \
+                configurations['ranger-hdfs-plugin-properties']['properties']:
+            ranger_hdfs_plugin_enabled = (configurations['ranger-hdfs-plugin-properties']['properties'][
+                                              'ranger-hdfs-plugin-enabled'].lower() == 'Yes'.lower())
+        elif 'ranger-hdfs-plugin-properties' in services['configurations'] and 'ranger-hdfs-plugin-enabled' in \
+                services['configurations']['ranger-hdfs-plugin-properties']['properties']:
+            ranger_hdfs_plugin_enabled = (services['configurations']['ranger-hdfs-plugin-properties']['properties'][
+                                              'ranger-hdfs-plugin-enabled'].lower() == 'Yes'.lower())
+
+        if ranger_hdfs_plugin_enabled:
+            putHdfsSiteProperty('dfs.permissions.ContentSummary.subAccess', 'true')
+        else:
+            putHdfsSiteProperty('dfs.permissions.ContentSummary.subAccess', 'false')
+
+    def recommendConfigurationsForSSO(self, configurations, clusterData, services, hosts):
+        ambari_configuration = self.get_ambari_configuration(services)
+        ambari_sso_details = ambari_configuration.get_ambari_sso_details() if ambari_configuration else None
+
+        if ambari_sso_details and ambari_sso_details.is_managing_services():
+            putHdfsSiteProperty = self.putProperty(configurations, "hdfs-site", services)
+
+            # If SSO should be enabled for this service
+            if ambari_sso_details.should_enable_sso('HDFS'):
+                if (self.is_kerberos_enabled(configurations, services)):
+                    putHdfsSiteProperty('hadoop.http.authentication.type',
+                                        "org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler")
+                    putHdfsSiteProperty('hadoop.http.authentication.authentication.provider.url',
+                                        ambari_sso_details.get_sso_provider_url())
+                    putHdfsSiteProperty('hadoop.http.authentication.public.key.pem',
+                                        ambari_sso_details.get_sso_provider_certificate(False, True))
+                else:
+                    # Since Kerberos is not enabled, we can not enable SSO
+                    self.logger.warn(
+                        "Enabling SSO integration for HDFS requires Kerberos, Since Kerberos is not enabled, SSO integration is not being recommended.")
+                    putHdfsSiteProperty('hadoop.http.authentication.type', "simple")
+                    pass
+
+            # If SSO should be disabled for this service
+            elif ambari_sso_details.should_disable_sso('HDFS'):
+                if (self.is_kerberos_enabled(configurations, services)):
+                    putHdfsSiteProperty('hadoop.http.authentication.type', "kerberos")
+                else:
+                    putHdfsSiteProperty('hadoop.http.authentication.type', "simple")
+
+    def is_kerberos_enabled(self, configurations, services):
+        """
+        Tests if HDFS has Kerberos enabled by first checking the recommended changes and then the
+        existing settings.
+        :type configurations dict
+        :type services dict
+        :rtype bool
+        """
+        return self._is_kerberos_enabled(configurations) or \
+            (services and 'configurations' in services and self._is_kerberos_enabled(services['configurations']))
+
+    def _is_kerberos_enabled(self, config):
+        """
+        Detects if HDFS has Kerberos enabled given a dictionary of configurations.
+        :type config dict
+        :rtype bool
+        """
+        return config and \
+            (
+                    (
+                            "hdfs-site" in config and
+                            'hadoop.security.authentication' in config['hdfs-site']["properties"] and
+                            config['hdfs-site']["properties"]['hadoop.security.authentication'] == 'kerberos'
+                    ) or (
+                            "core-site" in config and
+                            'hadoop.security.authentication' in config['core-site']["properties"] and
+                            config['core-site']["properties"]['hadoop.security.authentication'] == 'kerberos'
+                    )
+            )
+
+
+class HDFSValidator(service_advisor.ServiceAdvisor):
+    """
+    HDFS Validator checks the correctness of properties whenever the service is first added or the user attempts to
+    change configs via the UI.
+    """
+
+    def __init__(self, *args, **kwargs):
+        self.as_super = super(HDFSValidator, self)
+        self.as_super.__init__(*args, **kwargs)
+
+        self.validators = [("hdfs-site", self.validateHDFSConfigurationsFromHDP206),
+                           ("hadoop-env", self.validateHadoopEnvConfigurationsFromHDP206),
+                           ("core-site", self.validateHDFSCoreSiteFromHDP206),
+                           ("hdfs-site", self.validateHDFSConfigurationsFromHDP22),
+                           ("hadoop-env", self.validateHadoopEnvConfigurationsFromHDP22),
+                           ("ranger-hdfs-plugin-properties", self.validateHDFSRangerPluginConfigurationsFromHDP22),
+                           ("hdfs-site", self.validateRangerAuthorizerFromHDP23)]
+
+        # **********************************************************
+        # Example of how to add a function that validates a certain config type.
+        # If the same config type has multiple functions, can keep adding tuples to self.validators
+        # self.validators.append(("hadoop-env", self.sampleValidator))
+
+    def sampleValidator(self, properties, recommendedDefaults, configurations, services, hosts):
+        """
+        Example of a validator function other other Service Advisors to emulate.
+        :return: A list of configuration validation problems.
+        """
+        validationItems = []
+
+        '''
+        Item is a simple dictionary.
+        Two functions can be used to construct it depending on the log level: WARN|ERROR
+        E.g.,
+        self.getErrorItem(message) or self.getWarnItem(message)
+    
+        item = {"level": "ERROR|WARN", "message": "value"}
+        '''
+        validationItems.append({"config-name": "my_config_property_name",
+                                "item": self.getErrorItem("My custom message in method %s" % inspect.stack()[0][3])})
+        return self.toConfigurationValidationProblems(validationItems, "hadoop-env")
+
+    def validateHDFSConfigurationsFromHDP206(self, properties, recommendedDefaults, configurations, services, hosts):
+        """
+        This was copied from HDP 2.0.6; validate hdfs-site
+        :return: A list of configuration validation problems.
+        """
+        clusterEnv = self.getSiteProperties(configurations, "cluster-env")
+        validationItems = [{"config-name": 'dfs.datanode.du.reserved',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'dfs.datanode.du.reserved')},
+                           {"config-name": 'dfs.datanode.data.dir',
+                            "item": self.validatorOneDataDirPerPartition(properties, 'dfs.datanode.data.dir', services,
+                                                                         hosts, clusterEnv)}]
+        return self.toConfigurationValidationProblems(validationItems, "hdfs-site")
+
+    def validatorOneDataDirPerPartition(self, properties, propertyName, services, hosts, clusterEnv):
+        if not propertyName in properties:
+            return self.getErrorItem("Value should be set")
+        dirs = properties[propertyName]
+
+        if not (clusterEnv and "one_dir_per_partition" in clusterEnv and clusterEnv[
+            "one_dir_per_partition"].lower() == "true"):
+            return None
+
+        dataNodeHosts = self.getDataNodeHosts(services, hosts)
+
+        warnings = set()
+        for host in dataNodeHosts:
+            hostName = host["Hosts"]["host_name"]
+
+            mountPoints = []
+            for diskInfo in host["Hosts"]["disk_info"]:
+                mountPoints.append(diskInfo["mountpoint"])
+
+            if get_mounts_with_multiple_data_dirs(mountPoints, dirs):
+                # A detailed message can be too long on large clusters:
+                # warnings.append("Host: " + hostName + "; Mount: " + mountPoint + "; Data directories: " + ", ".join(dirList))
+                warnings.add(hostName)
+                break
+
+        if len(warnings) > 0:
+            return self.getWarnItem(
+                "cluster-env/one_dir_per_partition is enabled but there are multiple data directories on the same mount. Affected hosts: {0}".format(
+                    ", ".join(sorted(warnings))))
+
+        return None
+
+    def validateHadoopEnvConfigurationsFromHDP206(self, properties, recommendedDefaults, configurations, services,
+                                                  hosts):
+        """
+        This was copied from HDP 2.0.6; validate hadoop-env
+        :return: A list of configuration validation problems.
+        """
+        validationItems = [{"config-name": 'namenode_heapsize',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'namenode_heapsize')},
+                           {"config-name": 'namenode_opt_newsize',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'namenode_opt_newsize')},
+                           {"config-name": 'namenode_opt_maxnewsize',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'namenode_opt_maxnewsize')}]
+        return self.toConfigurationValidationProblems(validationItems, "hadoop-env")
+
+    def validateHDFSCoreSiteFromHDP206(self, properties, recommendedDefaults, configurations, services, hosts):
+        """
+        This was copied from HDP 2.0.6; validate core-site
+        :return: A list of configuration validation problems.
+        """
+        validationItems = []
+        validationItems.extend(self.getHadoopProxyUsersValidationItems(properties, services, hosts, configurations))
+        validationItems.extend(self.getAmbariProxyUsersForHDFSValidationItems(properties, services))
+        validationItems.extend(self.getLZOSupportValidationItems(properties, services))
+        return self.toConfigurationValidationProblems(validationItems, "core-site")
+
+    def getLZOSupportValidationItems(self, properties, services):
+        '''
+        Checks GPL license is accepted when GPL software is used.
+        :param properties: dict of properties' name and value pairs
+        :param services: list of services
+        :return: NOT_APPLICABLE messages in case GPL license is not accepted
+        '''
+        services_list = self.get_services_list(services)
+
+        validations = []
+        if "HDFS" in services_list:
+            lzo_allowed = services["gpl-license-accepted"]
+
+            self.validatePropertyToLZOCodec("io.compression.codecs", properties, lzo_allowed, validations)
+            self.validatePropertyToLZOCodec("io.compression.codec.lzo.class", properties, lzo_allowed, validations)
+        return validations
+
+    def validatePropertyToLZOCodec(self, property_name, properties, lzo_allowed, validations):
+        '''
+        Checks specified property contains LZO codec class and requires GPL license acceptance.
+        :param property_name: property name
+        :param properties: dict of properties' name and value pairs
+        :param lzo_allowed: is gpl license accepted
+        :param validations: list with validation failures
+        '''
+        lzo_codec_class = "com.hadoop.compression.lzo.LzoCodec"
+        if property_name in properties:
+            property_value = properties.get(property_name)
+            if not lzo_allowed and lzo_codec_class in property_value:
+                validations.append({"config-name": property_name, "item": self.getNotApplicableItem(
+                    "Your Ambari Server has not been configured to download LZO and install it. "
+                    "LZO is GPL software and requires you to explicitly enable Ambari to install and download LZO. "
+                    "Please refer to the documentation to configure Ambari before proceeding.")})
+
+    def getAmbariProxyUsersForHDFSValidationItems(self, properties, services):
+        validationItems = []
+        servicesList = self.get_services_list(services)
+
+        if "HDFS" in servicesList:
+            ambari_user = self.getAmbariUser(services)
+            props = (
+                "hadoop.proxyuser.{0}.hosts".format(ambari_user),
+                "hadoop.proxyuser.{0}.groups".format(ambari_user)
+            )
+            for prop in props:
+                validationItems.append({"config-name": prop, "item": self.validatorNotEmpty(properties, prop)})
+
+        return validationItems
+
+    def validateHDFSConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services, hosts):
+        """
+        This was copied from HDP 2.2; validate hdfs-site
+        :return: A list of configuration validation problems.
+        """
+        # We can not access property hadoop.security.authentication from the
+        # other config (core-site). That's why we are using another heuristic here
+        hdfs_site = properties
+        core_site = self.getSiteProperties(configurations, "core-site")
+
+        dfs_encrypt_data_transfer = 'dfs.encrypt.data.transfer'  # Hadoop Wire encryption
+        wire_encryption_enabled = False
+        try:
+            wire_encryption_enabled = hdfs_site[dfs_encrypt_data_transfer] == "true"
+        except KeyError:
+            pass
+
+        HTTP_ONLY = 'HTTP_ONLY'
+        HTTPS_ONLY = 'HTTPS_ONLY'
+        HTTP_AND_HTTPS = 'HTTP_AND_HTTPS'
+        VALID_HTTP_POLICY_VALUES = [HTTP_ONLY, HTTPS_ONLY, HTTP_AND_HTTPS]
+
+        TRANSFER_PROTECTION_AUTHENTICATION = 'authentication'
+        TRANSFER_PROTECTION_INTEGRITY = 'integrity'
+        TRANSFER_PROTECTION_PRIVACY = 'privacy'
+        TRANSFER_PROTECTION_AUTHENTICATION_AND_PRIVACY = 'authentication,privacy'
+        VALID_TRANSFER_PROTECTION_VALUES = [TRANSFER_PROTECTION_AUTHENTICATION, TRANSFER_PROTECTION_INTEGRITY,
+                                            TRANSFER_PROTECTION_PRIVACY, TRANSFER_PROTECTION_AUTHENTICATION_AND_PRIVACY]
+
+        validationItems = []
+        address_properties = [
+            # "dfs.datanode.address",
+            # "dfs.datanode.http.address",
+            # "dfs.datanode.https.address",
+            # "dfs.datanode.ipc.address",
+            # "dfs.journalnode.http-address",
+            # "dfs.journalnode.https-address",
+            # "dfs.namenode.rpc-address",
+            # "dfs.namenode.secondary.http-address",
+            "dfs.namenode.http-address",
+            "dfs.namenode.https-address",
+        ]
+        # Validating *address properties for correct values
+
+        for address_property in address_properties:
+            if address_property in hdfs_site:
+                value = hdfs_site[address_property]
+                if not HDFSValidator.is_valid_host_port_authority(value):
+                    validationItems.append({"config-name": address_property, "item":
+                        self.getErrorItem(
+                            address_property + " does not contain a valid host:port authority: " + value)})
+
+        # Adding Ranger Plugin logic here
+        ranger_plugin_properties = self.getSiteProperties(configurations, "ranger-hdfs-plugin-properties")
+        ranger_plugin_enabled = ranger_plugin_properties[
+            'ranger-hdfs-plugin-enabled'] if ranger_plugin_properties else 'No'
+        servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+        if ("RANGER" in servicesList) and (ranger_plugin_enabled.lower() == 'Yes'.lower()):
+            if 'dfs.permissions.enabled' in hdfs_site and \
+                    hdfs_site['dfs.permissions.enabled'] != 'true':
+                validationItems.append({"config-name": 'dfs.permissions.enabled',
+                                        "item": self.getWarnItem(
+                                            "dfs.permissions.enabled needs to be set to true if Ranger HDFS Plugin is enabled.")})
+
+        if (not wire_encryption_enabled and  # If wire encryption is enabled at Hadoop, it disables all our checks
+                'hadoop.security.authentication' in core_site and
+                core_site['hadoop.security.authentication'] == 'kerberos' and
+                'hadoop.security.authorization' in core_site and
+                core_site['hadoop.security.authorization'] == 'true'):
+            # security is enabled
+
+            dfs_http_policy = 'dfs.http.policy'
+            dfs_datanode_address = 'dfs.datanode.address'
+            datanode_http_address = 'dfs.datanode.http.address'
+            datanode_https_address = 'dfs.datanode.https.address'
+            data_transfer_protection = 'dfs.data.transfer.protection'
+
+            try:  # Params may be absent
+                privileged_dfs_dn_port = HDFSValidator.isSecurePort(
+                    HDFSValidator.getPort(hdfs_site[dfs_datanode_address]))
+            except KeyError:
+                privileged_dfs_dn_port = False
+            try:
+                privileged_dfs_http_port = HDFSValidator.isSecurePort(
+                    HDFSValidator.getPort(hdfs_site[datanode_http_address]))
+            except KeyError:
+                privileged_dfs_http_port = False
+            try:
+                privileged_dfs_https_port = HDFSValidator.isSecurePort(
+                    HDFSValidator.getPort(hdfs_site[datanode_https_address]))
+            except KeyError:
+                privileged_dfs_https_port = False
+            try:
+                dfs_http_policy_value = hdfs_site[dfs_http_policy]
+            except KeyError:
+                dfs_http_policy_value = HTTP_ONLY  # Default
+            try:
+                data_transfer_protection_value = hdfs_site[data_transfer_protection]
+            except KeyError:
+                data_transfer_protection_value = None
+
+            if dfs_http_policy_value not in VALID_HTTP_POLICY_VALUES:
+                validationItems.append({"config-name": dfs_http_policy,
+                                        "item": self.getWarnItem(
+                                            "Invalid property value: {0}. Valid values are {1}".format(
+                                                dfs_http_policy_value, VALID_HTTP_POLICY_VALUES))})
+
+            # determine whether we use secure ports
+            address_properties_with_warnings = []
+            if dfs_http_policy_value == HTTPS_ONLY:
+                if not privileged_dfs_dn_port and data_transfer_protection_value is None:
+                    important_properties = [dfs_datanode_address, datanode_https_address]
+                    message = "You have set up datanode to use non-secure rpc port. " \
+                              "If you want to run Datanode under non-root user in a secure cluster, " \
+                              "you should set {0} ('{1}' is a good default value).".format(
+                        data_transfer_protection, VALID_TRANSFER_PROTECTION_VALUES[0])
+                    address_properties_with_warnings.extend(important_properties)
+            else:  # dfs_http_policy_value == HTTP_AND_HTTPS or HTTP_ONLY
+                # We don't enforce datanode_https_address to use privileged ports here
+                any_nonprivileged_ports_are_in_use = not privileged_dfs_dn_port or not privileged_dfs_http_port
+                if any_nonprivileged_ports_are_in_use:
+                    important_properties = [dfs_datanode_address, datanode_http_address]
+                    message = "You have set up datanode to use some non-secure ports. " \
+                              "In a secure cluster:" \
+                              "1. non-secure rpc port is allowed only if sasl is enabled by setting {0}." \
+                              "2. non-secure http port is allowed only if {1} is set to {2}".format(
+                        data_transfer_protection, dfs_http_policy, HTTPS_ONLY)
+                    address_properties_with_warnings.extend(important_properties)
+
+            # Generate port-related warnings if any
+            for prop in address_properties_with_warnings:
+                validationItems.append({"config-name": prop,
+                                        "item": self.getWarnItem(message)})
+
+            # Check if it is appropriate to use dfs.data.transfer.protection
+            if data_transfer_protection_value is not None:
+                if data_transfer_protection_value not in VALID_TRANSFER_PROTECTION_VALUES:
+                    validationItems.append({"config-name": data_transfer_protection, "item": self.getWarnItem(
+                        "Invalid property value: {0}. Valid values are {1}.".format(data_transfer_protection_value,
+                                                                                    VALID_TRANSFER_PROTECTION_VALUES)
+                    )})
+        return self.toConfigurationValidationProblems(validationItems, "hdfs-site")
+
+    def validateHadoopEnvConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services,
+                                                 hosts):
+        """
+        This was copied from HDP 2.2; validate hadoop-env
+        :return: A list of configuration validation problems.
+        """
+        validationItems = [{"config-name": 'namenode_heapsize',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'namenode_heapsize')},
+                           {"config-name": 'namenode_opt_newsize',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'namenode_opt_newsize')},
+                           {"config-name": 'namenode_opt_maxnewsize',
+                            "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults,
+                                                                       'namenode_opt_maxnewsize')}]
+        return self.toConfigurationValidationProblems(validationItems, "hadoop-env")
+
+    def validateHDFSRangerPluginConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services,
+                                                        hosts):
+        """
+        This was copied from HDP 2.2; validate ranger-hdfs-plugin-properties
+        :return: A list of configuration validation problems.
+        """
+        validationItems = []
+        ranger_plugin_properties = self.getSiteProperties(configurations, "ranger-hdfs-plugin-properties")
+        ranger_plugin_enabled = ranger_plugin_properties[
+            'ranger-hdfs-plugin-enabled'] if ranger_plugin_properties else 'No'
+        if ranger_plugin_enabled.lower() == 'yes':
+            # ranger-hdfs-plugin must be enabled in ranger-env
+            ranger_env = self.getServicesSiteProperties(services, 'ranger-env')
+            if not ranger_env or not 'ranger-hdfs-plugin-enabled' in ranger_env or ranger_env[
+                'ranger-hdfs-plugin-enabled'].lower() != 'yes':
+                validationItems.append({"config-name": 'ranger-hdfs-plugin-enabled',
+                                        "item": self.getWarnItem(
+                                            "ranger-hdfs-plugin-properties/ranger-hdfs-plugin-enabled must correspond ranger-env/ranger-hdfs-plugin-enabled")})
+        return self.toConfigurationValidationProblems(validationItems, "ranger-hdfs-plugin-properties")
+
+    def validateRangerAuthorizerFromHDP23(self, properties, recommendedDefaults, configurations, services, hosts):
+        """
+        This was copied from HDP 2.3
+        If Ranger service is present and the ranger plugin is enabled, check that the provider class is correctly set.
+        :return: A list of configuration validation problems.
+        """
+        self.logger.info(
+            "Class: %s, Method: %s. Checking if Ranger service is present and if the provider class is using the Ranger Authorizer." %
+            (self.__class__.__name__, inspect.stack()[0][3]))
+        # We can not access property hadoop.security.authentication from the
+        # other config (core-site). That's why we are using another heuristics here
+        hdfs_site = properties
+        validationItems = []  # Adding Ranger Plugin logic here
+        ranger_plugin_properties = self.getSiteProperties(configurations, "ranger-hdfs-plugin-properties")
+        ranger_plugin_enabled = ranger_plugin_properties[
+            'ranger-hdfs-plugin-enabled'] if ranger_plugin_properties else 'No'
+        servicesList = self.getServiceNames(services)
+        if ("RANGER" in servicesList) and (ranger_plugin_enabled.lower() == 'yes'):
+
+            try:
+                if hdfs_site[
+                    'dfs.namenode.inode.attributes.provider.class'].lower() != 'org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer'.lower():
+                    raise ValueError()
+            except (KeyError, ValueError), e:
+                message = "dfs.namenode.inode.attributes.provider.class needs to be set to 'org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer' if Ranger HDFS Plugin is enabled."
+                validationItems.append({"config-name": 'dfs.namenode.inode.attributes.provider.class',
+                                        "item": self.getWarnItem(message)})
+
+        return self.toConfigurationValidationProblems(validationItems, "hdfs-site")
+
+    def getDataNodeHosts(self, services, hosts):
+        """
+        Returns the list of Data Node hosts. If none, return an empty list.
+        """
+        if len(hosts["items"]) > 0:
+            dataNodeHosts = self.getHostsWithComponent("HDFS", "DATANODE", services, hosts)
+            if dataNodeHosts is not None:
+                return dataNodeHosts
+        return []
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_ranger_config_for_solr.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_ranger_config_for_solr.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_ranger_config_for_solr.py
new file mode 100644
--- /dev/null	(date 1723880630240)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_ranger_config_for_solr.py	(date 1723880630240)
@@ -0,0 +1,39 @@
+#!/usr/bin/env python
+# coding=utf-8
+
+import os
+import json
+import urllib2
+
+from resource_management import Execute, Logger
+from resource_management.libraries.functions.format import format
+
+
+def collection_exists(collection_name):
+    import params
+    url = format("http://127.0.0.1:{solr_port}/solr/admin/collections?action=LIST")
+    handler = urllib2.urlopen(url)
+    if handler.getcode() == 200:
+        res = json.loads(handler.read())
+        collections = res.get('collections', [])
+        return collection_name in collections
+    else:
+        Logger.info(format("execute failed ,HTTP code: {handler}"))
+        return False
+
+
+# create
+def create_collection(collection_name):
+    import params
+    cmd = format('{solr_bindir}/solr'
+                 ' create -c {collection_name} -d {ranger_audit_conf} -s 3 -rf 3 -force')
+    Execute(cmd)
+
+
+def setup_ranger_collection(collection_name='ranger_audit'):
+    if collection_exists(collection_name):
+        Logger.debug(format("{collection_name} already setup"))
+    else:
+        create_collection(collection_name)
+
+
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/elevate.xml.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/elevate.xml.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/elevate.xml.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/elevate.xml.j2	(date 1719626239000)
@@ -0,0 +1,38 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- If this file is found in the config directory, it will only be
+     loaded once at startup.  If it is found in Solr's data
+     directory, it will be re-loaded every commit.
+
+   See http://wiki.apache.org/solr/QueryElevationComponent for more info
+
+-->
+<elevate>
+ <query text="foo bar">
+  <doc id="1" />
+  <doc id="2" />
+  <doc id="3" />
+ </query>
+ 
+ <query text="ipod">
+   <doc id="MA147LL/A" />  <!-- put the actual ipod at the top -->
+   <doc id="IW-02" exclude="true" /> <!-- exclude this cable -->
+ </query>
+ 
+</elevate>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/managed-schema.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/managed-schema.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/managed-schema.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/managed-schema.j2	(date 1719626239000)
@@ -0,0 +1,95 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<schema name="ranger-audit-schema" version="1.6">
+  <uniqueKey>id</uniqueKey>
+  <fieldType name="binary" class="solr.BinaryField"/>
+  <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true"/>
+  <fieldType name="booleans" class="solr.BoolField" multiValued="true" sortMissingLast="true"/>
+  <fieldType name="date" class="solr.TrieDateField" docValues="true" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="double" class="solr.TrieDoubleField" docValues="true" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="float" class="solr.TrieFloatField" docValues="true" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="ignored" class="solr.StrField" multiValued="true" indexed="false" stored="false"/>
+  <fieldType name="int" class="solr.TrieIntField" docValues="true" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="key_lower_case" class="solr.TextField" sortMissingLast="true" omitNorms="true">
+    <analyzer>
+      <tokenizer class="solr.KeywordTokenizerFactory"/>
+      <filter class="solr.LowerCaseFilterFactory"/>
+      <filter class="solr.LengthFilterFactory" min="0" max="2500"/>
+    </analyzer>
+  </fieldType>
+  <fieldType name="long" class="solr.TrieLongField" docValues="true" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="random" class="solr.RandomSortField" indexed="true"/>
+  <fieldType name="string" class="solr.StrField" sortMissingLast="true"/>
+  <fieldType name="tdate" class="solr.TrieDateField" docValues="true" precisionStep="6" positionIncrementGap="0"/>
+  <fieldType name="tdates" class="solr.TrieDateField" docValues="true" precisionStep="6" multiValued="true" positionIncrementGap="0"/>
+  <fieldType name="tdouble" class="solr.TrieDoubleField" docValues="true" precisionStep="8" positionIncrementGap="0"/>
+  <fieldType name="tdoubles" class="solr.TrieDoubleField" docValues="true" precisionStep="8" multiValued="true" positionIncrementGap="0"/>
+  <fieldType name="text_std_token_lower_case" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.StandardTokenizerFactory"/>
+      <filter class="solr.LowerCaseFilterFactory"/>
+    </analyzer>
+  </fieldType>
+  <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.WhitespaceTokenizerFactory"/>
+    </analyzer>
+  </fieldType>
+  <fieldType name="tfloat" class="solr.TrieFloatField" docValues="true" precisionStep="8" positionIncrementGap="0"/>
+  <fieldType name="tfloats" class="solr.TrieFloatField" docValues="true" precisionStep="8" multiValued="true" positionIncrementGap="0"/>
+  <fieldType name="tint" class="solr.TrieIntField" docValues="true" precisionStep="8" positionIncrementGap="0"/>
+  <fieldType name="tints" class="solr.TrieIntField" docValues="true" precisionStep="8" multiValued="true" positionIncrementGap="0"/>
+  <fieldType name="tlong" class="solr.TrieLongField" docValues="true" precisionStep="8" positionIncrementGap="0"/>
+  <fieldType name="tlongs" class="solr.TrieLongField" docValues="true" precisionStep="8" multiValued="true" positionIncrementGap="0"/>
+  <field name="_expire_at_" type="tdate" multiValued="false" stored="true" docValues="true"/>
+  <field name="_ttl_" type="string" multiValued="false" indexed="true" stored="true"/>
+  <field name="_version_" type="long" indexed="false" stored="true"/>
+  <field name="access" type="key_lower_case" multiValued="false"/>
+  <field name="action" type="key_lower_case" multiValued="false"/>
+  <field name="agent" type="key_lower_case" multiValued="false"/>
+  <field name="agentHost" type="key_lower_case" multiValued="false"/>
+  <field name="cliIP" type="key_lower_case" multiValued="false"/>
+  <field name="cliType" type="key_lower_case" multiValued="false"/>
+  <field name="cluster" type="key_lower_case" multiValued="false"/>
+  <field name="reqContext" type="key_lower_case" multiValued="true"/>
+  <field name="enforcer" type="key_lower_case" multiValued="false"/>
+  <field name="event_count" type="tlong" multiValued="false" docValues="true" default="1"/>
+  <field name="event_dur_ms" type="tlong" multiValued="false" docValues="true"/>
+  <field name="evtTime" type="tdate" docValues="true"/>
+  <field name="id" type="string" multiValued="false" indexed="true" required="true" stored="true"/>
+  <field name="logType" type="key_lower_case" multiValued="false"/>
+  <field name="policy" type="tlong" docValues="true"/>
+  <field name="proxyUsers" type="key_lower_case" multiValued="true"/>
+  <field name="reason" type="text_std_token_lower_case" multiValued="false" omitNorms="false"/>
+  <field name="repo" type="key_lower_case" multiValued="false"/>
+  <field name="repoType" type="tint" multiValued="false" docValues="true"/>
+  <field name="req_caller_id" type="key_lower_case" multiValued="false"/>
+  <field name="req_self_id" type="key_lower_case" multiValued="false"/>
+  <field name="reqData" type="text_std_token_lower_case" multiValued="false"/>
+  <field name="reqUser" type="key_lower_case" multiValued="false"/>
+  <field name="resType" type="key_lower_case" multiValued="false"/>
+  <field name="resource" type="key_lower_case" multiValued="false"/>
+  <field name="result" type="tint" multiValued="false"/>
+  <field name="seq_num" type="tlong" multiValued="false" docValues="true" default="0"/>
+  <field name="sess" type="key_lower_case" multiValued="false"/>
+  <field name="tags" type="key_lower_case" multiValued="true"/>
+  <field name="tags_str" type="text_std_token_lower_case" multiValued="false"/>
+  <field name="text" type="text_std_token_lower_case" multiValued="true" indexed="true" stored="false"/>
+  <field name="zoneName" type="key_lower_case" multiValued="false"/>
+  <field name="policyVersion" type="tlong" multiValued="false"/>
+</schema>
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/solrconfig.xml.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/solrconfig.xml.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/solrconfig.xml.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/templates/ranger_audit/solrconfig.xml.j2	(date 1719626239000)
@@ -0,0 +1,1155 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+     http://www.apache.org/licenses/LICENSE-2.0
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--
+     For more details about configurations options that may appear in
+     this file, see http://wiki.apache.org/solr/SolrConfigXml.
+-->
+<config>
+  <!-- In all configuration below, a prefix of "solr." for class names
+       is an alias that causes solr to search appropriate packages,
+       including org.apache.solr.(search|update|request|core|analysis)
+       You may also specify a fully qualified Java classname if you
+       have your own custom plugins.
+    -->
+
+  <!-- Controls what version of Lucene various components of Solr
+       adhere to.  Generally, you want to use the latest version to
+       get all bug fixes and improvements. It is highly recommended
+       that you fully re-index after changing this setting as it can
+       affect both how text is indexed and queried.
+  -->
+  <luceneMatchVersion>8.4.1</luceneMatchVersion>
+
+  <!-- <lib/> directives can be used to instruct Solr to load any Jars
+       identified and use them to resolve any "plugins" specified in
+       your solrconfig.xml or schema.xml (ie: Analyzers, Request
+       Handlers, etc...).
+       All directories and paths are resolved relative to the
+       instanceDir.
+       Please note that <lib/> directives are processed in the order
+       that they appear in your solrconfig.xml file, and are "stacked"
+       on top of each other when building a ClassLoader - so if you have
+       plugin jars with dependencies on other jars, the "lower level"
+       dependency jars should be loaded first.
+       If a "./lib" directory exists in your instanceDir, all files
+       found in it are included as if you had used the following
+       syntax...
+              <lib dir="./lib" />
+    -->
+
+  <!-- A 'dir' option by itself adds any files found in the directory
+       to the classpath, this is useful for including all jars in a
+       directory.
+       When a 'regex' is specified in addition to a 'dir', only the
+       files in that directory which completely match the regex
+       (anchored on both ends) will be included.
+       If a 'dir' option (with or without a regex) is used and nothing
+       is found that matches, a warning will be logged.
+       The example below can be used to load a solr-contrib along
+       with their external dependencies.
+    -->
+    <!-- <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-ltr-\d.*\.jar" /> -->
+
+  <!-- an exact 'path' can be used instead of a 'dir' to specify a
+       specific jar file.  This will cause a serious error to be logged
+       if it can't be loaded.
+    -->
+  <!--
+     <lib path="../a-jar-that-does-not-exist.jar" />
+  -->
+  
+  <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-dataimporthandler-.*\.jar" />
+
+  <!-- Data Directory
+       Used to specify an alternate directory to hold all index data
+       other than the default ./data under the Solr home.  If
+       replication is in use, this should match the replication
+       configuration.
+    -->
+  <dataDir>${solr.data.dir:}</dataDir>
+
+
+  <!-- The DirectoryFactory to use for indexes.
+       solr.StandardDirectoryFactory is filesystem
+       based and tries to pick the best implementation for the current
+       JVM and platform.  solr.NRTCachingDirectoryFactory, the default,
+       wraps solr.StandardDirectoryFactory and caches small files in memory
+       for better NRT performance.
+       One can force a particular implementation via solr.MMapDirectoryFactory,
+       solr.NIOFSDirectoryFactory, or solr.SimpleFSDirectoryFactory.
+       solr.RAMDirectoryFactory is memory based and not persistent.
+    -->
+
+  <directoryFactory name="DirectoryFactory" 
+                    class="${solr.directoryFactory:solr.NRTCachingDirectoryFactory}">
+    
+         
+    <!-- These will be used if you are using the solr.HdfsDirectoryFactory,
+         otherwise they will be ignored. If you don't plan on using hdfs,
+         you can safely remove this section. -->      
+    <!-- The root directory that collection data should be written to. -->     
+    <str name="solr.hdfs.home">${solr.hdfs.home:}</str>
+    <!-- The hadoop configuration files to use for the hdfs client. -->    
+    <str name="solr.hdfs.confdir">${solr.hdfs.confdir:}</str>
+    <!-- Enable/Disable the hdfs cache. -->    
+    <str name="solr.hdfs.blockcache.enabled">${solr.hdfs.blockcache.enabled:true}</str>
+    <!-- Enable/Disable using one global cache for all SolrCores. 
+         The settings used will be from the first HdfsDirectoryFactory created. -->    
+    <str name="solr.hdfs.blockcache.global">${solr.hdfs.blockcache.global:true}</str>
+    
+  </directoryFactory> 
+
+  <!-- The CodecFactory for defining the format of the inverted index.
+       The default implementation is SchemaCodecFactory, which is the official Lucene
+       index format, but hooks into the schema to provide per-field customization of
+       the postings lists and per-document values in the fieldType element
+       (postingsFormat/docValuesFormat). Note that most of the alternative implementations
+       are experimental, so if you choose to customize the index format, it's a good
+       idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
+       before upgrading to a newer version to avoid unnecessary reindexing.
+       A "compressionMode" string element can be added to <codecFactory> to choose
+       between the existing compression modes in the default codec: "BEST_SPEED" (default)
+       or "BEST_COMPRESSION".
+  -->
+  <codecFactory class="solr.SchemaCodecFactory"/>
+
+  <!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+       Index Config - These settings control low-level behavior of indexing
+       Most example settings here show the default value, but are commented
+       out, to more easily see where customizations have been made.
+       Note: This replaces <indexDefaults> and <mainIndex> from older versions
+       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+  <indexConfig>
+    <!-- maxFieldLength was removed in 4.0. To get similar behavior, include a
+         LimitTokenCountFilterFactory in your fieldType definition. E.g.
+     <filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="10000"/>
+    -->
+    <!-- Maximum time to wait for a write lock (ms) for an IndexWriter. Default: 1000 -->
+    <!-- <writeLockTimeout>1000</writeLockTimeout>  -->
+
+    <!-- Expert: Enabling compound file will use less files for the index,
+         using fewer file descriptors on the expense of performance decrease.
+         Default in Lucene is "true". Default in Solr is "false" (since 3.6) -->
+    <!-- <useCompoundFile>false</useCompoundFile> -->
+
+    <!-- ramBufferSizeMB sets the amount of RAM that may be used by Lucene
+         indexing for buffering added documents and deletions before they are
+         flushed to the Directory.
+         maxBufferedDocs sets a limit on the number of documents buffered
+         before flushing.
+         If both ramBufferSizeMB and maxBufferedDocs is set, then
+         Lucene will flush based on whichever limit is hit first.  -->
+    <ramBufferSizeMB>128</ramBufferSizeMB>
+    <!-- <maxBufferedDocs>1000</maxBufferedDocs> -->
+
+    <!-- Expert: ramPerThreadHardLimitMB sets the maximum amount of RAM that can be consumed
+         per thread before they are flushed. When limit is exceeded, this triggers a forced
+         flush even if ramBufferSizeMB has not been exceeded.
+         This is a safety limit to prevent Lucene's DocumentsWriterPerThread from address space
+         exhaustion due to its internal 32 bit signed integer based memory addressing.
+         The specified value should be greater than 0 and less than 2048MB. When not specified,
+         Solr uses Lucene's default value 1945. -->
+    <!-- <ramPerThreadHardLimitMB>1945</ramPerThreadHardLimitMB> -->
+
+    <!-- Expert: Merge Policy
+         The Merge Policy in Lucene controls how merging of segments is done.
+         The default since Solr/Lucene 3.3 is TieredMergePolicy.
+         The default since Lucene 2.3 was the LogByteSizeMergePolicy,
+         Even older versions of Lucene used LogDocMergePolicy.
+      -->
+    <!--
+        <mergePolicyFactory class="org.apache.solr.index.TieredMergePolicyFactory">
+          <int name="maxMergeAtOnce">10</int>
+          <int name="segmentsPerTier">10</int>
+          <double name="noCFSRatio">0.1</double>
+        </mergePolicyFactory>
+      -->
+
+    <!-- Expert: Merge Scheduler
+         The Merge Scheduler in Lucene controls how merges are
+         performed.  The ConcurrentMergeScheduler (Lucene 2.3 default)
+         can perform merges in the background using separate threads.
+         The SerialMergeScheduler (Lucene 2.2 default) does not.
+     -->
+    <!--
+       <mergeScheduler class="org.apache.lucene.index.ConcurrentMergeScheduler"/>
+       -->
+
+    <!-- LockFactory
+         This option specifies which Lucene LockFactory implementation
+         to use.
+         single = SingleInstanceLockFactory - suggested for a
+                  read-only index or when there is no possibility of
+                  another process trying to modify the index.
+         native = NativeFSLockFactory - uses OS native file locking.
+                  Do not use when multiple solr webapps in the same
+                  JVM are attempting to share a single index.
+         simple = SimpleFSLockFactory  - uses a plain file for locking
+         Defaults: 'native' is default for Solr3.6 and later, otherwise
+                   'simple' is the default
+         More details on the nuances of each LockFactory...
+         http://wiki.apache.org/lucene-java/AvailableLockFactories
+    -->
+    <lockType>${solr.lock.type:native}</lockType>
+
+    <!-- Commit Deletion Policy
+         Custom deletion policies can be specified here. The class must
+         implement org.apache.lucene.index.IndexDeletionPolicy.
+         The default Solr IndexDeletionPolicy implementation supports
+         deleting index commit points on number of commits, age of
+         commit point and optimized status.
+         The latest commit point should always be preserved regardless
+         of the criteria.
+    -->
+    <!--
+    <deletionPolicy class="solr.SolrDeletionPolicy">
+    -->
+    <!-- The number of commit points to be kept -->
+    <!-- <str name="maxCommitsToKeep">1</str> -->
+    <!-- The number of optimized commit points to be kept -->
+    <!-- <str name="maxOptimizedCommitsToKeep">0</str> -->
+    <!--
+        Delete all commit points once they have reached the given age.
+        Supports DateMathParser syntax e.g.
+      -->
+    <!--
+       <str name="maxCommitAge">30MINUTES</str>
+       <str name="maxCommitAge">1DAY</str>
+    -->
+    <!--
+    </deletionPolicy>
+    -->
+
+    <!-- Lucene Infostream
+         To aid in advanced debugging, Lucene provides an "InfoStream"
+         of detailed information when indexing.
+         Setting The value to true will instruct the underlying Lucene
+         IndexWriter to write its debugging info the specified file
+      -->
+    <!-- <infoStream file="INFOSTREAM.txt">false</infoStream> -->
+  </indexConfig>
+
+
+  <!-- JMX
+       This example enables JMX if and only if an existing MBeanServer
+       is found, use this if you want to configure JMX through JVM
+       parameters. Remove this to disable exposing Solr configuration
+       and statistics to JMX.
+       For more details see http://wiki.apache.org/solr/SolrJmx
+    -->
+  <jmx />
+  <!-- If you want to connect to a particular server, specify the
+       agentId
+    -->
+  <!-- <jmx agentId="myAgent" /> -->
+  <!-- If you want to start a new MBeanServer, specify the serviceUrl -->
+  <!-- <jmx serviceUrl="service:jmx:rmi:///jndi/rmi://localhost:9999/solr"/>
+    -->
+
+  <!-- The default high-performance update handler -->
+  <updateHandler class="solr.DirectUpdateHandler2">
+
+    <!-- Enables a transaction log, used for real-time get, durability, and
+         and solr cloud replica recovery.  The log can grow as big as
+         uncommitted changes to the index, so use of a hard autoCommit
+         is recommended (see below).
+         "dir" - the target directory for transaction logs, defaults to the
+                solr data directory.
+         "numVersionBuckets" - sets the number of buckets used to keep
+                track of max version values when checking for re-ordered
+                updates; increase this value to reduce the cost of
+                synchronizing access to version buckets during high-volume
+                indexing, this requires 8 bytes (long) * numVersionBuckets
+                of heap space per Solr core.
+    -->
+    <updateLog>
+      <str name="dir">${solr.ulog.dir:}</str>
+      <int name="tlogDfsReplication">${solr.ulog.tlogDfsReplication:3}</int>
+      <int name="numVersionBuckets">${solr.ulog.numVersionBuckets:65536}</int>
+    </updateLog>
+
+    <!-- AutoCommit
+         Perform a hard commit automatically under certain conditions.
+         Instead of enabling autoCommit, consider using "commitWithin"
+         when adding documents.
+         http://wiki.apache.org/solr/UpdateXmlMessages
+         maxDocs - Maximum number of documents to add since the last
+                   commit before automatically triggering a new commit.
+         maxTime - Maximum amount of time in ms that is allowed to pass
+                   since a document was added before automatically
+                   triggering a new commit.
+         openSearcher - if false, the commit causes recent index changes
+           to be flushed to stable storage, but does not cause a new
+           searcher to be opened to make those changes visible.
+         If the updateLog is enabled, then it's highly recommended to
+         have some sort of hard autoCommit to limit the log size.
+      -->
+    <autoCommit>
+      <maxTime>${solr.autoCommit.maxTime:60000}</maxTime>
+      <openSearcher>false</openSearcher>
+    </autoCommit>
+
+    <!-- softAutoCommit is like autoCommit except it causes a
+         'soft' commit which only ensures that changes are visible
+         but does not ensure that data is synced to disk.  This is
+         faster and more near-realtime friendly than a hard commit.
+      -->
+
+    <autoSoftCommit>
+      <maxTime>${solr.autoSoftCommit.maxTime:15000}</maxTime>
+    </autoSoftCommit>
+
+    <!-- Update Related Event Listeners
+         Various IndexWriter related events can trigger Listeners to
+         take actions.
+         postCommit - fired after every commit or optimize command
+         postOptimize - fired after every optimize command
+      -->
+    <!-- The RunExecutableListener executes an external command from a
+         hook such as postCommit or postOptimize.
+         exe - the name of the executable to run
+         dir - dir to use as the current working directory. (default=".")
+         wait - the calling thread waits until the executable returns.
+                (default="true")
+         args - the arguments to pass to the program.  (default is none)
+         env - environment variables to set.  (default is none)
+      -->
+    <!-- This example shows how RunExecutableListener could be used
+         with the script based replication...
+         http://wiki.apache.org/solr/CollectionDistribution
+      -->
+    <!--
+       <listener event="postCommit" class="solr.RunExecutableListener">
+         <str name="exe">solr/bin/snapshooter</str>
+         <str name="dir">.</str>
+         <bool name="wait">true</bool>
+         <arr name="args"> <str>arg1</str> <str>arg2</str> </arr>
+         <arr name="env"> <str>MYVAR=val1</str> </arr>
+       </listener>
+      -->
+
+  </updateHandler>
+
+  <!-- IndexReaderFactory
+       Use the following format to specify a custom IndexReaderFactory,
+       which allows for alternate IndexReader implementations.
+       ** Experimental Feature **
+       Please note - Using a custom IndexReaderFactory may prevent
+       certain other features from working. The API to
+       IndexReaderFactory may change without warning or may even be
+       removed from future releases if the problems cannot be
+       resolved.
+       ** Features that may not work with custom IndexReaderFactory **
+       The ReplicationHandler assumes a disk-resident index. Using a
+       custom IndexReader implementation may cause incompatibility
+       with ReplicationHandler and may cause replication to not work
+       correctly. See SOLR-1366 for details.
+    -->
+  <!--
+  <indexReaderFactory name="IndexReaderFactory" class="package.class">
+    <str name="someArg">Some Value</str>
+  </indexReaderFactory >
+  -->
+
+  <!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+       Query section - these settings control query time things like caches
+       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+  <query>
+
+    <!-- Maximum number of clauses allowed when parsing a boolean query string.
+         
+         This limit only impacts boolean queries specified by a user as part of a query string,
+         and provides per-collection controls on how complex user specified boolean queries can
+         be.  Query strings that specify more clauses then this will result in an error.
+         
+         If this per-collection limit is greater then the global `maxBooleanClauses` limit
+         specified in `solr.xml`, it will have no effect, as that setting also limits the size
+         of user specified boolean queries.
+      -->
+    <maxBooleanClauses>${solr.max.booleanClauses:1024}</maxBooleanClauses>
+
+    <!-- Solr Internal Query Caches
+         There are two implementations of cache available for Solr,
+         LRUCache, based on a synchronized LinkedHashMap, and
+         FastLRUCache, based on a ConcurrentHashMap.
+         FastLRUCache has faster gets and slower puts in single
+         threaded operation and thus is generally faster than LRUCache
+         when the hit ratio of the cache is high (> 75%), and may be
+         faster under other scenarios on multi-cpu systems.
+    -->
+
+    <!-- Filter Cache
+         Cache used by SolrIndexSearcher for filters (DocSets),
+         unordered sets of *all* documents that match a query.  When a
+         new searcher is opened, its caches may be prepopulated or
+         "autowarmed" using data from caches in the old searcher.
+         autowarmCount is the number of items to prepopulate.  For
+         LRUCache, the autowarmed items will be the most recently
+         accessed items.
+         Parameters:
+           class - the SolrCache implementation LRUCache or
+               (LRUCache or FastLRUCache)
+           size - the maximum number of entries in the cache
+           initialSize - the initial capacity (number of entries) of
+               the cache.  (see java.util.HashMap)
+           autowarmCount - the number of entries to prepopulate from
+               and old cache.
+           maxRamMB - the maximum amount of RAM (in MB) that this cache is allowed
+                      to occupy. Note that when this option is specified, the size
+                      and initialSize parameters are ignored.
+      -->
+    <filterCache class="solr.FastLRUCache"
+                 size="512"
+                 initialSize="512"
+                 autowarmCount="0"/>
+
+    <!-- Query Result Cache
+         Caches results of searches - ordered lists of document ids
+         (DocList) based on a query, a sort, and the range of documents requested.
+         Additional supported parameter by LRUCache:
+            maxRamMB - the maximum amount of RAM (in MB) that this cache is allowed
+                       to occupy
+      -->
+    <queryResultCache class="solr.LRUCache"
+                      size="512"
+                      initialSize="512"
+                      autowarmCount="0"/>
+
+    <!-- Document Cache
+         Caches Lucene Document objects (the stored fields for each
+         document).  Since Lucene internal document ids are transient,
+         this cache will not be autowarmed.
+      -->
+    <documentCache class="solr.LRUCache"
+                   size="512"
+                   initialSize="512"
+                   autowarmCount="0"/>
+
+    <!-- custom cache currently used by block join -->
+    <cache name="perSegFilter"
+           class="solr.search.LRUCache"
+           size="10"
+           initialSize="0"
+           autowarmCount="10"
+           regenerator="solr.NoOpRegenerator" />
+
+    <!-- Field Value Cache
+         Cache used to hold field values that are quickly accessible
+         by document id.  The fieldValueCache is created by default
+         even if not configured here.
+      -->
+    <!--
+       <fieldValueCache class="solr.FastLRUCache"
+                        size="512"
+                        autowarmCount="128"
+                        showItems="32" />
+      -->
+
+    <!-- Custom Cache
+         Example of a generic cache.  These caches may be accessed by
+         name through SolrIndexSearcher.getCache(),cacheLookup(), and
+         cacheInsert().  The purpose is to enable easy caching of
+         user/application level data.  The regenerator argument should
+         be specified as an implementation of solr.CacheRegenerator
+         if autowarming is desired.
+      -->
+    <!--
+       <cache name="myUserCache"
+              class="solr.LRUCache"
+              size="4096"
+              initialSize="1024"
+              autowarmCount="1024"
+              regenerator="com.mycompany.MyRegenerator"
+              />
+      -->
+
+
+    <!-- Lazy Field Loading
+         If true, stored fields that are not requested will be loaded
+         lazily.  This can result in a significant speed improvement
+         if the usual case is to not load all stored fields,
+         especially if the skipped fields are large compressed text
+         fields.
+    -->
+    <enableLazyFieldLoading>true</enableLazyFieldLoading>
+
+    <!-- Use Filter For Sorted Query
+         A possible optimization that attempts to use a filter to
+         satisfy a search.  If the requested sort does not include
+         score, then the filterCache will be checked for a filter
+         matching the query. If found, the filter will be used as the
+         source of document ids, and then the sort will be applied to
+         that.
+         For most situations, this will not be useful unless you
+         frequently get the same search repeatedly with different sort
+         options, and none of them ever use "score"
+      -->
+    <!--
+       <useFilterForSortedQuery>true</useFilterForSortedQuery>
+      -->
+
+    <!-- Result Window Size
+         An optimization for use with the queryResultCache.  When a search
+         is requested, a superset of the requested number of document ids
+         are collected.  For example, if a search for a particular query
+         requests matching documents 10 through 19, and queryWindowSize is 50,
+         then documents 0 through 49 will be collected and cached.  Any further
+         requests in that range can be satisfied via the cache.
+      -->
+    <queryResultWindowSize>20</queryResultWindowSize>
+
+    <!-- Maximum number of documents to cache for any entry in the
+         queryResultCache.
+      -->
+    <queryResultMaxDocsCached>200</queryResultMaxDocsCached>
+
+    <!-- Query Related Event Listeners
+         Various IndexSearcher related events can trigger Listeners to
+         take actions.
+         newSearcher - fired whenever a new searcher is being prepared
+         and there is a current searcher handling requests (aka
+         registered).  It can be used to prime certain caches to
+         prevent long request times for certain requests.
+         firstSearcher - fired whenever a new searcher is being
+         prepared but there is no current registered searcher to handle
+         requests or to gain autowarming data from.
+      -->
+    <!-- QuerySenderListener takes an array of NamedList and executes a
+         local query request for each NamedList in sequence.
+      -->
+    <listener event="newSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <!--
+           <lst><str name="q">solr</str><str name="sort">price asc</str></lst>
+           <lst><str name="q">rocks</str><str name="sort">weight asc</str></lst>
+          -->
+      </arr>
+    </listener>
+    <listener event="firstSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <!--
+        <lst>
+          <str name="q">static firstSearcher warming in solrconfig.xml</str>
+        </lst>
+        -->
+      </arr>
+    </listener>
+
+    <!-- Use Cold Searcher
+         If a search request comes in and there is no current
+         registered searcher, then immediately register the still
+         warming searcher and use it.  If "false" then all requests
+         will block until the first searcher is done warming.
+      -->
+    <useColdSearcher>false</useColdSearcher>
+
+    <!-- Slow Query Request Logging
+         Any queries that take longer than the specified threshold
+         will be logged as "slow" queries.
+         To disable slow request logging for this Solr config, 
+         set the value to -1 
+      -->
+    <slowQueryThresholdMillis>5000</slowQueryThresholdMillis>
+
+  </query>
+
+
+  <!-- Request Dispatcher
+       This section contains instructions for how the SolrDispatchFilter
+       should behave when processing requests for this SolrCore.
+    -->
+  <requestDispatcher>
+    <!-- Request Parsing
+         These settings indicate how Solr Requests may be parsed, and
+         what restrictions may be placed on the ContentStreams from
+         those requests
+         enableRemoteStreaming - enables use of the stream.file
+         and stream.url parameters for specifying remote streams.
+         multipartUploadLimitInKB - specifies the max size (in KiB) of
+         Multipart File Uploads that Solr will allow in a Request.
+         formdataUploadLimitInKB - specifies the max size (in KiB) of
+         form data (application/x-www-form-urlencoded) sent via
+         POST. You can use POST to pass request parameters not
+         fitting into the URL.
+         addHttpRequestToContext - if set to true, it will instruct
+         the requestParsers to include the original HttpServletRequest
+         object in the context map of the SolrQueryRequest under the
+         key "httpRequest". It will not be used by any of the existing
+         Solr components, but may be useful when developing custom
+         plugins.
+         *** WARNING ***
+         Before enabling remote streaming, you should make sure your
+         system has authentication enabled.
+      -->
+    <requestParsers enableRemoteStreaming="true"
+                    multipartUploadLimitInKB="2048000"
+                    formdataUploadLimitInKB="2048"
+                    addHttpRequestToContext="true"/>
+
+    <!-- HTTP Caching
+         Set HTTP caching related parameters (for proxy caches and clients).
+         The options below instruct Solr not to output any HTTP Caching
+         related headers
+      -->
+    <httpCaching never304="true" />
+    <!-- If you include a <cacheControl> directive, it will be used to
+         generate a Cache-Control header (as well as an Expires header
+         if the value contains "max-age=")
+         By default, no Cache-Control header is generated.
+         You can use the <cacheControl> option even if you have set
+         never304="true"
+      -->
+    <!--
+       <httpCaching never304="true" >
+         <cacheControl>max-age=30, public</cacheControl>
+       </httpCaching>
+      -->
+    <!-- To enable Solr to respond with automatically generated HTTP
+         Caching headers, and to response to Cache Validation requests
+         correctly, set the value of never304="false"
+         This will cause Solr to generate Last-Modified and ETag
+         headers based on the properties of the Index.
+         The following options can also be specified to affect the
+         values of these headers...
+         lastModFrom - the default value is "openTime" which means the
+         Last-Modified value (and validation against If-Modified-Since
+         requests) will all be relative to when the current Searcher
+         was opened.  You can change it to lastModFrom="dirLastMod" if
+         you want the value to exactly correspond to when the physical
+         index was last modified.
+         etagSeed="..." is an option you can change to force the ETag
+         header (and validation against If-None-Match requests) to be
+         different even if the index has not changed (ie: when making
+         significant changes to your config file)
+         (lastModifiedFrom and etagSeed are both ignored if you use
+         the never304="true" option)
+      -->
+    <!--
+       <httpCaching lastModifiedFrom="openTime"
+                    etagSeed="Solr">
+         <cacheControl>max-age=30, public</cacheControl>
+       </httpCaching>
+      -->
+  </requestDispatcher>
+
+  <!-- Request Handlers
+       http://wiki.apache.org/solr/SolrRequestHandler
+       Incoming queries will be dispatched to a specific handler by name
+       based on the path specified in the request.
+       If a Request Handler is declared with startup="lazy", then it will
+       not be initialized until the first request that uses it.
+    -->
+  <!-- SearchHandler
+       http://wiki.apache.org/solr/SearchHandler
+       For processing Search Queries, the primary Request Handler
+       provided with Solr is "SearchHandler" It delegates to a sequent
+       of SearchComponents (see below) and supports distributed
+       queries across multiple shards
+    -->
+  <requestHandler name="/select" class="solr.SearchHandler">
+    <!-- default values for query parameters can be specified, these
+         will be overridden by parameters in the request
+      -->
+    <lst name="defaults">
+      <str name="echoParams">explicit</str>
+      <int name="rows">10</int>
+      <!-- Default search field
+         <str name="df">text</str> 
+        -->
+      <!-- Change from JSON to XML format (the default prior to Solr 7.0)
+         <str name="wt">xml</str> 
+        -->
+    </lst>
+    <!-- In addition to defaults, "appends" params can be specified
+         to identify values which should be appended to the list of
+         multi-val params from the query (or the existing "defaults").
+      -->
+    <!-- In this example, the param "fq=instock:true" would be appended to
+         any query time fq params the user may specify, as a mechanism for
+         partitioning the index, independent of any user selected filtering
+         that may also be desired (perhaps as a result of faceted searching).
+         NOTE: there is *absolutely* nothing a client can do to prevent these
+         "appends" values from being used, so don't use this mechanism
+         unless you are sure you always want it.
+      -->
+    <!--
+       <lst name="appends">
+         <str name="fq">inStock:true</str>
+       </lst>
+      -->
+    <!-- "invariants" are a way of letting the Solr maintainer lock down
+         the options available to Solr clients.  Any params values
+         specified here are used regardless of what values may be specified
+         in either the query, the "defaults", or the "appends" params.
+         In this example, the facet.field and facet.query params would
+         be fixed, limiting the facets clients can use.  Faceting is
+         not turned on by default - but if the client does specify
+         facet=true in the request, these are the only facets they
+         will be able to see counts for; regardless of what other
+         facet.field or facet.query params they may specify.
+         NOTE: there is *absolutely* nothing a client can do to prevent these
+         "invariants" values from being used, so don't use this mechanism
+         unless you are sure you always want it.
+      -->
+    <!--
+       <lst name="invariants">
+         <str name="facet.field">cat</str>
+         <str name="facet.field">manu_exact</str>
+         <str name="facet.query">price:[* TO 500]</str>
+         <str name="facet.query">price:[500 TO *]</str>
+       </lst>
+      -->
+    <!-- If the default list of SearchComponents is not desired, that
+         list can either be overridden completely, or components can be
+         prepended or appended to the default list.  (see below)
+      -->
+    <!--
+       <arr name="components">
+         <str>nameOfCustomComponent1</str>
+         <str>nameOfCustomComponent2</str>
+       </arr>
+      -->
+  </requestHandler>
+
+  <!-- A request handler that returns indented JSON by default -->
+  <requestHandler name="/query" class="solr.SearchHandler">
+    <lst name="defaults">
+      <str name="echoParams">explicit</str>
+      <str name="wt">json</str>
+      <str name="indent">true</str>
+    </lst>
+  </requestHandler>
+
+  <initParams path="/update/**,/query,/select,/spell">
+    <lst name="defaults">
+      <str name="df">_text_</str>
+    </lst>
+  </initParams>
+
+  <!-- Search Components
+       Search components are registered to SolrCore and used by
+       instances of SearchHandler (which can access them by name)
+       By default, the following components are available:
+       <searchComponent name="query"     class="solr.QueryComponent" />
+       <searchComponent name="facet"     class="solr.FacetComponent" />
+       <searchComponent name="mlt"       class="solr.MoreLikeThisComponent" />
+       <searchComponent name="highlight" class="solr.HighlightComponent" />
+       <searchComponent name="stats"     class="solr.StatsComponent" />
+       <searchComponent name="debug"     class="solr.DebugComponent" />
+       Default configuration in a requestHandler would look like:
+       <arr name="components">
+         <str>query</str>
+         <str>facet</str>
+         <str>mlt</str>
+         <str>highlight</str>
+         <str>stats</str>
+         <str>debug</str>
+       </arr>
+       If you register a searchComponent to one of the standard names,
+       that will be used instead of the default.
+       To insert components before or after the 'standard' components, use:
+       <arr name="first-components">
+         <str>myFirstComponentName</str>
+       </arr>
+       <arr name="last-components">
+         <str>myLastComponentName</str>
+       </arr>
+       NOTE: The component registered with the name "debug" will
+       always be executed after the "last-components"
+     -->
+
+  <!-- Spell Check
+       The spell check component can return a list of alternative spelling
+       suggestions.
+       http://wiki.apache.org/solr/SpellCheckComponent
+    -->
+  <searchComponent name="spellcheck" class="solr.SpellCheckComponent">
+
+    <str name="queryAnalyzerFieldType">text_general</str>
+
+    <!-- Multiple "Spell Checkers" can be declared and used by this
+         component
+      -->
+
+    <!-- a spellchecker built from a field of the main index -->
+    <lst name="spellchecker">
+      <str name="name">default</str>
+      <str name="field">_text_</str>
+      <str name="classname">solr.DirectSolrSpellChecker</str>
+      <!-- the spellcheck distance measure used, the default is the internal levenshtein -->
+      <str name="distanceMeasure">internal</str>
+      <!-- minimum accuracy needed to be considered a valid spellcheck suggestion -->
+      <float name="accuracy">0.5</float>
+      <!-- the maximum #edits we consider when enumerating terms: can be 1 or 2 -->
+      <int name="maxEdits">2</int>
+      <!-- the minimum shared prefix when enumerating terms -->
+      <int name="minPrefix">1</int>
+      <!-- maximum number of inspections per result. -->
+      <int name="maxInspections">5</int>
+      <!-- minimum length of a query term to be considered for correction -->
+      <int name="minQueryLength">4</int>
+      <!-- maximum threshold of documents a query term can appear to be considered for correction -->
+      <float name="maxQueryFrequency">0.01</float>
+      <!-- uncomment this to require suggestions to occur in 1% of the documents
+        <float name="thresholdTokenFrequency">.01</float>
+      -->
+    </lst>
+
+    <!-- a spellchecker that can break or combine words.  See "/spell" handler below for usage -->
+    <!--
+    <lst name="spellchecker">
+      <str name="name">wordbreak</str>
+      <str name="classname">solr.WordBreakSolrSpellChecker</str>
+      <str name="field">name</str>
+      <str name="combineWords">true</str>
+      <str name="breakWords">true</str>
+      <int name="maxChanges">10</int>
+    </lst>
+    -->
+  </searchComponent>
+
+  <!-- A request handler for demonstrating the spellcheck component.
+       NOTE: This is purely as an example.  The whole purpose of the
+       SpellCheckComponent is to hook it into the request handler that
+       handles your normal user queries so that a separate request is
+       not needed to get suggestions.
+       IN OTHER WORDS, THERE IS REALLY GOOD CHANCE THE SETUP BELOW IS
+       NOT WHAT YOU WANT FOR YOUR PRODUCTION SYSTEM!
+       See http://wiki.apache.org/solr/SpellCheckComponent for details
+       on the request parameters.
+    -->
+  <requestHandler name="/spell" class="solr.SearchHandler" startup="lazy">
+    <lst name="defaults">
+      <!-- Solr will use suggestions from both the 'default' spellchecker
+           and from the 'wordbreak' spellchecker and combine them.
+           collations (re-written queries) can include a combination of
+           corrections from both spellcheckers -->
+      <str name="spellcheck.dictionary">default</str>
+      <str name="spellcheck">on</str>
+      <str name="spellcheck.extendedResults">true</str>
+      <str name="spellcheck.count">10</str>
+      <str name="spellcheck.alternativeTermCount">5</str>
+      <str name="spellcheck.maxResultsForSuggest">5</str>
+      <str name="spellcheck.collate">true</str>
+      <str name="spellcheck.collateExtendedResults">true</str>
+      <str name="spellcheck.maxCollationTries">10</str>
+      <str name="spellcheck.maxCollations">5</str>
+    </lst>
+    <arr name="last-components">
+      <str>spellcheck</str>
+    </arr>
+  </requestHandler>
+
+  <!-- Terms Component
+       http://wiki.apache.org/solr/TermsComponent
+       A component to return terms and document frequency of those
+       terms
+    -->
+  <searchComponent name="terms" class="solr.TermsComponent"/>
+
+  <!-- A request handler for demonstrating the terms component -->
+  <requestHandler name="/terms" class="solr.SearchHandler" startup="lazy">
+    <lst name="defaults">
+      <bool name="terms">true</bool>
+      <bool name="distrib">false</bool>
+    </lst>
+    <arr name="components">
+      <str>terms</str>
+    </arr>
+  </requestHandler>
+
+  <!-- Highlighting Component
+       http://wiki.apache.org/solr/HighlightingParameters
+    -->
+  <searchComponent class="solr.HighlightComponent" name="highlight">
+    <highlighting>
+      <!-- Configure the standard fragmenter -->
+      <!-- This could most likely be commented out in the "default" case -->
+      <fragmenter name="gap"
+                  default="true"
+                  class="solr.highlight.GapFragmenter">
+        <lst name="defaults">
+          <int name="hl.fragsize">100</int>
+        </lst>
+      </fragmenter>
+
+      <!-- A regular-expression-based fragmenter
+           (for sentence extraction)
+        -->
+      <fragmenter name="regex"
+                  class="solr.highlight.RegexFragmenter">
+        <lst name="defaults">
+          <!-- slightly smaller fragsizes work better because of slop -->
+          <int name="hl.fragsize">70</int>
+          <!-- allow 50% slop on fragment sizes -->
+          <float name="hl.regex.slop">0.5</float>
+          <!-- a basic sentence pattern -->
+          <str name="hl.regex.pattern">[-\w ,/\n\&quot;&apos;]{20,200}</str>
+        </lst>
+      </fragmenter>
+
+      <!-- Configure the standard formatter -->
+      <formatter name="html"
+                 default="true"
+                 class="solr.highlight.HtmlFormatter">
+        <lst name="defaults">
+          <str name="hl.simple.pre"><![CDATA[<em>]]></str>
+          <str name="hl.simple.post"><![CDATA[</em>]]></str>
+        </lst>
+      </formatter>
+
+      <!-- Configure the standard encoder -->
+      <encoder name="html"
+               class="solr.highlight.HtmlEncoder" />
+
+      <!-- Configure the standard fragListBuilder -->
+      <fragListBuilder name="simple"
+                       class="solr.highlight.SimpleFragListBuilder"/>
+
+      <!-- Configure the single fragListBuilder -->
+      <fragListBuilder name="single"
+                       class="solr.highlight.SingleFragListBuilder"/>
+
+      <!-- Configure the weighted fragListBuilder -->
+      <fragListBuilder name="weighted"
+                       default="true"
+                       class="solr.highlight.WeightedFragListBuilder"/>
+
+      <!-- default tag FragmentsBuilder -->
+      <fragmentsBuilder name="default"
+                        default="true"
+                        class="solr.highlight.ScoreOrderFragmentsBuilder">
+        <!--
+        <lst name="defaults">
+          <str name="hl.multiValuedSeparatorChar">/</str>
+        </lst>
+        -->
+      </fragmentsBuilder>
+
+      <!-- multi-colored tag FragmentsBuilder -->
+      <fragmentsBuilder name="colored"
+                        class="solr.highlight.ScoreOrderFragmentsBuilder">
+        <lst name="defaults">
+          <str name="hl.tag.pre"><![CDATA[
+               <b style="background:yellow">,<b style="background:lawgreen">,
+               <b style="background:aquamarine">,<b style="background:magenta">,
+               <b style="background:palegreen">,<b style="background:coral">,
+               <b style="background:wheat">,<b style="background:khaki">,
+               <b style="background:lime">,<b style="background:deepskyblue">]]></str>
+          <str name="hl.tag.post"><![CDATA[</b>]]></str>
+        </lst>
+      </fragmentsBuilder>
+
+      <boundaryScanner name="default"
+                       default="true"
+                       class="solr.highlight.SimpleBoundaryScanner">
+        <lst name="defaults">
+          <str name="hl.bs.maxScan">10</str>
+          <str name="hl.bs.chars">.,!? &#9;&#10;&#13;</str>
+        </lst>
+      </boundaryScanner>
+
+      <boundaryScanner name="breakIterator"
+                       class="solr.highlight.BreakIteratorBoundaryScanner">
+        <lst name="defaults">
+          <!-- type should be one of CHARACTER, WORD(default), LINE and SENTENCE -->
+          <str name="hl.bs.type">WORD</str>
+          <!-- language and country are used when constructing Locale object.  -->
+          <!-- And the Locale object will be used when getting instance of BreakIterator -->
+          <str name="hl.bs.language">en</str>
+          <str name="hl.bs.country">US</str>
+        </lst>
+      </boundaryScanner>
+    </highlighting>
+  </searchComponent>
+
+  <!-- Update Processors
+       Chains of Update Processor Factories for dealing with Update
+       Requests can be declared, and then used by name in Update
+       Request Processors
+       http://wiki.apache.org/solr/UpdateRequestProcessor
+    -->
+
+  <!-- Add unknown fields to the schema
+       Field type guessing update processors that will
+       attempt to parse string-typed field values as Booleans, Longs,
+       Doubles, or Dates, and then add schema fields with the guessed
+       field types. Text content will be indexed as "text_general" as
+       well as a copy to a plain string version in *_str.
+       These require that the schema is both managed and mutable, by
+       declaring schemaFactory as ManagedIndexSchemaFactory, with
+       mutable specified as true.
+       See http://wiki.apache.org/solr/GuessingFieldTypes
+    -->
+  <updateProcessor class="solr.UUIDUpdateProcessorFactory" name="uuid"/>
+  <updateProcessor class="solr.RemoveBlankFieldUpdateProcessorFactory" name="remove-blank"/>
+  <updateProcessor class="solr.FieldNameMutatingUpdateProcessorFactory" name="field-name-mutating">
+    <str name="pattern">[^\w-\.]</str>
+    <str name="replacement">_</str>
+  </updateProcessor>
+  <updateProcessor class="solr.ParseBooleanFieldUpdateProcessorFactory" name="parse-boolean"/>
+  <updateProcessor class="solr.ParseLongFieldUpdateProcessorFactory" name="parse-long"/>
+  <updateProcessor class="solr.ParseDoubleFieldUpdateProcessorFactory" name="parse-double"/>
+  <updateProcessor class="solr.ParseDateFieldUpdateProcessorFactory" name="parse-date">
+    <arr name="format">
+      <str>yyyy-MM-dd['T'[HH:mm[:ss[.SSS]][z</str>
+      <str>yyyy-MM-dd['T'[HH:mm[:ss[,SSS]][z</str>
+      <str>yyyy-MM-dd HH:mm[:ss[.SSS]][z</str>
+      <str>yyyy-MM-dd HH:mm[:ss[,SSS]][z</str>
+      <str>[EEE, ]dd MMM yyyy HH:mm[:ss] z</str>
+      <str>EEEE, dd-MMM-yy HH:mm:ss z</str>
+      <str>EEE MMM ppd HH:mm:ss [z ]yyyy</str>
+    </arr>
+  </updateProcessor>
+  <updateProcessor class="solr.AddSchemaFieldsUpdateProcessorFactory" name="add-schema-fields">
+    <str name="defaultFieldType">key_lower_case</str>
+    <lst name="typeMapping">
+      <str name="valueClass">java.lang.Boolean</str>
+      <str name="fieldType">boolean</str>
+    </lst>
+    <lst name="typeMapping">
+      <str name="valueClass">java.util.Date</str>
+      <str name="fieldType">tdate</str>
+    </lst>
+    <lst name="typeMapping">
+      <str name="valueClass">java.lang.Long</str>
+      <str name="valueClass">java.lang.Integer</str>
+      <str name="fieldType">tlong</str>
+    </lst>
+    <lst name="typeMapping">
+      <str name="valueClass">java.lang.Number</str>
+      <str name="fieldType">tdouble</str>
+    </lst>
+  </updateProcessor>
+
+  <!-- The update.autoCreateFields property can be turned to false to disable schemaless mode -->
+  <updateRequestProcessorChain name="add-unknown-fields-to-the-schema" default="${update.autoCreateFields:true}"
+           processor="uuid,remove-blank,field-name-mutating,parse-boolean,parse-long,parse-double,parse-date,add-schema-fields">
+    <processor class="solr.DefaultValueUpdateProcessorFactory">
+    	<str name="fieldName">_ttl_</str>
+    	<str name="value">+90DAYS</str>
+    </processor>
+    <processor class="solr.processor.DocExpirationUpdateProcessorFactory">
+    	<int name="autoDeletePeriodSeconds">86400</int>
+    	<str name="ttlFieldName">_ttl_</str>
+    	<str name="expirationFieldName">_expire_at_</str>
+    </processor>
+    <processor class="solr.FirstFieldValueUpdateProcessorFactory">
+    	<str name="fieldName">_expire_at_</str>
+    </processor>
+    <processor class="solr.LogUpdateProcessorFactory"/>
+    <processor class="solr.DistributedUpdateProcessorFactory"/>
+    <processor class="solr.RunUpdateProcessorFactory"/>
+  </updateRequestProcessorChain>
+
+  <!-- Deduplication
+       An example dedup update processor that creates the "id" field
+       on the fly based on the hash code of some other fields.  This
+       example has overwriteDupes set to false since we are using the
+       id field as the signatureField and Solr will maintain
+       uniqueness based on that anyway.
+    -->
+  <!--
+     <updateRequestProcessorChain name="dedupe">
+       <processor class="solr.processor.SignatureUpdateProcessorFactory">
+         <bool name="enabled">true</bool>
+         <str name="signatureField">id</str>
+         <bool name="overwriteDupes">false</bool>
+         <str name="fields">name,features,cat</str>
+         <str name="signatureClass">solr.processor.Lookup3Signature</str>
+       </processor>
+       <processor class="solr.LogUpdateProcessorFactory" />
+       <processor class="solr.RunUpdateProcessorFactory" />
+     </updateRequestProcessorChain>
+    -->
+
+  <!-- Response Writers
+       http://wiki.apache.org/solr/QueryResponseWriter
+       Request responses will be written using the writer specified by
+       the 'wt' request parameter matching the name of a registered
+       writer.
+       The "default" writer is the default and will be used if 'wt' is
+       not specified in the request.
+    -->
+  <!-- The following response writers are implicitly configured unless
+       overridden...
+    -->
+  <!--
+     <queryResponseWriter name="xml"
+                          default="true"
+                          class="solr.XMLResponseWriter" />
+     <queryResponseWriter name="json" class="solr.JSONResponseWriter"/>
+     <queryResponseWriter name="python" class="solr.PythonResponseWriter"/>
+     <queryResponseWriter name="ruby" class="solr.RubyResponseWriter"/>
+     <queryResponseWriter name="php" class="solr.PHPResponseWriter"/>
+     <queryResponseWriter name="phps" class="solr.PHPSerializedResponseWriter"/>
+     <queryResponseWriter name="csv" class="solr.CSVResponseWriter"/>
+     <queryResponseWriter name="schema.xml" class="solr.SchemaXmlResponseWriter"/>
+    -->
+
+  <queryResponseWriter name="json" class="solr.JSONResponseWriter">
+    <!-- For the purposes of the tutorial, JSON responses are written as
+     plain text so that they are easy to read in *any* browser.
+     If you expect a MIME type of "application/json" just remove this override.
+    -->
+    <str name="content-type">text/plain; charset=UTF-8</str>
+  </queryResponseWriter>
+
+  <!-- Query Parsers
+       https://lucene.apache.org/solr/guide/query-syntax-and-parsing.html
+       Multiple QParserPlugins can be registered by name, and then
+       used in either the "defType" param for the QueryComponent (used
+       by SearchHandler) or in LocalParams
+    -->
+  <!-- example of registering a query parser -->
+  <!--
+     <queryParser name="myparser" class="com.mycompany.MyQParserPlugin"/>
+    -->
+
+  <!-- Function Parsers
+       http://wiki.apache.org/solr/FunctionQuery
+       Multiple ValueSourceParsers can be registered by name, and then
+       used as function names when using the "func" QParser.
+    -->
+  <!-- example of registering a custom function parser  -->
+  <!--
+     <valueSourceParser name="myfunc"
+                        class="com.mycompany.MyValueSourceParser" />
+    -->
+
+
+  <!-- Document Transformers
+       http://wiki.apache.org/solr/DocTransformers
+    -->
+  <!--
+     Could be something like:
+     <transformer name="db" class="com.mycompany.LoadFromDatabaseTransformer" >
+       <int name="connection">jdbc://....</int>
+     </transformer>
+     To add a constant value to all docs, use:
+     <transformer name="mytrans2" class="org.apache.solr.response.transform.ValueAugmenterFactory" >
+       <int name="value">5</int>
+     </transformer>
+     If you want the user to still be able to change it with _value:something_ use this:
+     <transformer name="mytrans3" class="org.apache.solr.response.transform.ValueAugmenterFactory" >
+       <double name="defaultValue">5</double>
+     </transformer>
+      If you are using the QueryElevationComponent, you may wish to mark documents that get boosted.  The
+      EditorialMarkerFactory will do exactly that:
+     <transformer name="qecBooster" class="org.apache.solr.response.transform.EditorialMarkerFactory" />
+    -->
+</config>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/resource-types.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/resource-types.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/resource-types.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/resource-types.xml	(date 1719626239000)
@@ -0,0 +1,47 @@
+<?xml version="1.0"?>
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+<configuration>
+    <property>
+        <name>yarn.resource-types</name>
+        <value></value>
+        <description>Enable resource types other than memory and vcores, values split by comma. For example value=yarn.io/gpu enables GPU resource types</description>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>container-executor</type>
+                <name>gpu_module_enabled</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>yarn.resource-types.yarn.io_gpu.maximum-allocation</name>
+        <value>8</value>
+        <display-name>Maximum Container Size (GPU)</display-name>
+        <description>Maximum GPU Allocation</description>
+        <value-attributes>
+            <type>int</type>
+            <minimum>0</minimum>
+            <maximum>8</maximum>
+            <increment-step>1</increment-step>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-env.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-env.xml	(date 1719626239000)
@@ -0,0 +1,261 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="true">
+
+  <property>
+    <name>yarn_hbase_pid_dir_prefix</name>
+    <value>/var/run/hadoop-yarn-hbase</value>
+    <display-name>HBase PID Dir</display-name>
+    <description>Pid Directory for HBase.</description>
+    <value-attributes>
+      <type>directory</type>
+      <overridable>false</overridable>
+      <editable-only-at-install>true</editable-only-at-install>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase_java_io_tmpdir</name>
+    <value>/tmp</value>
+    <description>Used in hbase-env.sh as HBASE_OPTS=-Djava.io.tmpdir=java_io_tmpdir</description>
+    <value-attributes>
+      <type>directory</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <!-- hbase-env.sh -->
+  <property>
+    <name>content</name>
+    <display-name>hbase-env template</display-name>
+    <description>This is the jinja template for hbase-env.sh file</description>
+    <value>
+      # Set environment variables here.
+
+      # The java implementation to use. Java 1.6 required.
+      export JAVA_HOME={{java64_home}}
+
+      # HBase Configuration directory
+      export HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{yarn_hbase_conf_dir}}}
+
+      # Extra Java CLASSPATH elements. Optional.
+      export HBASE_CLASSPATH=${HBASE_CLASSPATH}
+
+
+      # The maximum amount of heap to use. Default is left to JVM default.
+      # export HBASE_HEAPSIZE=4G
+
+      # Extra Java runtime options.
+      # Below are what we set by default. May only work with SUN JVM.
+      # For more on why as well as other possible settings,
+      # see http://wiki.apache.org/hadoop/PerformanceTuning
+      export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{yarn_hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`"
+      # Uncomment below to enable java garbage collection logging.
+      # export HBASE_OPTS="$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log"
+
+      # Uncomment and adjust to enable JMX exporting
+      # See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.
+      # More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html
+      #
+      # export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"
+      # If you want to configure BucketCache, specify '-XX: MaxDirectMemorySize=' with proper direct memory size
+      # export HBASE_THRIFT_OPTS="$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103"
+      # export HBASE_ZOOKEEPER_OPTS="$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104"
+
+      # File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.
+      export HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers
+
+      # Extra ssh options. Empty by default.
+      # export HBASE_SSH_OPTS="-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR"
+
+      # Where log files are stored. $HBASE_HOME/logs by default.
+      export HBASE_LOG_DIR=${HBASE_LOG_DIR:-{{yarn_hbase_log_dir}}}
+
+      # A string representing this instance of hbase. $USER by default.
+      # export HBASE_IDENT_STRING=$USER
+
+      # The scheduling priority for daemon processes. See 'man nice'.
+      # export HBASE_NICENESS=10
+
+      # The directory where pid files are stored. /tmp by default.
+      export HBASE_PID_DIR=${HBASE_PID_DIR:-{{yarn_hbase_pid_dir}}}
+
+      # Seconds to sleep between slave commands. Unset by default. This
+      # can be useful in large clusters, where, e.g., slave rsyncs can
+      # otherwise arrive faster than the master can service them.
+      # export HBASE_SLAVE_SLEEP=0.1
+
+      # Tell HBase whether it should manage it's own instance of Zookeeper or not.
+      export HBASE_MANAGES_ZK=false
+
+      {% if java_version &lt; 8 %}
+      JDK_DEPENDED_OPTS="-XX:PermSize=128m -XX:MaxPermSize=128m"
+      {% endif %}
+
+      export HBASE_OPTS="$HBASE_OPTS -XX:+UseConcMarkSweepGC -XX:ErrorFile=$HBASE_LOG_DIR/hs_err_pid%p.log -Djava.io.tmpdir={{yarn_hbase_java_io_tmpdir}}"
+      export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx{{yarn_hbase_master_heapsize}} $JDK_DEPENDED_OPTS"
+      export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:CMSInitiatingOccupancyFraction=70 -XX:ReservedCodeCacheSize=256m -Xms{{yarn_hbase_regionserver_heapsize}} -Xmx{{yarn_hbase_regionserver_heapsize}} $JDK_DEPENDED_OPTS"
+
+      {% if security_enabled %}
+      export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{yarn_hbase_master_jaas_file}}"
+      export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{yarn_hbase_regionserver_jaas_file}}"
+      {% endif %}
+    </value>
+    <value-attributes>
+      <type>content</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>is_hbase_system_service_launch</name>
+    <value>false</value>
+    <description>Should Hbase cluster started as system service. This
+      configuration depends on use_external_hbase property. If
+      use_external_hbase is set, then this property doesn't take effect.
+    </description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_system_service_queue_name</name>
+    <value>default</value>
+    <description>
+      The queue that used by service check.
+    </description>
+    <depends-on>
+      <property>
+        <type>capacity-scheduler</type>
+        <name>yarn.scheduler.capacity.root.queues</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_system_service_launch_mode</name>
+    <value>sync</value>
+    <description>Should services are launched as sync or async.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>yarn_hbase_master_cpu</name>
+    <value>1</value>
+    <description>Number of CPU for master container.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_master_memory</name>
+    <value>4096</value>
+    <description>master container memory in MB.</description>
+    <value-attributes>
+      <type>int</type>
+      <minimum>2048</minimum>
+      <maximum>4096</maximum>
+      <unit>MB</unit>
+      <increment-step>256</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_master_containers</name>
+    <value>1</value>
+    <description>Number of containers to launch for master.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>yarn_hbase_regionserver_cpu</name>
+    <value>1</value>
+    <description>Number of CPU for regionserver container.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_regionserver_memory</name>
+    <value>4096</value>
+    <description>regionserver container memory in MB.</description>
+    <value-attributes>
+      <type>int</type>
+      <minimum>2048</minimum>
+      <maximum>4096</maximum>
+      <unit>MB</unit>
+      <increment-step>256</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_regionserver_containers</name>
+    <value>1</value>
+    <description>Number of containers to launch for regionserver.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>yarn_hbase_client_cpu</name>
+    <value>1</value>
+    <description>Number of CPU for client container.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_client_memory</name>
+    <value>1536</value>
+    <description>client container memory in MB.</description>
+    <value-attributes>
+      <type>int</type>
+      <minimum>1024</minimum>
+      <maximum>2048</maximum>
+      <unit>MB</unit>
+      <increment-step>256</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_client_containers</name>
+    <value>1</value>
+    <description>Number of containers to launch for client.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>yarn_hbase_heap_memory_factor</name>
+    <value>0.8</value>
+    <description>Heap memory is auto derived using this factor.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>use_external_hbase</name>
+    <value>false</value>
+    <description>Setting true, doesn't start embedded hbase or system service
+      hbase. Note: Admin/User need to take care of pointing right hbase-site.xml
+      into RM/NM classpath. If system service hbase is started, then admin must
+      clean up system service hbase before making this change.</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="true"/>
+  </property>
+  <property>
+    <name>yarn_hbase_log_level</name>
+    <value>INFO</value>
+    <description>Setting log level to hmaster and regionserver. Default to info logs. Log levels could be INFO, DEBUG, WARN</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-site.xml	(date 1719626239000)
@@ -0,0 +1,629 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+
+  <property>
+    <name>hbase.rootdir</name>
+    <value>/atsv2/hbase/data</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.cluster.distributed</name>
+    <value>true</value>
+    <on-ambari-upgrade add="false"/>
+    <description>The mode the cluster will be in. Possible values are false for
+      standalone mode and true for distributed mode. If false, startup will run
+      all HBase and ZooKeeper daemons together in the one JVM.
+    </description>
+  </property>
+  <property>
+    <name>hbase.master.port</name>
+    <value>17000</value>
+    <display-name>HBase Master Port</display-name>
+    <value-attributes>
+      <type>int</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.tmp.dir</name>
+    <value>/tmp/hbase-${user.name}</value>
+    <display-name>HBase tmp directory</display-name>
+    <value-attributes>
+      <type>directory</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.local.dir</name>
+    <value>${hbase.tmp.dir}/local</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.master.info.bindAddress</name>
+    <value>0.0.0.0</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.master.info.port</name>
+    <value>17010</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.regionserver.info.port</name>
+    <value>17030</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>zookeeper.session.timeout</name>
+    <value>90000</value>
+    <display-name>Zookeeper Session Timeout</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>10000</minimum>
+      <maximum>180000</maximum>
+      <unit>milliseconds</unit>
+      <increment-step>10000</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.client.retries.number</name>
+    <value>7</value>
+    <description>Maximum retries.  Used as maximum for all retryable
+    operations such as the getting of a cell's value, starting a row update,
+    etc.  Retry interval is a rough function based on hbase.client.pause.  At
+    first we retry at this interval but then with backoff, we pretty quickly reach
+    retrying every ten seconds.  See HConstants#RETRY_BACKOFF for how the backup
+    ramps up.  Change this setting and hbase.client.pause to suit your workload.</description>
+    <display-name>Maximum Client Retries</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>5</minimum>
+      <maximum>50</maximum>
+      <increment-step>1</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.rpc.timeout</name>
+    <value>90000</value>
+    <description>
+      This is for the RPC layer to define how long HBase client applications
+      take for a remote call to time out. It uses pings to check connections
+      but will eventually throw a TimeoutException.
+    </description>
+    <display-name>HBase RPC Timeout</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>10000</minimum>
+      <maximum>180000</maximum>
+      <unit>milliseconds</unit>
+      <increment-step>10000</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.zookeeper.quorum</name>
+    <value>{{zookeeper_quorum_hosts}}</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>hbase.zookeeper.property.clientPort</name>
+    <value>{{zookeeper_clientPort}}</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>zookeeper.znode.parent</name>
+    <value>/atsv2-hbase-unsecure</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.regionserver.port</name>
+    <value>17020</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>hbase.regionserver.handler.count</name>
+    <value>30</value>
+    <description>
+      Count of RPC Listener instances spun up on RegionServers.
+      Same property is used by the Master for count of master handlers.
+    </description>
+    <display-name>Number of Handlers per RegionServer</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>5</minimum>
+      <maximum>240</maximum>
+      <increment-step>1</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hregion.majorcompaction</name>
+    <value>604800000</value>
+    <description>Time between major compactions, expressed in milliseconds. Set to 0 to disable
+      time-based automatic major compactions. User-requested and size-based major compactions will
+      still run. This value is multiplied by hbase.hregion.majorcompaction.jitter to cause
+      compaction to start at a somewhat-random time during a given window of time. The default value
+      is 7 days, expressed in milliseconds. If major compactions are causing disruption in your
+      environment, you can configure them to run at off-peak times for your deployment, or disable
+      time-based major compactions by setting this parameter to 0, and run major compactions in a
+      cron job or by another external mechanism.</description>
+    <display-name>Major Compaction Interval</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+      <maximum>2592000000</maximum>
+      <unit>milliseconds</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hregion.memstore.block.multiplier</name>
+    <value>4</value>
+    <description>
+      Block updates if memstore has hbase.hregion.memstore.block.multiplier
+      times hbase.hregion.memstore.flush.size bytes.  Useful preventing
+      runaway memstore during spikes in update traffic.  Without an
+      upper-bound, memstore fills such that when it flushes the
+      resultant flush files take a long time to compact or split, or
+      worse, we OOME.
+    </description>
+    <display-name>HBase Region Block Multiplier</display-name>
+    <value-attributes>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>2</value>
+        </entry>
+        <entry>
+          <value>4</value>
+        </entry>
+        <entry>
+          <value>8</value>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hregion.memstore.flush.size</name>
+    <value>134217728</value>
+    <description>
+      The size of an individual memstore. Each column familiy within each region is allocated its own memstore.
+    </description>
+    <display-name>Memstore Flush Size</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>33554432</minimum>
+      <maximum>268435456</maximum>
+      <increment-step>1048576</increment-step>
+      <unit>B</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hregion.memstore.mslab.enabled</name>
+    <value>true</value>
+    <description>
+      Enables the MemStore-Local Allocation Buffer,
+      a feature which works to prevent heap fragmentation under
+      heavy write loads. This can reduce the frequency of stop-the-world
+      GC pauses on large heaps.
+    </description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hregion.max.filesize</name>
+    <value>10737418240</value>
+    <description>
+      Maximum HFile size. If the sum of the sizes of a region's HFiles has grown to exceed this
+      value, the region is split in two.
+    </description>
+    <display-name>Maximum Region File Size</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>1073741824</minimum>
+      <maximum>107374182400</maximum>
+      <unit>B</unit>
+      <increment-step>1073741824</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.client.scanner.caching</name>
+    <value>100</value>
+    <description>Number of rows that will be fetched when calling next
+    on a scanner if it is not served from (local, client) memory. Higher
+    caching values will enable faster scanners but will eat up more memory
+    and some calls of next may take longer and longer times when the cache is empty.
+    Do not set this value such that the time between invocations is greater
+    than the scanner timeout; i.e. hbase.regionserver.lease.period
+    </description>
+    <display-name>Number of Fetched Rows when Scanning from Disk</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>100</minimum>
+      <maximum>10000</maximum>
+      <increment-step>100</increment-step>
+      <unit>rows</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.client.keyvalue.maxsize</name>
+    <value>1048576</value>
+    <description>
+      Specifies the combined maximum allowed size of a KeyValue
+      instance. This is to set an upper boundary for a single entry saved in a
+      storage file. Since they cannot be split it helps avoiding that a region
+      cannot be split any further because the data is too large. It seems wise
+      to set this to a fraction of the maximum region size. Setting it to zero
+      or less disables the check.
+    </description>
+    <display-name>Maximum Record Size</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>1048576</minimum>
+      <maximum>31457280</maximum>
+      <unit>B</unit>
+      <increment-step>262144</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hstore.compactionThreshold</name>
+    <value>3</value>
+    <description>
+      The maximum number of StoreFiles which will be selected for a single minor
+      compaction, regardless of the number of eligible StoreFiles. Effectively, the value of
+      hbase.hstore.compaction.max controls the length of time it takes a single compaction to
+      complete. Setting it larger means that more StoreFiles are included in a compaction. For most
+      cases, the default value is appropriate.
+    </description>
+    <display-name>Maximum Store Files before Minor Compaction</display-name>
+    <value-attributes>
+      <type>int</type>
+      <entries>
+        <entry>
+          <value>2</value>
+        </entry>
+        <entry>
+          <value>3</value>
+        </entry>
+        <entry>
+          <value>4</value>
+        </entry>
+      </entries>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hstore.blockingStoreFiles</name>
+    <display-name>hstore blocking storefiles</display-name>
+    <value>10</value>
+    <description>
+    If more than this number of StoreFiles in any one Store
+    (one StoreFile is written per flush of MemStore) then updates are
+    blocked for this HRegion until a compaction is completed, or
+    until hbase.hstore.blockingWaitTime has been exceeded.
+    </description>
+    <value-attributes>
+      <type>int</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hfile.block.cache.size</name>
+    <value>0.40</value>
+    <description>Percentage of RegionServer memory to allocate to read buffers.</description>
+    <display-name>% of RegionServer Allocated to Read Buffers</display-name>
+    <value-attributes>
+      <type>float</type>
+      <minimum>0</minimum>
+      <maximum>0.8</maximum>
+      <increment-step>0.01</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <!-- Additional configuration specific to HBase security -->
+  <property>
+    <name>hbase.superuser</name>
+    <value>yarn</value>
+    <description>List of users or groups (comma-separated), who are allowed
+    full privileges, regardless of stored ACLs, across the cluster.
+    Only used when HBase security is enabled.
+    </description>
+    <depends-on>
+      <property>
+        <type>hbase-env</type>
+        <name>hbase_user</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.security.authentication</name>
+    <value>simple</value>
+    <description>
+      Select Simple or Kerberos authentication. Note: Kerberos must be set up before the Kerberos option will take effect.
+    </description>
+    <display-name>Enable Authentication</display-name>
+    <value-attributes>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <label>Simple</label>
+          <value>simple</value>
+        </entry>
+        <entry>
+          <label>Kerberos</label>
+          <value>kerberos</value>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.security.authorization</name>
+    <value>false</value>
+    <description> Set Authorization Method.</description>
+    <display-name>Enable Authorization</display-name>
+    <value-attributes>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Native</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>Off</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.coprocessor.region.classes</name>
+    <value/>
+    <description>A comma-separated list of Coprocessors that are loaded by
+      default on all tables. For any override coprocessor method, these classes
+      will be called in order. After implementing your own Coprocessor, just put
+      it in HBase's classpath and add the fully qualified class name here.
+      A coprocessor can also be loaded on demand by setting HTableDescriptor.
+    </description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.coprocessor.master.classes</name>
+    <value/>
+    <description>A comma-separated list of
+      org.apache.hadoop.hbase.coprocessor.MasterObserver coprocessors that are
+      loaded by default on the active HMaster process. For any implemented
+      coprocessor methods, the listed classes will be called in order. After
+      implementing your own MasterObserver, just put it in HBase's classpath
+      and add the fully qualified class name here.
+    </description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>hbase-site</type>
+        <name>hbase.security.authorization</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.zookeeper.useMulti</name>
+    <value>true</value>
+    <description>Instructs HBase to make use of ZooKeeper's multi-update functionality.
+    This allows certain ZooKeeper operations to complete more quickly and prevents some issues
+    with rare Replication failure scenarios (see the release note of HBASE-2611 for an example).&#xB7;
+    IMPORTANT: only set this to true if all ZooKeeper servers in the cluster are on version 3.4+
+    and will not be downgraded.  ZooKeeper versions before 3.4 do not support multi-update and will
+    not fail gracefully if multi-update is invoked (see ZOOKEEPER-1495).
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.defaults.for.version.skip</name>
+    <value>true</value>
+    <description>Disables version verification.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>dfs.domain.socket.path</name>
+    <value>/var/lib/hadoop-hdfs/dn_socket</value>
+    <description>Path to domain socket.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.rpc.protection</name>
+    <value>authentication</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <!-- Inherited from HBase in HDP 2.2 -->
+  <property>
+    <name>hbase.hregion.majorcompaction.jitter</name>
+    <value>0.50</value>
+    <description>A multiplier applied to hbase.hregion.majorcompaction to cause compaction to occur
+      a given amount of time either side of hbase.hregion.majorcompaction. The smaller the number,
+      the closer the compactions will happen to the hbase.hregion.majorcompaction
+      interval.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.bucketcache.ioengine</name>
+    <value/>
+    <description>Where to store the contents of the bucketcache. One of: onheap,
+      offheap, or file. If a file, set it to file:PATH_TO_FILE.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.bucketcache.size</name>
+    <value/>
+    <description>The size of the buckets for the bucketcache if you only use a single size.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.bucketcache.percentage.in.combinedcache</name>
+    <value/>
+    <description>Value to be set between 0.0 and 1.0</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.coprocessor.regionserver.classes</name>
+    <value/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>hbase-site</type>
+        <name>hbase.security.authorization</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.hstore.compaction.max</name>
+    <value>10</value>
+    <description>The maximum number of StoreFiles which will be selected for a single minor
+      compaction, regardless of the number of eligible StoreFiles. Effectively, the value of
+      hbase.hstore.compaction.max controls the length of time it takes a single compaction to
+      complete. Setting it larger means that more StoreFiles are included in a compaction. For most
+      cases, the default value is appropriate.
+    </description>
+    <display-name>Maximum Files for Compaction</display-name>
+    <value-attributes>
+      <type>int</type>
+      <entries>
+        <entry>
+          <value>8</value>
+        </entry>
+        <entry>
+          <value>9</value>
+        </entry>
+        <entry>
+          <value>10</value>
+        </entry>
+        <entry>
+          <value>11</value>
+        </entry>
+        <entry>
+          <value>12</value>
+        </entry>
+        <entry>
+          <value>13</value>
+        </entry>
+        <entry>
+          <value>14</value>
+        </entry>
+        <entry>
+          <value>15</value>
+        </entry>
+      </entries>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.regionserver.global.memstore.size</name>
+    <value>0.4</value>
+    <description>Percentage of RegionServer memory to allocate to write buffers.
+      Each column family within each region is allocated a smaller pool (the memstore) within this shared write pool.
+      If this buffer is full, updates are blocked and data is flushed from memstores until a global low watermark
+      (hbase.regionserver.global.memstore.size.lower.limit) is reached.
+    </description>
+    <display-name>% of RegionServer Allocated to Write Buffers</display-name>
+    <value-attributes>
+      <type>float</type>
+      <minimum>0</minimum>
+      <maximum>0.8</maximum>
+      <increment-step>0.01</increment-step>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <!-- Inherited from HBase in HDP 2.5 -->
+  <property>
+    <name>hbase.master.ui.readonly</name>
+    <value>false</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>zookeeper.recovery.retry</name>
+    <value>6</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <!-- Inherited from HBase in HDP 2.6 -->
+  <property>
+    <name>hbase.regionserver.executor.openregion.threads</name>
+    <value>20</value>
+    <description>The number of threads region server uses to open regions
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.master.namespace.init.timeout</name>
+    <value>2400000</value>
+    <description>The number of milliseconds master waits for hbase:namespace table to be initialized
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hbase.master.wait.on.regionservers.timeout</name>
+    <value>30000</value>
+    <description>The number of milliseconds master waits for region servers to report in
+    </description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-log4j.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-log4j.xml	(date 1719626239000)
@@ -0,0 +1,188 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="false">
+ <property>
+    <name>hbase_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>HBase Log: backup file size</display-name>
+    <value-attributes>
+        <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+ </property>
+ <property>
+      <name>hbase_log_maxbackupindex</name>
+      <value>20</value>
+      <description>The number of backup files</description>
+      <display-name>HBase Log: # of backup files</display-name>
+      <value-attributes>
+        <type>int</type>
+        <minimum>0</minimum>
+      </value-attributes>
+      <on-ambari-upgrade add="false"/>
+ </property>
+ <property>
+    <name>hbase_security_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of security backup file before the log is rotated</description>
+    <display-name>HBase Security Log: backup file size</display-name>
+    <value-attributes>
+        <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+ </property>
+ <property>
+      <name>hbase_security_log_maxbackupindex</name>
+      <value>20</value>
+      <description>The number of security backup files</description>
+      <display-name>HBase Security Log: # of backup files</display-name>
+      <value-attributes>
+        <type>int</type>
+        <minimum>0</minimum>
+      </value-attributes>
+      <on-ambari-upgrade add="false"/>
+ </property>
+  <property>
+    <name>content</name>
+    <display-name>hbase-log4j template</display-name>
+    <description>Custom log4j.properties</description>
+    <value>
+            # Licensed to the Apache Software Foundation (ASF) under one
+            # or more contributor license agreements.  See the NOTICE file
+            # distributed with this work for additional information
+            # regarding copyright ownership.  The ASF licenses this file
+            # to you under the Apache License, Version 2.0 (the
+            # "License"); you may not use this file except in compliance
+            # with the License.  You may obtain a copy of the License at
+            #
+            #     http://www.apache.org/licenses/LICENSE-2.0
+            #
+            # Unless required by applicable law or agreed to in writing, software
+            # distributed under the License is distributed on an "AS IS" BASIS,
+            # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+            # See the License for the specific language governing permissions and
+            # limitations under the License.
+
+
+            # Define some default values that can be overridden by system properties
+            hbase.root.logger=INFO,console
+            hbase.security.logger=INFO,console
+            hbase.log.dir=.
+            hbase.log.file=hbase.log
+
+            # Define the root logger to the system property "hbase.root.logger".
+            log4j.rootLogger=${hbase.root.logger}
+
+            # Logging Threshold
+            log4j.threshold=ALL
+
+            #
+            # Daily Rolling File Appender
+            #
+            log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
+            log4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}
+
+            # Rollver at midnight
+            log4j.appender.DRFA.DatePattern=.yyyy-MM-dd
+
+            # 30-day backup
+            #log4j.appender.DRFA.MaxBackupIndex=30
+            log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
+
+            # Pattern format: Date LogLevel LoggerName LogMessage
+            log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n
+
+            # Rolling File Appender properties
+            hbase.log.maxfilesize={{hbase_log_maxfilesize}}MB
+            hbase.log.maxbackupindex={{hbase_log_maxbackupindex}}
+
+            # Rolling File Appender
+            log4j.appender.RFA=org.apache.log4j.RollingFileAppender
+            log4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}
+
+            log4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}
+            log4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}
+
+            log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
+            log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n
+
+            #
+            # Security audit appender
+            #
+            hbase.security.log.file=SecurityAuth.audit
+            hbase.security.log.maxfilesize={{hbase_security_log_maxfilesize}}MB
+            hbase.security.log.maxbackupindex={{hbase_security_log_maxbackupindex}}
+            log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
+            log4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}
+            log4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}
+            log4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}
+            log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
+            log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
+            log4j.category.SecurityLogger=${hbase.security.logger}
+            log4j.additivity.SecurityLogger=false
+            #log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE
+
+            #
+            # Null Appender
+            #
+            log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender
+
+            #
+            # console
+            # Add "console" to rootlogger above if you want to use this
+            #
+            log4j.appender.console=org.apache.log4j.ConsoleAppender
+            log4j.appender.console.target=System.err
+            log4j.appender.console.layout=org.apache.log4j.PatternLayout
+            log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n
+
+            # Custom Logging levels
+
+            log4j.logger.org.apache.zookeeper=INFO
+            #log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
+            log4j.logger.org.apache.hadoop.hbase=INFO
+            # Make these two classes INFO-level. Make them DEBUG to see more zk debug.
+            log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO
+            log4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO
+            #log4j.logger.org.apache.hadoop.dfs=DEBUG
+            # Set this class to log INFO only otherwise its OTT
+            # Enable this to get detailed connection error/retry logging.
+            # log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE
+
+
+            # Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)
+            #log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG
+
+            # Uncomment the below if you want to remove logging of client region caching'
+            # and scan of .META. messages
+            # log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO
+            # log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO
+
+        </value>
+        <value-attributes>
+            <type>content</type>
+            <show-property-name>false</show-property-name>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-audit.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-audit.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-audit.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-audit.xml	(date 1719626239000)
@@ -0,0 +1,186 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.audit.is.enabled</name>
+    <value>true</value>
+    <description>Is Audit enabled?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.db</name>
+    <value>false</value>
+    <display-name>Audit to DB</display-name>
+    <description>Is Audit to DB enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.db</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.db.jdbc.url</name>
+    <value>{{audit_jdbc_url}}</value>
+    <description>Audit DB JDBC URL</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.db.user</name>
+    <value>{{xa_audit_db_user}}</value>
+    <description>Audit DB JDBC User</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.db.password</name>
+    <value>crypted</value>
+    <property-type>PASSWORD</property-type>
+    <description>Audit DB JDBC Password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.db.jdbc.driver</name>
+    <value>{{jdbc_driver}}</value>
+    <description>Audit DB JDBC Driver</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.credential.provider.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>Credential file store</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.db.batch.filespool.dir</name>
+    <value>/var/log/hadoop/yarn/audit/db/spool</value>
+    <description>/var/log/hadoop/yarn/audit/db/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs</name>
+    <value>true</value>
+    <display-name>Audit to HDFS</display-name>
+    <description>Is Audit to HDFS enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.dir</name>
+    <value>hdfs://NAMENODE_HOSTNAME:8020/ranger/audit</value>
+    <description>HDFS folder to write audit to, make sure the service user has requried permissions</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs.dir</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.batch.filespool.dir</name>
+    <value>/var/log/hadoop/yarn/audit/hdfs/spool</value>
+    <description>/var/log/hadoop/yarn/audit/hdfs/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr</name>
+    <value>false</value>
+    <display-name>Audit to SOLR</display-name>
+    <description>Is Solr audit enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.solr</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.urls</name>
+    <value/>
+    <description>Solr URL</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.urls</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.zookeepers</name>
+    <value>NONE</value>
+    <description>Solr Zookeeper string</description>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.zookeepers</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.batch.filespool.dir</name>
+    <value>/var/log/hadoop/yarn/audit/solr/spool</value>
+    <description>/var/log/hadoop/yarn/audit/solr/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.provider.summary.enabled</name>
+    <value>false</value>
+    <display-name>Audit provider summary enabled</display-name>
+    <description>Enable Summary audit?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.yarn.ambari.cluster.name</name>
+    <value>{{cluster_name}}</value>
+    <description>Capture cluster name from where Ranger yarn plugin is enabled.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-policy.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-policy.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-policy.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/yarn-hbase-policy.xml	(date 1719626239000)
@@ -0,0 +1,53 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+  <property>
+    <name>security.client.protocol.acl</name>
+    <value>*</value>
+    <description>ACL for HRegionInterface protocol implementations (ie.
+    clients talking to HRegionServers)
+    The ACL is a comma-separated list of user and group names. The user and
+    group list is separated by a blank. For e.g. "alice,bob users,wheel".
+    A special value of "*" means all users are allowed.</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+  <property>
+    <name>security.admin.protocol.acl</name>
+    <value>*</value>
+    <description>ACL for HMasterInterface protocol implementation (ie.
+    clients talking to HMaster for admin operations).
+    The ACL is a comma-separated list of user and group names. The user and
+    group list is separated by a blank. For e.g. "alice,bob users,wheel".
+    A special value of "*" means all users are allowed.</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+  <property>
+    <name>security.masterregion.protocol.acl</name>
+    <value>*</value>
+    <description>ACL for HMasterRegionInterface protocol implementations
+    (for HRegionServers communicating with HMaster)
+    The ACL is a comma-separated list of user and group names. The user and
+    group list is separated by a blank. For e.g. "alice,bob users,wheel".
+    A special value of "*" means all users are allowed.</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-security.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-security.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-security.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-security.xml	(date 1719626239000)
@@ -0,0 +1,75 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>ranger.plugin.yarn.service.name</name>
+    <value>{{repo_name}}</value>
+    <description>Name of the Ranger service containing policies for this Yarn instance</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.yarn.policy.source.impl</name>
+    <value>org.apache.ranger.admin.client.RangerAdminRESTClient</value>
+    <description>Class to retrieve policies from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.yarn.policy.rest.url</name>
+    <value>{{policymgr_mgr_url}}</value>
+    <description>URL to Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>policymgr_external_url</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>ranger.plugin.yarn.policy.rest.ssl.config.file</name>
+    <value>/etc/hadoop/conf/ranger-policymgr-ssl-yarn.xml</value>
+    <description>Path to the file containing SSL details to contact Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.yarn.policy.pollIntervalMs</name>
+    <value>30000</value>
+    <description>How often to poll for changes in policies?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.yarn.policy.cache.dir</name>
+    <value>/etc/ranger/{{repo_name}}/policycache</value>
+    <description>Directory where Ranger policies are cached after successful retrieval from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.add-yarn-authorization</name>
+    <display-name>Add YARN Authorization</display-name>
+    <value>true</value>
+    <description>YARN ACLs and RANGER ACLs will be taken into consideration if enabled, otherwise only RANGER ACLs will be taken into consideration</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-policymgr-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-policymgr-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-policymgr-ssl.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-policymgr-ssl.xml	(date 1719626239000)
@@ -0,0 +1,72 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <description>password for keystore</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>java truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file{{credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-plugin-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-plugin-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-plugin-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/configuration/ranger-yarn-plugin-properties.xml	(date 1719626239000)
@@ -0,0 +1,140 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+  <property>
+    <name>policy_user</name>
+    <value>ambari-qa</value>
+    <display-name>Policy user for YARN</display-name>
+    <description>This user must be system user and also present at Ranger admin portal</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.rpc.protection</name>
+    <value/>
+    <description>Used for repository creation on ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>common.name.for.certificate</name>
+    <value/>
+    <description>Common name for certificate, this value should match what is specified in repo within ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-yarn-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Enable Ranger for YARN</display-name>
+    <description>Enable ranger yarn plugin ?</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>ranger-yarn-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <value-attributes>
+      <type>boolean</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_USERNAME</name>
+    <value>yarn</value>
+    <display-name>Ranger repository config user</display-name>
+    <description>Used for repository creation on ranger admin</description>
+    <depends-on>
+      <property>
+        <type>ranger-yarn-plugin-properties</type>
+        <name>ranger-yarn-plugin-enabled</name>
+      </property>
+      <property>
+        <type>yarn-env</type>
+        <name>yarn_user</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_PASSWORD</name>
+    <value>yarn</value>
+    <display-name>Ranger repository config password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Used for repository creation on ranger admin</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_admin_username</name>
+    <value></value>
+    <display-name>External Ranger admin username</display-name>
+    <description>Add ranger default admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_admin_password</name>
+    <value></value>
+    <display-name>External Ranger admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_username</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin username</display-name>
+    <description>Add ranger default ambari admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_password</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default ambari admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/service_advisor.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/YARN/service_advisor.py	(date 1719626239000)
@@ -0,0 +1,2527 @@
+#!/usr/bin/env ambari-python-wrap
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+# Python imports
+import imp
+import os
+import traceback
+import inspect
+import socket
+import math
+import re
+from math import floor, ceil
+
+# Local imports
+
+
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../../../stacks/')
+PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+
+try:
+  if "BASE_SERVICE_ADVISOR" in os.environ:
+    PARENT_FILE = os.environ["BASE_SERVICE_ADVISOR"]
+  with open(PARENT_FILE, 'rb') as fp:
+    service_advisor = imp.load_module('service_advisor', fp, PARENT_FILE, ('.py', 'rb', imp.PY_SOURCE))
+except Exception as e:
+  traceback.print_exc()
+  print "Failed to load parent"
+
+
+class YARNServiceAdvisor(service_advisor.ServiceAdvisor):
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(YARNServiceAdvisor, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    self.initialize_logger("YARNServiceAdvisorf")
+
+    self.CLUSTER_CREATE_OPERATION = "ClusterCreate"
+
+    # Always call these methods
+    self.modifyMastersWithMultipleInstances()
+    self.modifyCardinalitiesDict()
+    self.modifyHeapSizeProperties()
+    self.modifyNotValuableComponents()
+    self.modifyComponentsNotPreferableOnServer()
+    self.modifyComponentLayoutSchemes()
+
+  def modifyMastersWithMultipleInstances(self):
+    """
+    Modify the set of masters with multiple instances.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyCardinalitiesDict(self):
+    """
+    Modify the dictionary of cardinalities.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyHeapSizeProperties(self):
+    """
+    Modify the dictionary of heap size properties.
+    Must be overridden in child class.
+    """
+    self.heap_size_properties = {}
+
+  def modifyNotValuableComponents(self):
+    """
+    Modify the set of components whose host assignment is based on other services.
+    Must be overridden in child class.
+    """
+    self.notValuableComponents.add("APP_TIMELINE_SERVER")
+
+  def modifyComponentsNotPreferableOnServer(self):
+    """
+    Modify the set of components that are not preferable on the server.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentLayoutSchemes(self):
+    """
+    Modify layout scheme dictionaries for components.
+    The scheme dictionary basically maps the number of hosts to
+    host index where component should exist.
+    Must be overridden in child class.
+    """
+    self.componentLayoutSchemes.update({
+      'APP_TIMELINE_SERVER': {31: 1, "else": 2},
+    })
+
+  def getServiceComponentLayoutValidations(self, services, hosts):
+    """
+    Get a list of errors.
+    Must be overridden in child class.
+    """
+    self.logger.info("Class: %s, Method: %s. Validating Service Component Layout." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    return self.getServiceComponentCardinalityValidations(services, hosts, "YARN")
+
+  def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overridden in child class.
+    """
+    self.logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    # Due to the existing stack inheritance, make it clear where each calculation came from.
+    recommender = YARNRecommender()
+
+    if 'forced-configurations' not in services:
+      services["forced-configurations"] = []
+
+    # YARN
+    recommender.recommendYARNConfigurationsFromHDP206(configurations, clusterData, services, hosts)
+    recommender.recommendYARNConfigurationsFromHDP22(configurations, clusterData, services, hosts)
+    recommender.recommendYARNConfigurationsFromHDP23(configurations, clusterData, services, hosts)
+    recommender.recommendYARNConfigurationsFromHDP25(configurations, clusterData, services, hosts)
+    recommender.recommendYARNConfigurationsFromHDP26(configurations, clusterData, services, hosts)
+    recommender.recommendYARNConfigurationsFromHDP30(configurations, clusterData, services, hosts)
+    recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationRecommendationsForSSO(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overridden in child class.
+    """
+    recommender = YARNRecommender()
+    recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+    """
+    Entry point.
+    Validate configurations for the service. Return a list of errors.
+    The code for this function should be the same for each Service Advisor.
+    """
+    self.logger.info("Class: %s, Method: %s. Validating Configurations." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    validator = YARNValidator()
+    # Calls the methods of the validator using arguments,
+    # method(siteProperties, siteRecommendations, configurations, services, hosts)
+    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
+
+  @staticmethod
+  def isKerberosEnabled(services, configurations):
+    """
+    Determines if security is enabled by testing the value of core-site/hadoop.security.authentication enabled.
+    If the property exists and is equal to "kerberos", then is it enabled; otherwise is it assumed to be
+    disabled.
+
+    :type services: dict
+    :param services: the dictionary containing the existing configuration values
+    :type configurations: dict
+    :param configurations: the dictionary containing the updated configuration values
+    :rtype: bool
+    :return: True or False
+    """
+    if configurations and "core-site" in configurations and \
+            "hadoop.security.authentication" in configurations["core-site"]["properties"]:
+      return configurations["core-site"]["properties"]["hadoop.security.authentication"].lower() == "kerberos"
+    elif services and "core-site" in services["configurations"] and \
+            "hadoop.security.authentication" in services["configurations"]["core-site"]["properties"]:
+      return services["configurations"]["core-site"]["properties"]["hadoop.security.authentication"].lower() == "kerberos"
+    else:
+      return False
+
+
+class MAPREDUCE2ServiceAdvisor(service_advisor.ServiceAdvisor):
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(MAPREDUCE2ServiceAdvisor, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    # Always call these methods
+    self.modifyMastersWithMultipleInstances()
+    self.modifyCardinalitiesDict()
+    self.modifyHeapSizeProperties()
+    self.modifyNotValuableComponents()
+    self.modifyComponentsNotPreferableOnServer()
+    self.modifyComponentLayoutSchemes()
+
+  def modifyMastersWithMultipleInstances(self):
+    """
+    Modify the set of masters with multiple instances.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyCardinalitiesDict(self):
+    """
+    Modify the dictionary of cardinalities.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyHeapSizeProperties(self):
+    """
+    Modify the dictionary of heap size properties.
+    Must be overridden in child class.
+    """
+    self.heap_size_properties = {}
+
+  def modifyNotValuableComponents(self):
+    """
+    Modify the set of components whose host assignment is based on other services.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentsNotPreferableOnServer(self):
+    """
+    Modify the set of components that are not preferable on the server.
+    Must be overridden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentLayoutSchemes(self):
+    """
+    Modify layout scheme dictionaries for components.
+    The scheme dictionary basically maps the number of hosts to
+    host index where component should exist.
+    Must be overridden in child class.
+    """
+    self.componentLayoutSchemes.update({
+      'HISTORYSERVER': {31: 1, "else": 2},
+    })
+
+  def getServiceComponentLayoutValidations(self, services, hosts):
+    """
+    Get a list of errors.
+    Must be overridden in child class.
+    """
+    self.logger.info("Class: %s, Method: %s. Validating Service Component Layout." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    return self.getServiceComponentCardinalityValidations(services, hosts, "MAPREDUCE2")
+
+  def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overridden in child class.
+    """
+    self.logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    # Due to the existing stack inheritance, make it clear where each calculation came from.
+    recommender = MAPREDUCE2Recommender()
+    recommender.recommendMapReduce2ConfigurationsFromHDP206(configurations, clusterData, services, hosts)
+    recommender.recommendMapReduce2ConfigurationsFromHDP22(configurations, clusterData, services, hosts)
+    recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationRecommendationsForSSO(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overridden in child class.
+    """
+    recommender = MAPREDUCE2Recommender()
+    recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+    """
+    Entry point.
+    Validate configurations for the service. Return a list of errors.
+    The code for this function should be the same for each Service Advisor.
+    """
+    self.logger.info("Class: %s, Method: %s. Validating Configurations." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    validator = YARNValidator()
+    # Calls the methods of the validator using arguments,
+    # method(siteProperties, siteRecommendations, configurations, services, hosts)
+    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
+
+
+class YARNRecommender(service_advisor.ServiceAdvisor):
+  """
+  YARN Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+  """
+
+  HIVE_INTERACTIVE_SITE = 'hive-interactive-site'
+  YARN_ROOT_DEFAULT_QUEUE_NAME = 'default'
+  CONFIG_VALUE_UINITIALIZED = 'SET_ON_FIRST_INVOCATION'
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(YARNRecommender, self)
+    self.as_super.__init__(*args, **kwargs)
+
+  def recommendYARNConfigurationsFromHDP206(self, configurations, clusterData, services, hosts):
+    """
+    Recommend configurations for this service based on HDP 2.0.6.
+    """
+    self.logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+                (self.__class__.__name__, inspect.stack()[0][3]))
+
+    putYarnProperty = self.putProperty(configurations, "yarn-site", services)
+    putYarnPropertyAttribute = self.putPropertyAttribute(configurations, "yarn-site")
+    putYarnEnvProperty = self.putProperty(configurations, "yarn-env", services)
+
+    self.calculateYarnAllocationSizes(configurations, services, hosts)
+
+    putYarnEnvProperty('min_user_id', self.get_system_min_uid())
+
+    yarn_mount_properties = [
+      ("yarn.nodemanager.local-dirs", "NODEMANAGER", "/hadoop/yarn/local", "multi"),
+      ("yarn.nodemanager.log-dirs", "NODEMANAGER", "/hadoop/yarn/log", "multi"),
+      ("yarn.timeline-service.leveldb-timeline-store.path", "APP_TIMELINE_SERVER", "/hadoop/yarn/timeline", "single"),
+      ("yarn.timeline-service.leveldb-state-store.path", "APP_TIMELINE_SERVER", "/hadoop/yarn/timeline", "single")
+    ]
+
+    self.updateMountProperties("yarn-site", yarn_mount_properties, configurations, services, hosts)
+
+    sc_queue_name = self.recommendYarnQueue(services, "yarn-env", "service_check.queue.name")
+    if sc_queue_name is not None:
+      putYarnEnvProperty("service_check.queue.name", sc_queue_name)
+
+    containerExecutorGroup = 'hadoop'
+    if 'cluster-env' in services['configurations'] and 'user_group' in services['configurations']['cluster-env']['properties']:
+      containerExecutorGroup = services['configurations']['cluster-env']['properties']['user_group']
+    putYarnProperty("yarn.nodemanager.linux-container-executor.group", containerExecutorGroup)
+
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    if "TEZ" in servicesList:
+      ambari_user = self.getAmbariUser(services)
+      ambariHostName = socket.getfqdn()
+      putYarnProperty("yarn.timeline-service.http-authentication.proxyuser.{0}.hosts".format(ambari_user), ambariHostName)
+      putYarnProperty("yarn.timeline-service.http-authentication.proxyuser.{0}.groups".format(ambari_user), "*")
+      old_ambari_user = self.getOldAmbariUser(services)
+      if old_ambari_user is not None:
+        putYarnPropertyAttribute("yarn.timeline-service.http-authentication.proxyuser.{0}.hosts".format(old_ambari_user), 'delete', 'true')
+        putYarnPropertyAttribute("yarn.timeline-service.http-authentication.proxyuser.{0}.groups".format(old_ambari_user), 'delete', 'true')
+
+  def recommendYARNConfigurationsFromHDP22(self, configurations, clusterData, services, hosts):
+    capacity_scheduler_properties, received_as_key_value_pair = self.getCapacitySchedulerProperties(services)
+    putYarnProperty = self.putProperty(configurations, "yarn-site", services)
+    putYarnEnvProperty = self.putProperty(configurations, "yarn-env", services)
+    putCapScheProperty = self.putProperty(configurations, "capacity-scheduler", services)
+    putYarnProperty('yarn.nodemanager.resource.cpu-vcores', clusterData['cpu'])
+    putYarnProperty('yarn.scheduler.minimum-allocation-vcores', 1)
+    putYarnProperty('yarn.scheduler.maximum-allocation-vcores', configurations["yarn-site"]["properties"]["yarn.nodemanager.resource.cpu-vcores"])
+    # Property Attributes
+    putYarnPropertyAttribute = self.putPropertyAttribute(configurations, "yarn-site")
+    nodeManagerHost = self.getHostWithComponent("YARN", "NODEMANAGER", services, hosts)
+    if (nodeManagerHost is not None):
+      cpuPercentageLimit = 80.0
+      if "yarn-site" in services["configurations"] and "yarn.nodemanager.resource.percentage-physical-cpu-limit" in services["configurations"]["yarn-site"]["properties"]:
+        cpuPercentageLimit = float(services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.resource.percentage-physical-cpu-limit"])
+      cpuLimit = max(1, int(floor(nodeManagerHost["Hosts"]["cpu_count"] * (cpuPercentageLimit / 100.0))))
+      putYarnProperty('yarn.nodemanager.resource.cpu-vcores', str(cpuLimit))
+      putYarnProperty('yarn.scheduler.maximum-allocation-vcores', configurations["yarn-site"]["properties"]["yarn.nodemanager.resource.cpu-vcores"])
+      putYarnPropertyAttribute('yarn.nodemanager.resource.memory-mb', 'maximum', int(nodeManagerHost["Hosts"]["total_mem"] / 1024)) # total_mem in kb
+      putYarnPropertyAttribute('yarn.nodemanager.resource.cpu-vcores', 'maximum', nodeManagerHost["Hosts"]["cpu_count"] * 2)
+      putYarnPropertyAttribute('yarn.scheduler.minimum-allocation-vcores', 'maximum', configurations["yarn-site"]["properties"]["yarn.nodemanager.resource.cpu-vcores"])
+      putYarnPropertyAttribute('yarn.scheduler.maximum-allocation-vcores', 'maximum', configurations["yarn-site"]["properties"]["yarn.nodemanager.resource.cpu-vcores"])
+
+      kerberos_authentication_enabled = YARNServiceAdvisor.isKerberosEnabled(services, configurations)
+      if kerberos_authentication_enabled:
+        putYarnProperty('yarn.nodemanager.container-executor.class',
+                        'org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor')
+
+      container_executor = self.getServicesSiteProperties(services, "container-executor")
+      gpu_module_enabled = container_executor is not None and "gpu_module_enabled" in container_executor and \
+                           container_executor["gpu_module_enabled"].lower() == "true"
+      yarn_env = self.getServicesSiteProperties(services, "yarn-env")
+      yarn_cgroups_enabled = yarn_env is not None and "yarn_cgroups_enabled" in yarn_env and \
+                             yarn_env["yarn_cgroups_enabled"].lower() == "true"
+
+      if gpu_module_enabled:
+        putYarnEnvProperty("yarn_cgroups_enabled", "true")
+        yarn_cgroups_enabled = "true"
+
+      if yarn_cgroups_enabled or self.has_multiple_resource_types(services):
+        # ResourceCalculator must switch to DominantResourceCalculator when more than resource types are involved
+        # If capacity-scheduler configs are received as one concatenated string, we deposit the changed configs back as
+        # one concatenated string.
+        updated_cap_sched_configs_str = ''
+        if not received_as_key_value_pair:
+          for prop, val in capacity_scheduler_properties.items():
+            if prop == 'yarn.scheduler.capacity.resource-calculator':
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                                              + prop + "=org.apache.hadoop.yarn.util.resource.DominantResourceCalculator\n"
+            elif prop.startswith('yarn.') and '.resource-calculator' not in prop:
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str + prop + "=" + val + "\n"
+          putCapScheProperty("capacity-scheduler", updated_cap_sched_configs_str)
+          self.logger.info("Updated 'capacity-scheduler' configs as one concatenated string.")
+        else:
+          # If capacity-scheduler configs are received as a  dictionary (generally 1st time), we deposit the changed
+          # values back as dictionary itself.
+          # Update existing configs in 'capacity-scheduler'.
+          for prop, val in capacity_scheduler_properties.items():
+            if prop == 'yarn.scheduler.capacity.resource-calculator':
+              putCapScheProperty(prop, 'org.apache.hadoop.yarn.util.resource.DominantResourceCalculator')
+            elif prop.startswith('yarn.') and '.resource-calculator' not in prop:
+              putCapScheProperty(prop, val)
+          self.logger.info("Updated 'capacity-scheduler' configs as a dictionary.")
+      else:
+        # only one resource involved in resource-type, reset resource-calculator to default
+        # If capacity-scheduler configs are received as one concatenated string, we deposit the changed configs back as
+        # one concatenated string.
+        updated_cap_sched_configs_str = ''
+        if not received_as_key_value_pair:
+          for prop, val in capacity_scheduler_properties.items():
+            if prop == 'yarn.scheduler.capacity.resource-calculator':
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                                              + prop + "=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator\n"
+            elif prop.startswith('yarn.') and '.resource-calculator' not in prop:
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str + prop + "=" + val + "\n"
+          putCapScheProperty("capacity-scheduler", updated_cap_sched_configs_str)
+          self.logger.info("Updated 'capacity-scheduler' configs as one concatenated string.")
+        else:
+          # If capacity-scheduler configs are received as a  dictionary (generally 1st time), we deposit the changed
+          # values back as dictionary itself.
+          # Update existing configs in 'capacity-scheduler'.
+          for prop, val in capacity_scheduler_properties.items():
+            if prop == 'yarn.scheduler.capacity.resource-calculator':
+              putCapScheProperty(prop, 'org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator')
+            elif prop.startswith('yarn.') and '.resource-calculator' not in prop:
+              putCapScheProperty(prop, val)
+          self.logger.info("Updated 'capacity-scheduler' configs as a dictionary.")
+
+      if yarn_cgroups_enabled:
+        putYarnProperty('yarn.nodemanager.container-executor.class', 'org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor')
+        putYarnProperty('yarn.nodemanager.linux-container-executor.group', 'hadoop')
+        putYarnProperty('yarn.nodemanager.linux-container-executor.resources-handler.class', 'org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler')
+        putYarnProperty('yarn.nodemanager.linux-container-executor.cgroups.hierarchy', '/yarn')
+        putYarnProperty('yarn.nodemanager.linux-container-executor.cgroups.mount', 'false')
+        putYarnProperty('yarn.nodemanager.linux-container-executor.cgroups.mount-path', '/sys/fs/cgroup')
+      else:
+        if not kerberos_authentication_enabled:
+          putYarnProperty('yarn.nodemanager.container-executor.class', 'org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor')
+        putYarnPropertyAttribute('yarn.nodemanager.linux-container-executor.resources-handler.class', 'delete', 'true')
+        putYarnPropertyAttribute('yarn.nodemanager.linux-container-executor.cgroups.hierarchy', 'delete', 'true')
+        putYarnPropertyAttribute('yarn.nodemanager.linux-container-executor.cgroups.mount', 'delete', 'true')
+        putYarnPropertyAttribute('yarn.nodemanager.linux-container-executor.cgroups.mount-path', 'delete', 'true')
+
+  def has_multiple_resource_types(self, services):
+    return "resource-types" in services["configurations"] \
+        and "yarn.resource-types" in services["configurations"]["resource-types"]["properties"] \
+        and len(services["configurations"]["resource-types"]["properties"]["yarn.resource-types"]) > 0
+
+  def recommendYARNConfigurationsFromHDP23(self, configurations, clusterData, services, hosts):
+    putYarnSiteProperty = self.putProperty(configurations, "yarn-site", services)
+    putYarnSitePropertyAttributes = self.putPropertyAttribute(configurations, "yarn-site")
+
+    if "ranger-env" in services["configurations"] and "ranger-yarn-plugin-properties" in services["configurations"] and \
+            "ranger-yarn-plugin-enabled" in services["configurations"]["ranger-env"]["properties"]:
+      putYarnRangerPluginProperty = self.putProperty(configurations, "ranger-yarn-plugin-properties", services)
+      rangerEnvYarnPluginProperty = services["configurations"]["ranger-env"]["properties"]["ranger-yarn-plugin-enabled"]
+      putYarnRangerPluginProperty("ranger-yarn-plugin-enabled", rangerEnvYarnPluginProperty)
+    rangerPluginEnabled = ''
+    if 'ranger-yarn-plugin-properties' in configurations and 'ranger-yarn-plugin-enabled' in configurations['ranger-yarn-plugin-properties']['properties']:
+      rangerPluginEnabled = configurations['ranger-yarn-plugin-properties']['properties']['ranger-yarn-plugin-enabled']
+    elif 'ranger-yarn-plugin-properties' in services['configurations'] and 'ranger-yarn-plugin-enabled' in services['configurations']['ranger-yarn-plugin-properties']['properties']:
+      rangerPluginEnabled = services['configurations']['ranger-yarn-plugin-properties']['properties']['ranger-yarn-plugin-enabled']
+    self.logger.info('========== rangerPluginEnable is {0}'.format(rangerPluginEnabled))
+    if rangerPluginEnabled.lower() == 'Yes'.lower():
+      putYarnSiteProperty('yarn.acl.enable','true')
+      putYarnSiteProperty('yarn.authorization-provider','org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer')
+    else:
+      putYarnSitePropertyAttributes('yarn.authorization-provider', 'delete', 'true')
+
+  def recommendYARNConfigurationsFromHDP25(self, configurations, clusterData, services, hosts):
+    hsi_env_poperties = self.getServicesSiteProperties(services, "hive-interactive-env")
+    cluster_env = self.getServicesSiteProperties(services, "cluster-env")
+
+    # Queue 'llap' creation/removal logic (Used by Hive Interactive server and associated LLAP)
+    if hsi_env_poperties and 'enable_hive_interactive' in hsi_env_poperties:
+      enable_hive_interactive = hsi_env_poperties['enable_hive_interactive']
+      LLAP_QUEUE_NAME = 'llap'
+
+      # Hive Server interactive is already added or getting added
+      if enable_hive_interactive == 'true':
+        self.updateLlapConfigs(configurations, services, hosts, LLAP_QUEUE_NAME)
+      else:  # When Hive Interactive Server is in 'off/removed' state.
+        self.checkAndStopLlapQueue(services, configurations, LLAP_QUEUE_NAME)
+
+  def recommendYARNConfigurationsFromHDP26(self, configurations, clusterData, services, hosts):
+    putYarnSiteProperty = self.putProperty(configurations, "yarn-site", services)
+    putYarnEnvProperty = self.putProperty(configurations, "yarn-env", services)
+    putResTypsProperty = self.putProperty(configurations, "resource-types", services)
+    putCapScheProperty = self.putProperty(configurations, "capacity-scheduler", services)
+    putCanExecProperty = self.putProperty(configurations, "container-executor", services)
+
+    if "yarn-site" in services["configurations"] and \
+                    "yarn.resourcemanager.scheduler.monitor.enable" in services["configurations"]["yarn-site"]["properties"]:
+      scheduler_monitor_enabled = services["configurations"]["yarn-site"]["properties"]["yarn.resourcemanager.scheduler.monitor.enable"]
+      if scheduler_monitor_enabled.lower() == 'true':
+        putYarnSiteProperty('yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled', "true")
+      else:
+        putYarnSiteProperty('yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled', "false")
+
+    # calculate total_preemption_per_round
+    total_preemption_per_round = str(round(max(float(1)/len(hosts['items']), 0.1),2))
+    putYarnSiteProperty('yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round', total_preemption_per_round)
+
+    if 'yarn-env' in services['configurations'] and 'yarn_user' in services['configurations']['yarn-env']['properties']:
+      yarn_user = services['configurations']['yarn-env']['properties']['yarn_user']
+    else:
+      yarn_user = 'yarn'
+    if 'ranger-yarn-plugin-properties' in configurations and 'ranger-yarn-plugin-enabled' in configurations['ranger-yarn-plugin-properties']['properties']:
+      ranger_yarn_plugin_enabled = (configurations['ranger-yarn-plugin-properties']['properties']['ranger-yarn-plugin-enabled'].lower() == 'Yes'.lower())
+    elif 'ranger-yarn-plugin-properties' in services['configurations'] and 'ranger-yarn-plugin-enabled' in services['configurations']['ranger-yarn-plugin-properties']['properties']:
+      ranger_yarn_plugin_enabled = (services['configurations']['ranger-yarn-plugin-properties']['properties']['ranger-yarn-plugin-enabled'].lower() == 'Yes'.lower())
+    else:
+      ranger_yarn_plugin_enabled = False
+
+     #yarn timeline service url depends on http policy and takes the host name of the yarn webapp.
+    if "yarn-site" in services["configurations"] and \
+         "yarn.http.policy" in services["configurations"]["yarn-site"]["properties"] and \
+          "yarn.log.server.web-service.url" in services["configurations"]["yarn-site"]["properties"]:
+      webservice_url = ''
+      if services["configurations"]["yarn-site"]["properties"]["yarn.http.policy"] == 'HTTP_ONLY':
+         if "yarn.timeline-service.webapp.address" in services["configurations"]["yarn-site"]["properties"]:
+           webapp_address = services["configurations"]["yarn-site"]["properties"]["yarn.timeline-service.webapp.address"]
+           webservice_url = "http://"+webapp_address+"/ws/v1/applicationhistory"
+         else:
+           self.logger.error("Required config yarn.timeline-service.webapp.address in yarn-site does not exist. Unable to set yarn.log.server.web-service.url")
+      else:
+         if "yarn.timeline-service.webapp.https.address" in services["configurations"]["yarn-site"]["properties"]:
+           webapp_address = services["configurations"]["yarn-site"]["properties"]["yarn.timeline-service.webapp.https.address"]
+           webservice_url = "https://"+webapp_address+"/ws/v1/applicationhistory"
+         else:
+           self.logger.error("Required config yarn.timeline-service.webapp.https.address in yarn-site does not exist. Unable to set yarn.log.server.web-service.url")
+      putYarnSiteProperty('yarn.log.server.web-service.url',webservice_url )
+
+    if ranger_yarn_plugin_enabled and 'ranger-yarn-plugin-properties' in services['configurations'] and 'REPOSITORY_CONFIG_USERNAME' in services['configurations']['ranger-yarn-plugin-properties']['properties']:
+      self.logger.info("Setting Yarn Repo user for Ranger.")
+      putRangerYarnPluginProperty = self.putProperty(configurations, "ranger-yarn-plugin-properties", services)
+      putRangerYarnPluginProperty("REPOSITORY_CONFIG_USERNAME",yarn_user)
+    else:
+      self.logger.info("Not setting Yarn Repo user for Ranger.")
+
+    yarn_timeline_app_cache_size = None
+    host_mem = None
+    for host in hosts["items"]:
+      host_mem = host["Hosts"]["total_mem"]
+      break
+    # Check if 'yarn.timeline-service.entity-group-fs-store.app-cache-size' in changed configs.
+    changed_configs_has_ats_cache_size = self.isConfigPropertiesChanged(
+      services, "yarn-site", ['yarn.timeline-service.entity-group-fs-store.app-cache-size'], False)
+    # Check if it's : 1. 'apptimelineserver_heapsize' changed detected in changed-configurations)
+    # OR 2. cluster initialization (services['changed-configurations'] should be empty in this case)
+    if changed_configs_has_ats_cache_size:
+      yarn_timeline_app_cache_size = self.read_yarn_apptimelineserver_cache_size(services)
+    elif 0 == len(services['changed-configurations']):
+      # Fetch host memory from 1st host, to be used for ATS config calculations below.
+      if host_mem is not None:
+        yarn_timeline_app_cache_size = self.calculate_yarn_apptimelineserver_cache_size(host_mem)
+        putYarnSiteProperty('yarn.timeline-service.entity-group-fs-store.app-cache-size', yarn_timeline_app_cache_size)
+        self.logger.info("Updated YARN config 'yarn.timeline-service.entity-group-fs-store.app-cache-size' as : {0}, "
+                         "using 'host_mem' = {1}".format(yarn_timeline_app_cache_size, host_mem))
+      else:
+        self.logger.info("Couldn't update YARN config 'yarn.timeline-service.entity-group-fs-store.app-cache-size' as "
+                         "'host_mem' read = {0}".format(host_mem))
+
+    if yarn_timeline_app_cache_size is not None:
+      # Calculation for 'ats_heapsize' is in MB.
+      ats_heapsize = self.calculate_yarn_apptimelineserver_heapsize(host_mem, yarn_timeline_app_cache_size)
+      putYarnEnvProperty('apptimelineserver_heapsize', ats_heapsize) # Value in MB
+      self.logger.info("Updated YARN config 'apptimelineserver_heapsize' as : {0}, ".format(ats_heapsize))
+
+    restyps_list = []
+    yn_cgrp_active = None
+    gpu_module_enabled = None
+    docker_module_enabled = None
+    allow_dev_list = []
+    allow_vol_drive_list = []
+    allow_romounts_list = []
+    cg_root_list = []
+    yn_hirch_list = []
+    lce_cgrp_hirch_list = []
+    lce_cgrp_mt = None
+    lce_cgrp_mtp_list = []
+    rp_gpu_agd_list = []
+    rp_gpu_dp_list = []
+    rp_gpu_dp_nv1_ep_list = []
+
+    if "resource-types" in services["configurations"] and \
+      "yarn.resource-types" in services["configurations"]["resource-types"]["properties"]:
+      yarn_restyps = services["configurations"]["resource-types"]["properties"]["yarn.resource-types"]
+      restyps_list = yarn_restyps.split(',') if len(yarn_restyps) > 1 else yarn_restyps.split()
+      self.logger.info("new what is yarn_restyps: '{0}'.".format(restyps_list))
+
+    if "yarn-env" in services["configurations"] and \
+                    "yarn_cgroups_enabled" in services["configurations"]["yarn-env"]["properties"]:
+      yn_cgrp_active = services["configurations"]["yarn-env"]["properties"]["yarn_cgroups_enabled"]
+
+    if "container-executor" in services["configurations"]:
+      if "gpu_module_enabled" in services["configurations"]["container-executor"]["properties"]:
+        gpu_module_enabled = services["configurations"]["container-executor"]["properties"]["gpu_module_enabled"]
+      if "docker_module_enabled" in services["configurations"]["container-executor"]["properties"]:
+        docker_module_enabled = services["configurations"]["container-executor"]["properties"]["docker_module_enabled"]
+
+      if "docker_allowed_devices" in services["configurations"]["container-executor"]["properties"]:
+        docker_allow_dev = services["configurations"]["container-executor"]["properties"]["docker_allowed_devices"]
+        allow_dev_list = docker_allow_dev.split(',') if len(docker_allow_dev) > 1 else docker_allow_dev.split()
+        self.logger.info("new what is docker_allowed_devices: '{0}'.".format(allow_dev_list))
+
+      if "docker_allowed_volume-drivers" in services["configurations"]["container-executor"]["properties"]:
+        docker_allow_vol_drive = services["configurations"]["container-executor"]["properties"]["docker_allowed_volume-drivers"]
+        allow_vol_drive_list = docker_allow_vol_drive.split(',') if len(docker_allow_vol_drive) > 1 else docker_allow_vol_drive.split()
+        self.logger.info("new what is docker_allowed_volume-drivers: '{0}'.".format(allow_vol_drive_list))
+
+      if "docker_allowed_ro-mounts" in services["configurations"]["container-executor"]["properties"]:
+        docker_allow_romounts = services["configurations"]["container-executor"]["properties"]["docker_allowed_ro-mounts"]
+        allow_romounts_list = docker_allow_romounts.split(',') if len(docker_allow_romounts) > 1 else docker_allow_romounts.split()
+        self.logger.info("new what is docker.allowed.ro-mounts: '{0}'.".format(allow_romounts_list))
+
+      if "cgroup_root" in services["configurations"]["container-executor"]["properties"]:
+        cg_root = services["configurations"]["container-executor"]["properties"]["cgroup_root"]
+        cg_root_list = cg_root.split(',') if len(cg_root) > 1 else cg_root.split()
+
+      if "yarn_hierarchy" in services["configurations"]["container-executor"]["properties"]:
+        yn_hirch = services["configurations"]["container-executor"]["properties"]["yarn_hierarchy"]
+        yn_hirch_list = yn_hirch.split(',') if len(yn_hirch) > 1 else yn_hirch.split()
+
+    if "yarn-site" in services["configurations"]:
+      if "yarn.nodemanager.linux-container-executor.cgroups.hierarchy" in services["configurations"]["yarn-site"]["properties"]:
+        lce_cgrp_hirch = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.linux-container-executor.cgroups.hierarchy"]
+        lce_cgrp_hirch_list = lce_cgrp_hirch.split(',') if len(lce_cgrp_hirch) > 1 else lce_cgrp_hirch.split()
+        self.logger.info("new what is yarn.nodemanager.linux-container-executor.cgroups.hierarchy: '{0}'.".format(lce_cgrp_hirch_list))
+
+      if "yarn.nodemanager.linux-container-executor.cgroups.mount" in services["configurations"]["yarn-site"]["properties"]:
+        lce_cgrp_mt = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.linux-container-executor.cgroups.mount"]
+
+      if "yarn.nodemanager.linux-container-executor.cgroups.mount-path" in services["configurations"]["yarn-site"]["properties"]:
+        lce_cgrp_mtp = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.linux-container-executor.cgroups.mount-path"]
+        lce_cgrp_mtp_list = lce_cgrp_mtp.split(',') if len(lce_cgrp_mtp) > 1 else lce_cgrp_mtp.split()
+        self.logger.info("new what is yarn.nodemanager.linux-container-executor.cgroups.mount-path: '{0}'.".format(lce_cgrp_mtp_list))
+
+      if "yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices" in services["configurations"]["yarn-site"]["properties"]:
+        rp_gpu_agd = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices"]
+        rp_gpu_agd_list = rp_gpu_agd.split(',') if len(rp_gpu_agd) > 1 else rp_gpu_agd.split()
+        self.logger.info("new what is yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices: '{0}'.".format(rp_gpu_agd_list))
+
+      if "yarn.nodemanager.resource-plugins.gpu.docker-plugin" in services["configurations"]["yarn-site"]["properties"]:
+        rp_gpu_dp = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.resource-plugins.gpu.docker-plugin"]
+        rp_gpu_dp_list = rp_gpu_dp.split(',') if len(rp_gpu_dp) > 1 else rp_gpu_dp.split()
+        self.logger.info("new what is yarn.nodemanager.resource-plugins.gpu.docker-plugin: '{0}'.".format(rp_gpu_dp_list))
+
+      if "yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint" in services["configurations"]["yarn-site"]["properties"]:
+        rp_gpu_dp_nv1_ep = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint"]
+        rp_gpu_dp_nv1_ep_list = rp_gpu_dp_nv1_ep.split(',') if len(rp_gpu_dp_nv1_ep) > 1 else rp_gpu_dp_nv1_ep.split()
+        self.logger.info("new what is yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint: '{0}'.".format(rp_gpu_dp_nv1_ep_list))
+
+    if gpu_module_enabled and gpu_module_enabled.lower() == 'true':
+      # put yarn.io/gpu if it is absent in resource-types.xml
+      if "resource-types" in services["configurations"] and \
+                      "yarn.resource-types" in services["configurations"]["resource-types"]["properties"]:
+        if 'yarn.io/gpu' in restyps_list:
+          self.logger.info("GPU types already in resource-types.")
+        else:
+          restyps_list.append("yarn.io/gpu")
+          yarn_restyps = ','.join(str(x) for x in restyps_list)
+          putResTypsProperty('yarn.resource-types', yarn_restyps)
+      # auto fill gpu related property values in yarn-site
+      yarn_restyps = ','.join(str(x) for x in restyps_list)
+      putYarnSiteProperty('yarn.nodemanager.resource-plugins', yarn_restyps)
+
+      if "auto" in rp_gpu_agd_list:
+        self.logger.info("allowed gpu devices already in resource-plugins.gpu.allowed-gpu-devices")
+      else:
+        rp_gpu_agd_list.append("auto")
+        rp_gpu_agd = ','.join(str(x) for x in rp_gpu_agd_list)
+        putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices', rp_gpu_agd)
+
+      # yarn_hierarchy should always have same value of yarn.nodemanager.linux-container-executor.cgroups.hierarchy
+      putCanExecProperty('yarn_hierarchy', '/yarn')
+      # cgroup_root should always have same value of yarn.nodemanager.linux-container-executor.cgroups.mount-path
+      putCanExecProperty('cgroup_root', '/sys/fs/cgroup')
+
+      if docker_module_enabled and docker_module_enabled.lower() == 'true':
+        if "nvidia-docker-v1" in rp_gpu_dp_list:
+          self.logger.info("nvidia gpu docker plugin already in resource-plugins.gpu.docker-plugin")
+        else:
+          rp_gpu_dp_list.append("nvidia-docker-v1")
+          rp_gpu_dp = ','.join(str(x) for x in rp_gpu_dp_list)
+          putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.docker-plugin', rp_gpu_dp)
+
+        if "http://localhost:3476/v1.0/docker/cli" in rp_gpu_dp_nv1_ep_list:
+          self.logger.info("nvidia gpu docker plugin endpoint already in resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint")
+        else:
+          rp_gpu_dp_nv1_ep_list.append("http://localhost:3476/v1.0/docker/cli")
+          rp_gpu_dp_nv1_ep = ','.join(str(x) for x in rp_gpu_dp_nv1_ep_list)
+          putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint', rp_gpu_dp_nv1_ep)
+
+        # add gpu related devices if it is absent in docker section
+        if "regex:^/dev/nvidia.*$" in allow_dev_list:
+          self.logger.info("gpu related devices already in docker.allowed.devices.")
+        else:
+          allow_dev_list.append("regex:^/dev/nvidia.*$")
+          docker_allow_dev = ','.join(str(x) for x in allow_dev_list)
+          putCanExecProperty('docker_allowed_devices', docker_allow_dev)
+
+        # add nvidia-docker if it is absent in docker allowed volume-drivers
+        if 'nvidia-docker' in allow_vol_drive_list:
+          self.logger.info("nvidia-docker already in docker_allowed_volume-drivers.")
+        else:
+          allow_vol_drive_list.append("nvidia-docker")
+          docker_allow_vol_drive = ','.join(str(x) for x in allow_vol_drive_list)
+          putCanExecProperty('docker_allowed_volume-drivers', docker_allow_vol_drive)
+
+        # add nvidia_driver_<version> if it is absent in docker allowed ro-mounts
+        if "regex:^nvidia_driver_.*$" in allow_romounts_list:
+          self.logger.info("nvidia_driver_<version> already in allow_romounts_list.")
+        else:
+          allow_romounts_list.append("regex:^nvidia_driver_.*$")
+          docker_allow_romounts = ','.join(str(x) for x in allow_romounts_list)
+          putCanExecProperty('docker_allowed_ro-mounts', docker_allow_romounts)
+
+      # gpu_module_enabled is true and docker_module_enabled is false
+      else:
+        # revert gpu docker related settings only
+        # yarn-site
+        if "nvidia-docker-v1" in rp_gpu_dp_list:
+          rp_gpu_dp_list.remove("nvidia-docker-v1")
+          rp_gpu_dp = ','.join(str(x) for x in rp_gpu_dp_list)
+          putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.docker-plugin', rp_gpu_dp)
+
+        if "http://localhost:3476/v1.0/docker/cli" in rp_gpu_dp_nv1_ep_list:
+          rp_gpu_dp_nv1_ep_list.remove("http://localhost:3476/v1.0/docker/cli")
+          rp_gpu_dp_nv1_ep = ','.join(str(x) for x in rp_gpu_dp_nv1_ep_list)
+          putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint', rp_gpu_dp_nv1_ep)
+
+        # container-executor
+        if "regex:^/dev/nvidia.*$" in allow_dev_list:
+          allow_dev_list.remove("regex:^/dev/nvidia.*$")
+          docker_allow_dev = ','.join(str(x) for x in allow_dev_list)
+          putCanExecProperty('docker_allowed_devices', docker_allow_dev)
+
+        if "nvidia-docker" in allow_vol_drive_list:
+          allow_vol_drive_list.remove("nvidia-docker")
+          docker_allow_vol_drive = ','.join(str(x) for x in allow_vol_drive_list)
+          putCanExecProperty('docker_allowed_volume-drivers', docker_allow_vol_drive)
+
+        if "regex:^nvidia_driver_.*$" in allow_romounts_list:
+          allow_romounts_list.remove("regex:^nvidia_driver_.*$")
+          docker_allow_romounts = ','.join(str(x) for x in allow_romounts_list)
+          putCanExecProperty('docker_allowed_ro-mounts', docker_allow_romounts)
+
+    # gpu_module_enabled is false, we will revert all gpu settings no matter
+    # docker_module_enabled is true or false.
+    else:
+      # auto revert gpu related property values when gpu is disabled
+      # revert gpu types from resource-types.xml
+      if 'yarn.io/gpu' in restyps_list:
+        restyps_list.remove("yarn.io/gpu")
+        yarn_restyps = ','.join(str(x) for x in restyps_list)
+        putResTypsProperty('yarn.resource-types', yarn_restyps)
+
+      if "auto" in rp_gpu_agd_list:
+        rp_gpu_agd_list.remove("auto")
+        rp_gpu_agd = ','.join(str(x) for x in rp_gpu_agd_list)
+        putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices', rp_gpu_agd)
+
+      if "nvidia-docker-v1" in rp_gpu_dp_list:
+        rp_gpu_dp_list.remove("nvidia-docker-v1")
+        rp_gpu_dp = ','.join(str(x) for x in rp_gpu_dp_list)
+        putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.docker-plugin', rp_gpu_dp)
+
+      if "http://localhost:3476/v1.0/docker/cli" in rp_gpu_dp_nv1_ep_list:
+        rp_gpu_dp_nv1_ep_list.remove("http://localhost:3476/v1.0/docker/cli")
+        rp_gpu_dp_nv1_ep = ','.join(str(x) for x in rp_gpu_dp_nv1_ep_list)
+        putYarnSiteProperty('yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-v1.endpoint', rp_gpu_dp_nv1_ep)
+
+      # yarn_hierarchy should always have same value of yarn.nodemanager.linux-container-executor.cgroups.hierarchy
+      putCanExecProperty('yarn_hierarchy', '')
+      # cgroup_root should always have same value of yarn.nodemanager.linux-container-executor.cgroups.mount-path
+      putCanExecProperty('cgroup_root', '')
+
+      # revert docker related settings from docker section
+      if "regex:^/dev/nvidia.*$" in allow_dev_list:
+        allow_dev_list.remove("regex:^/dev/nvidia.*$")
+        docker_allow_dev = ','.join(str(x) for x in allow_dev_list)
+        putCanExecProperty('docker_allowed_devices', docker_allow_dev)
+
+      if "nvidia-docker" in allow_vol_drive_list:
+        allow_vol_drive_list.remove("nvidia-docker")
+        docker_allow_vol_drive = ','.join(str(x) for x in allow_vol_drive_list)
+        putCanExecProperty('docker_allowed_volume-drivers', docker_allow_vol_drive)
+
+      if "regex:^nvidia_driver_.*$" in allow_romounts_list:
+        allow_romounts_list.remove("regex:^nvidia_driver_.*$")
+        docker_allow_romounts = ','.join(str(x) for x in allow_romounts_list)
+        putCanExecProperty('docker_allowed_ro-mounts', docker_allow_romounts)
+
+
+  def recommendYARNConfigurationsFromHDP30(self, configurations, clusterData, services, hosts):
+    putYarnSiteProperty = self.putProperty(configurations, "yarn-site", services)
+    putCapSchedProperty = self.putProperty(configurations, "capacity-scheduler", services)
+
+    self.update_timeline_reader_address(configurations, services, 'yarn.timeline-service.reader.webapp.address')
+    self.update_timeline_reader_address(configurations, services, 'yarn.timeline-service.reader.webapp.https.address')
+
+    hsi_env_poperties = self.getServicesSiteProperties(services, "hive-interactive-env")
+    if hsi_env_poperties and 'enable_hive_interactive' in hsi_env_poperties:
+      if hsi_env_poperties['enable_hive_interactive'] == 'true':
+        if 'forced-configurations' not in services:
+          services["forced-configurations"] = []
+        services["forced-configurations"].append({"type" : "yarn-site", "name" : "yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled"})
+        putYarnSiteProperty("yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled", "true")
+
+    hive_env_properties = self.getServicesSiteProperties(services, "hive-env")
+    cap_sched_properties, received_as_key_value_pair = self.getCapacitySchedulerProperties(services)
+    if hive_env_properties and "hive_user" in hive_env_properties and \
+       cap_sched_properties and "yarn.scheduler.capacity.root.acl_administer_queue" in cap_sched_properties:
+      hive_user = hive_env_properties["hive_user"]
+      acl_administer_queue = cap_sched_properties["yarn.scheduler.capacity.root.acl_administer_queue"]
+      acl_administer_queue_items = acl_administer_queue.split(",")
+      if not("*" in acl_administer_queue_items or hive_user in acl_administer_queue_items):
+        if not received_as_key_value_pair:
+          updated_cap_sched_configs_str = ""
+          for prop, val in cap_sched_properties.items():
+            if prop == "yarn.scheduler.capacity.root.acl_administer_queue":
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                + prop + "=" + acl_administer_queue + "," + hive_user + "\n"
+            elif prop:
+                updated_cap_sched_configs_str = updated_cap_sched_configs_str + prop + "=" + val + "\n"
+
+          putCapSchedProperty("capacity-scheduler", updated_cap_sched_configs_str)
+        else:
+          putCapSchedProperty("yarn.scheduler.capacity.root.acl_administer_queue", acl_administer_queue + "," + hive_user)
+
+    spark2_env_properties = self.getServicesSiteProperties(services, "spark2-env")
+    if spark2_env_properties and "spark_user" in spark2_env_properties and \
+            cap_sched_properties and "yarn.scheduler.capacity.root.acl_administer_queue" in cap_sched_properties:
+      sprak_user = spark2_env_properties["spark_user"]
+      acl_administer_queue = cap_sched_properties["yarn.scheduler.capacity.root.acl_administer_queue"]
+      acl_administer_queue_items = acl_administer_queue.split(",")
+      if not("*" in acl_administer_queue_items or sprak_user in acl_administer_queue_items):
+        if not received_as_key_value_pair:
+          updated_cap_sched_configs_str = ""
+          for prop, val in cap_sched_properties.items():
+            if prop == "yarn.scheduler.capacity.root.acl_administer_queue":
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                                              + prop + "=" + acl_administer_queue + "," + sprak_user + "\n"
+            elif prop:
+              updated_cap_sched_configs_str = updated_cap_sched_configs_str + prop + "=" + val + "\n"
+
+          putCapSchedProperty("capacity-scheduler", updated_cap_sched_configs_str)
+        else:
+          putCapSchedProperty("yarn.scheduler.capacity.root.acl_administer_queue", acl_administer_queue + "," + sprak_user)
+
+    # auto detect whether system service launch is required or not
+    # Set is_hbase_system_service_launch flag based on number of NM and cluster capacity.
+    # (1). if each NM capacity is greater than 10GB and cluster capacity greater than 50GB
+    if 'yarn-hbase-env' in services['configurations'] and 'is_hbase_system_service_launch' in services['configurations']['yarn-hbase-env']['properties']:
+        putYarnHBaseEnv = self.putProperty(configurations, "yarn-hbase-env", services)
+        node_manager_host_list = self.getHostsForComponent(services, "YARN", "NODEMANAGER")
+        node_manager_cnt = len(node_manager_host_list)
+        yarn_nm_mem_in_mb = self.get_yarn_nm_mem_in_mb(services, configurations)
+        total_cluster_capacity = node_manager_cnt * yarn_nm_mem_in_mb
+        if yarn_nm_mem_in_mb >= 10240 and total_cluster_capacity >= 51200:
+           putYarnHBaseEnv("is_hbase_system_service_launch", "true")
+        # Do not set to false in else
+
+  def recommendConfigurationsForSSO(self, configurations, clusterData, services, hosts):
+    ambari_configuration = self.get_ambari_configuration(services)
+    ambari_sso_details = ambari_configuration.get_ambari_sso_details() if ambari_configuration else None
+
+    if ambari_sso_details and ambari_sso_details.is_managing_services():
+      putYarnSiteProperty = self.putProperty(configurations, "yarn-site", services)
+
+      # If SSO should be enabled for this service
+      if ambari_sso_details.should_enable_sso('YARN'):
+        if(self.is_kerberos_enabled(configurations, services)):
+          putYarnSiteProperty('hadoop.http.authentication.type', "org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler")
+          putYarnSiteProperty('hadoop.http.authentication.authentication.provider.url', ambari_sso_details.get_sso_provider_url())
+          putYarnSiteProperty('hadoop.http.authentication.public.key.pem', ambari_sso_details.get_sso_provider_certificate(False, True))
+        else:
+          # Since Kerberos is not enabled, we can not enable SSO
+          self.logger.warn("Enabling SSO integration for Yarn requires Kerberos, Since Kerberos is not enabled, SSO integration is not being recommended.")
+          putYarnSiteProperty('hadoop.http.authentication.type', "simple")
+          pass
+
+      # If SSO should be disabled for this service
+      elif ambari_sso_details.should_disable_sso('YARN'):
+        if(self.is_kerberos_enabled(configurations, services)):
+          putYarnSiteProperty('hadoop.http.authentication.type', "kerberos")
+        else:
+          putYarnSiteProperty('hadoop.http.authentication.type', "simple")
+
+  def is_kerberos_enabled(self, configurations, services):
+    """
+    Tests if Yarn has Kerberos enabled by first checking the recommended changes and then the
+    existing settings.
+    :type configurations dict
+    :type services dict
+    :rtype bool
+    """
+    return self._is_kerberos_enabled(configurations) or \
+           (services and 'configurations' in services and self._is_kerberos_enabled(services['configurations']))
+
+  def _is_kerberos_enabled(self, config):
+    """
+    Detects if Yarn has Kerberos enabled given a dictionary of configurations.
+    :type config dict
+    :rtype bool
+    """
+    return config and \
+           (
+             (
+               "yarn-site" in config and
+               'hadoop.security.authentication' in config['yarn-site']["properties"] and
+               config['yarn-site']["properties"]['hadoop.security.authentication'] == 'kerberos'
+             ) or (
+               "core-site" in config and
+               'hadoop.security.authentication' in config['core-site']["properties"] and
+               config['core-site']["properties"]['hadoop.security.authentication'] == 'kerberos'
+             )
+           )
+
+  """
+  Calculate YARN config 'apptimelineserver_heapsize' in MB.
+  """
+  def calculate_yarn_apptimelineserver_heapsize(self, host_mem, yarn_timeline_app_cache_size):
+    ats_heapsize = None
+    if host_mem < 4096:
+      ats_heapsize = 1024
+    else:
+      ats_heapsize = long(min(math.floor(host_mem/2), long(yarn_timeline_app_cache_size) * 500 + 3072))
+    return ats_heapsize
+
+  """
+  Calculates for YARN config 'yarn.timeline-service.entity-group-fs-store.app-cache-size', based on YARN's NodeManager size.
+  """
+  def calculate_yarn_apptimelineserver_cache_size(self, host_mem):
+    yarn_timeline_app_cache_size = None
+    if host_mem < 4096:
+      yarn_timeline_app_cache_size = 3
+    elif host_mem >= 4096 and host_mem < 8192:
+      yarn_timeline_app_cache_size = 7
+    elif host_mem >= 8192:
+      yarn_timeline_app_cache_size = 10
+    self.logger.info("Calculated and returning 'yarn_timeline_app_cache_size' : {0}".format(yarn_timeline_app_cache_size))
+    return yarn_timeline_app_cache_size
+
+
+  """
+  Reads YARN config 'yarn.timeline-service.entity-group-fs-store.app-cache-size'.
+  """
+  def read_yarn_apptimelineserver_cache_size(self, services):
+    """
+    :type services dict
+    :rtype str
+    """
+    yarn_ats_app_cache_size = None
+    yarn_ats_app_cache_size_config = "yarn.timeline-service.entity-group-fs-store.app-cache-size"
+    yarn_site_in_services = self.getServicesSiteProperties(services, "yarn-site")
+
+    if yarn_site_in_services and yarn_ats_app_cache_size_config in yarn_site_in_services:
+      yarn_ats_app_cache_size = yarn_site_in_services[yarn_ats_app_cache_size_config]
+      self.logger.info("'yarn.scheduler.minimum-allocation-mb' read from services as : {0}".format(yarn_ats_app_cache_size))
+
+    if not yarn_ats_app_cache_size:
+      self.logger.error("'{0}' was not found in the services".format(yarn_ats_app_cache_size_config))
+
+    return yarn_ats_app_cache_size
+
+  def update_timeline_reader_address(self, configurations, services, property_name):
+    putYarnProperty = self.putProperty(configurations, 'yarn-site', services)
+    yarn_site = self.getServicesSiteProperties(services, 'yarn-site')
+    if yarn_site and property_name in yarn_site:
+      timeline_hosts = self.getHostsForComponent(services, 'YARN', 'TIMELINE_READER')
+      old_address = yarn_site[property_name]
+      if old_address and old_address.count(':') <= 1 and len(timeline_hosts) == 1:
+        new_address = re.sub('[^:]+', timeline_hosts[0], old_address, 1)
+        if old_address != new_address:
+          putYarnProperty(property_name, new_address)
+          self.logger.info('Updated YARN config {0} to {1}'.format(property_name, new_address))
+
+  #region LLAP
+  def updateLlapConfigs(self, configurations, services, hosts, llap_queue_name):
+    """
+    Entry point for updating Hive's 'LLAP app' configs namely :
+      (1). num_llap_nodes (2). hive.llap.daemon.yarn.container.mb
+      (3). hive.llap.daemon.num.executors (4). hive.llap.io.memory.size (5). llap_heap_size
+      (6). hive.server2.tez.sessions.per.default.queue, (7). tez.am.resource.memory.mb (8). hive.tez.container.size
+      (9). tez.runtime.io.sort.mb  (10). tez.runtime.unordered.output.buffer.size-mb (11). hive.llap.io.threadpool.size, and
+      (12). hive.llap.io.enabled.
+
+      The trigger point for updating LLAP configs (mentioned above) is change in values of any of the following:
+      (1). 'enable_hive_interactive' set to 'true' (2). 'num_llap_nodes' (3). 'hive.server2.tez.sessions.per.default.queue'
+      (4). Change in queue selection for config 'hive.llap.daemon.queue.name'.
+
+      If change in value for 'num_llap_nodes' or 'hive.server2.tez.sessions.per.default.queue' is detected, that config
+      value is not calulated, but read and use in calculation for dependent configs.
+
+      Note: All memory calculations are in MB, unless specified otherwise.
+    """
+    self.logger.info("DBG: Entered updateLlapConfigs")
+
+    # Determine if we entered here during cluster creation.
+    operation = self.getUserOperationContext(services, "operation")
+    is_cluster_create_opr = False
+    if operation == self.CLUSTER_CREATE_OPERATION:
+      is_cluster_create_opr = True
+    self.logger.info("Is cluster create operation ? = {0}".format(is_cluster_create_opr))
+
+    putHiveInteractiveSiteProperty = self.putProperty(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE, services)
+    putHiveInteractiveSitePropertyAttribute = self.putPropertyAttribute(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE)
+    putHiveInteractiveEnvProperty = self.putProperty(configurations, "hive-interactive-env", services)
+    putHiveInteractiveEnvPropertyAttribute = self.putPropertyAttribute(configurations, "hive-interactive-env")
+    putTezInteractiveSiteProperty = self.putProperty(configurations, "tez-interactive-site", services)
+    putTezInteractiveSitePropertyAttribute = self.putPropertyAttribute(configurations, "tez-interactive-site")
+    llap_daemon_selected_queue_name = None
+    selected_queue_is_ambari_managed_llap = None  # Queue named 'llap' at root level is Ambari managed.
+    llap_selected_queue_am_percent = None
+    DEFAULT_EXECUTOR_TO_AM_RATIO = 20
+    MIN_EXECUTOR_TO_AM_RATIO = 10
+    MAX_CONCURRENT_QUERIES = 32
+    MAX_CONCURRENT_QUERIES_SMALL_CLUSTERS = 4 # Concurrency for clusters with <10 executors
+    leafQueueNames = None
+    MB_TO_BYTES = 1048576
+    hsi_site = self.getServicesSiteProperties(services, YARNRecommender.HIVE_INTERACTIVE_SITE)
+    yarn_site = self.getServicesSiteProperties(services, "yarn-site")
+    min_memory_required = 0
+
+    # Update 'hive.llap.daemon.queue.name' prop combo entries
+    self.setLlapDaemonQueuePropAttributes(services, configurations)
+
+    if not services["changed-configurations"]:
+      read_llap_daemon_yarn_cont_mb = long(self.get_yarn_min_container_size(services, configurations))
+      putHiveInteractiveSiteProperty("hive.llap.daemon.yarn.container.mb", read_llap_daemon_yarn_cont_mb)
+      putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.yarn.container.mb', "minimum", read_llap_daemon_yarn_cont_mb)
+      putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.yarn.container.mb', "maximum", self.__get_min_hsi_mem(services, hosts) * 0.8)
+
+    if hsi_site and "hive.llap.daemon.queue.name" in hsi_site:
+      llap_daemon_selected_queue_name = hsi_site["hive.llap.daemon.queue.name"]
+
+    # Update Visibility of 'num_llap_nodes' YARN Service. Visible only if selected queue is Ambari created 'llap'.
+    capacity_scheduler_properties, received_as_key_value_pair = self.getCapacitySchedulerProperties(services)
+    if capacity_scheduler_properties:
+      # Get all leaf queues.
+      leafQueueNames = self.getAllYarnLeafQueues(capacity_scheduler_properties)
+      self.logger.info("YARN leaf Queues = {0}".format(leafQueueNames))
+      if len(leafQueueNames) == 0:
+        self.logger.error("Queue(s) couldn't be retrieved from capacity-scheduler.")
+        return
+
+      # Check if it's 1st invocation after enabling Hive Server Interactive (config: enable_hive_interactive).
+      changed_configs_has_enable_hive_int = self.isConfigPropertiesChanged(services, "hive-interactive-env", ['enable_hive_interactive'], False)
+      llap_named_queue_selected_in_curr_invocation = None
+      # Check if its : 1. 1st invocation from UI ('enable_hive_interactive' in changed-configurations)
+      # OR 2. 1st invocation from BP (services['changed-configurations'] should be empty in this case)
+      if (changed_configs_has_enable_hive_int or  0 == len(services['changed-configurations'])) \
+        and services['configurations']['hive-interactive-env']['properties']['enable_hive_interactive']:
+        if len(leafQueueNames) == 1 or (len(leafQueueNames) == 2 and llap_queue_name in leafQueueNames):
+          llap_named_queue_selected_in_curr_invocation = True
+          putHiveInteractiveSiteProperty('hive.llap.daemon.queue.name', llap_queue_name)
+          putHiveInteractiveSiteProperty('hive.server2.tez.default.queues', llap_queue_name)
+        else:
+          first_leaf_queue = list(leafQueueNames)[0]  # 1st invocation, pick the 1st leaf queue and set it as selected.
+          putHiveInteractiveSiteProperty('hive.llap.daemon.queue.name', first_leaf_queue)
+          putHiveInteractiveSiteProperty('hive.server2.tez.default.queues', first_leaf_queue)
+          llap_named_queue_selected_in_curr_invocation = False
+      self.logger.info("DBG: llap_named_queue_selected_in_curr_invocation = {0}".format(llap_named_queue_selected_in_curr_invocation))
+
+      if (len(leafQueueNames) >= 2 and (llap_daemon_selected_queue_name and llap_daemon_selected_queue_name == llap_queue_name) or
+            llap_named_queue_selected_in_curr_invocation) or \
+        (len(leafQueueNames) == 1 and llap_daemon_selected_queue_name == 'default' and llap_named_queue_selected_in_curr_invocation):
+        self.logger.info("DBG: Setting 'num_llap_nodes' config's  READ ONLY attribute as 'False'.")
+        putHiveInteractiveEnvPropertyAttribute("num_llap_nodes", "read_only", "false")
+        selected_queue_is_ambari_managed_llap = True
+        self.logger.info("DBG: Selected YARN queue for LLAP is : '{0}'. Current YARN queues : {1}. Setting 'Number of LLAP nodes' "
+                    "YARN Service visibility to 'True'".format(llap_queue_name, list(leafQueueNames)))
+      else:
+        self.logger.info("DBG: Setting 'num_llap_nodes' config's  READ ONLY attribute as 'True'.")
+        putHiveInteractiveEnvPropertyAttribute("num_llap_nodes", "read_only", "true")
+        self.logger.info("Selected YARN queue for LLAP is : '{0}'. Current YARN queues : {1}. Setting 'Number of LLAP nodes' "
+                    "visibility to 'False'.".format(llap_daemon_selected_queue_name, list(leafQueueNames)))
+        selected_queue_is_ambari_managed_llap = False
+
+      if not llap_named_queue_selected_in_curr_invocation:  # We would be creating the 'llap' queue later. Thus, cap-sched doesn't have
+        # state information pertaining to 'llap' queue.
+        # Check: State of the selected queue should not be STOPPED.
+        if llap_daemon_selected_queue_name:
+          llap_selected_queue_state = self.__getQueueStateFromCapacityScheduler(capacity_scheduler_properties, llap_daemon_selected_queue_name)
+          if llap_selected_queue_state is None or llap_selected_queue_state == "STOPPED":
+            self.logger.error("Selected LLAP app queue '{0}' current state is : '{1}'. Setting LLAP configs to default "
+                         "values.".format(llap_daemon_selected_queue_name, llap_selected_queue_state))
+            self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+            return
+        else:
+          self.logger.error("Retrieved LLAP app queue name is : '{0}'. Setting LLAP configs to default values."
+                       .format(llap_daemon_selected_queue_name))
+          self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+          return
+    else:
+      self.logger.error("Couldn't retrieve 'capacity-scheduler' properties while doing YARN queue adjustment for Hive Server Interactive."
+                   " Not calculating LLAP configs.")
+      return
+
+    changed_configs_in_hive_int_env = None
+    llap_concurrency_in_changed_configs = None
+    llap_daemon_queue_in_changed_configs = None
+    # Calculations are triggered only if there is change in any one of the following props :
+    # 'num_llap_nodes', 'enable_hive_interactive', 'hive.server2.tez.sessions.per.default.queue'
+    # or 'hive.llap.daemon.queue.name' has change in value selection.
+    # OR
+    # services['changed-configurations'] is empty implying that this is the Blueprint call. (1st invocation)
+    if 'changed-configurations' in services.keys():
+      config_names_to_be_checked = set(['num_llap_nodes', 'enable_hive_interactive'])
+      changed_configs_in_hive_int_env = self.isConfigPropertiesChanged(services, "hive-interactive-env", config_names_to_be_checked, False)
+
+      # Determine if there is change detected in "hive-interactive-site's" configs based on which we calculate llap configs.
+      llap_concurrency_in_changed_configs = self.isConfigPropertiesChanged(services, YARNRecommender.HIVE_INTERACTIVE_SITE, ['hive.server2.tez.sessions.per.default.queue'], False)
+      llap_daemon_queue_in_changed_configs = self.isConfigPropertiesChanged(services, YARNRecommender.HIVE_INTERACTIVE_SITE, ['hive.llap.daemon.queue.name'], False)
+
+    if not changed_configs_in_hive_int_env and not llap_concurrency_in_changed_configs and \
+        not llap_daemon_queue_in_changed_configs and services["changed-configurations"]:
+      self.logger.info("DBG: LLAP parameters not modified. Not adjusting LLAP configs.")
+      self.logger.info("DBG: Current 'changed-configuration' received is : {0}".format(services["changed-configurations"]))
+      return
+
+    self.logger.info("\nDBG: Performing LLAP config calculations ......")
+    node_manager_host_list = self.getHostsForComponent(services, "YARN", "NODEMANAGER")
+    node_manager_cnt = len(node_manager_host_list)
+    yarn_nm_mem_in_mb = self.get_yarn_nm_mem_in_mb(services, configurations)
+    total_cluster_capacity = node_manager_cnt * yarn_nm_mem_in_mb
+    self.logger.info("DBG: Calculated total_cluster_capacity : {0}, using following : node_manager_cnt : {1}, "
+                "yarn_nm_mem_in_mb : {2}".format(total_cluster_capacity, node_manager_cnt, yarn_nm_mem_in_mb))
+    yarn_min_container_size = float(self.get_yarn_min_container_size(services, configurations))
+    tez_am_container_size = self.calculate_tez_am_container_size(services, long(total_cluster_capacity), is_cluster_create_opr,
+                                                                 changed_configs_has_enable_hive_int)
+    normalized_tez_am_container_size = self._normalizeUp(tez_am_container_size, yarn_min_container_size)
+
+    if yarn_site and "yarn.nodemanager.resource.cpu-vcores" in yarn_site:
+      cpu_per_nm_host = float(yarn_site["yarn.nodemanager.resource.cpu-vcores"])
+    else:
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+    self.logger.info("DBG Calculated normalized_tez_am_container_size : {0}, using following : tez_am_container_size : {1}, "
+                "total_cluster_capacity : {2}".format(normalized_tez_am_container_size, tez_am_container_size,
+                                                      total_cluster_capacity))
+
+    # Calculate the available memory for LLAP app
+    yarn_nm_mem_in_mb_normalized = self._normalizeDown(yarn_nm_mem_in_mb, yarn_min_container_size)
+    mem_per_thread_for_llap = float(self.calculate_mem_per_thread_for_llap(services, yarn_nm_mem_in_mb_normalized, cpu_per_nm_host,
+                                                                           is_cluster_create_opr, changed_configs_has_enable_hive_int))
+    self.logger.info("DBG: Calculated mem_per_thread_for_llap : {0}, using following: yarn_nm_mem_in_mb_normalized : {1}, "
+                "cpu_per_nm_host : {2}".format(mem_per_thread_for_llap, yarn_nm_mem_in_mb_normalized, cpu_per_nm_host))
+
+
+    if mem_per_thread_for_llap is None:
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+
+    # Get calculated value for YARN Service AM container Size
+    yarn_service_am_container_size = self._normalizeUp(self.calculate_yarn_service_am_size(yarn_min_container_size),
+                                                 yarn_min_container_size)
+    self.logger.info("DBG: Calculated 'yarn_service_am_container_size' : {0}, using following: yarn_min_container_size : "
+                "{1}".format(yarn_service_am_container_size, yarn_min_container_size))
+
+    min_memory_required = normalized_tez_am_container_size + yarn_service_am_container_size + self._normalizeUp(mem_per_thread_for_llap, yarn_min_container_size)
+    self.logger.info("DBG: Calculated 'min_memory_required': {0} using following : yarn_service_am_container_size: {1}, "
+                "normalized_tez_am_container_size : {2}, mem_per_thread_for_llap : {3}, yarn_min_container_size : "
+                "{4}".format(min_memory_required, yarn_service_am_container_size, normalized_tez_am_container_size, mem_per_thread_for_llap, yarn_min_container_size))
+
+    min_nodes_required = int(ceil( min_memory_required / yarn_nm_mem_in_mb_normalized))
+    self.logger.info("DBG: Calculated 'min_node_required': {0}, using following : min_memory_required : {1}, yarn_nm_mem_in_mb_normalized "
+                ": {2}".format(min_nodes_required, min_memory_required, yarn_nm_mem_in_mb_normalized))
+    if min_nodes_required > node_manager_cnt:
+      self.logger.warning("ERROR: Not enough memory/nodes to run LLAP")
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+
+    mem_per_thread_for_llap = float(mem_per_thread_for_llap)
+
+    self.logger.info("DBG: selected_queue_is_ambari_managed_llap = {0}".format(selected_queue_is_ambari_managed_llap))
+    if not selected_queue_is_ambari_managed_llap:
+      llap_daemon_selected_queue_cap = self.__getSelectedQueueTotalCap(capacity_scheduler_properties, llap_daemon_selected_queue_name, total_cluster_capacity)
+
+      if llap_daemon_selected_queue_cap <= 0:
+        self.logger.warning("'{0}' queue capacity percentage retrieved = {1}. Expected > 0.".format(
+          llap_daemon_selected_queue_name, llap_daemon_selected_queue_cap))
+        self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+        return
+
+      total_llap_mem_normalized = self._normalizeDown(llap_daemon_selected_queue_cap, yarn_min_container_size)
+      self.logger.info("DBG: Calculated '{0}' queue available capacity : {1}, using following: llap_daemon_selected_queue_cap : {2}, "
+                  "yarn_min_container_size : {3}".format(llap_daemon_selected_queue_name, total_llap_mem_normalized,
+                                                         llap_daemon_selected_queue_cap, yarn_min_container_size))
+      '''Rounding up numNodes so that we run more daemons, and utilitze more CPUs. The rest of the calcaulations will take care of cutting this down if required'''
+      num_llap_nodes_requested = ceil(total_llap_mem_normalized / yarn_nm_mem_in_mb_normalized)
+      self.logger.info("DBG: Calculated 'num_llap_nodes_requested' : {0}, using following: total_llap_mem_normalized : {1}, "
+                  "yarn_nm_mem_in_mb_normalized : {2}".format(num_llap_nodes_requested, total_llap_mem_normalized, yarn_nm_mem_in_mb_normalized))
+      # Pouplate the 'num_llap_nodes_requested' in config 'num_llap_nodes', a read only config for non-Ambari managed queue case.
+      putHiveInteractiveEnvProperty('num_llap_nodes', num_llap_nodes_requested)
+      self.logger.info("Setting config 'num_llap_nodes' as : {0}".format(num_llap_nodes_requested))
+      queue_am_fraction_perc = float(self.__getQueueAmFractionFromCapacityScheduler(capacity_scheduler_properties, llap_daemon_selected_queue_name))
+      hive_tez_am_cap_available = queue_am_fraction_perc * total_llap_mem_normalized
+      self.logger.info("DBG: Calculated 'hive_tez_am_cap_available' : {0}, using following: queue_am_fraction_perc : {1}, "
+                  "total_llap_mem_normalized : {2}".format(hive_tez_am_cap_available, queue_am_fraction_perc, total_llap_mem_normalized))
+    else:  # Ambari managed 'llap' named queue at root level.
+      # Set 'num_llap_nodes_requested' for 1st invocation, as it gets passed as 1 otherwise, read from config.
+
+      # Check if its : 1. 1st invocation from UI ('enable_hive_interactive' in changed-configurations)
+      # OR 2. 1st invocation from BP (services['changed-configurations'] should be empty in this case)
+      if (changed_configs_has_enable_hive_int or  0 == len(services['changed-configurations'])) \
+        and services['configurations']['hive-interactive-env']['properties']['enable_hive_interactive']:
+        num_llap_nodes_requested = min_nodes_required
+      else:
+        num_llap_nodes_requested = self.get_num_llap_nodes(services, configurations) #Input
+      total_llap_mem = num_llap_nodes_requested * yarn_nm_mem_in_mb_normalized
+      self.logger.info("DBG: Calculated 'total_llap_mem' : {0}, using following: num_llap_nodes_requested : {1}, "
+                  "yarn_nm_mem_in_mb_normalized : {2}".format(total_llap_mem, num_llap_nodes_requested, yarn_nm_mem_in_mb_normalized))
+      total_llap_mem_normalized = float(self._normalizeDown(total_llap_mem, yarn_min_container_size))
+      self.logger.info("DBG: Calculated 'total_llap_mem_normalized' : {0}, using following: total_llap_mem : {1}, "
+                  "yarn_min_container_size : {2}".format(total_llap_mem_normalized, total_llap_mem, yarn_min_container_size))
+
+      # What percent is 'total_llap_mem' of 'total_cluster_capacity' ?
+      llap_named_queue_cap_fraction = ceil(total_llap_mem_normalized / total_cluster_capacity * 100)
+      self.logger.info("DBG: Calculated '{0}' queue capacity percent = {1}.".format(llap_queue_name, llap_named_queue_cap_fraction))
+
+      if llap_named_queue_cap_fraction > 100:
+        self.logger.warning("Calculated '{0}' queue size = {1}. Cannot be > 100.".format(llap_queue_name, llap_named_queue_cap_fraction))
+        self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+        return
+
+      # Adjust capacity scheduler for the 'llap' named queue.
+      self.checkAndManageLlapQueue(services, configurations, hosts, llap_queue_name, llap_named_queue_cap_fraction)
+      hive_tez_am_cap_available = total_llap_mem_normalized
+      self.logger.info("DBG: hive_tez_am_cap_available : {0}".format(hive_tez_am_cap_available))
+
+    # Common calculations now, irrespective of the queue selected.
+
+    llap_mem_for_tezAm_and_daemons = total_llap_mem_normalized - yarn_service_am_container_size
+    self.logger.info("DBG: Calculated 'llap_mem_for_tezAm_and_daemons' : {0}, using following : total_llap_mem_normalized : {1}, "
+                "yarn_service_am_container_size : {2}".format(llap_mem_for_tezAm_and_daemons, total_llap_mem_normalized, yarn_service_am_container_size))
+
+    if llap_mem_for_tezAm_and_daemons < 2 * yarn_min_container_size:
+      self.logger.warning("Not enough capacity available on the cluster to run LLAP")
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+
+    # Calculate llap concurrency (i.e. Number of Tez AM's)
+    max_executors_per_node = self.get_max_executors_per_node(yarn_nm_mem_in_mb_normalized, cpu_per_nm_host, mem_per_thread_for_llap)
+
+    # Read 'hive.server2.tez.sessions.per.default.queue' prop if it's in changed-configs, else calculate it.
+    if not llap_concurrency_in_changed_configs:
+      if max_executors_per_node <= 0:
+        self.logger.warning("Calculated 'max_executors_per_node' = {0}. Expected value >= 1.".format(max_executors_per_node))
+        self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+        return
+
+      self.logger.info("DBG: Calculated 'max_executors_per_node' : {0}, using following: yarn_nm_mem_in_mb_normalized : {1}, cpu_per_nm_host : {2}, "
+                  "mem_per_thread_for_llap: {3}".format(max_executors_per_node, yarn_nm_mem_in_mb_normalized, cpu_per_nm_host, mem_per_thread_for_llap))
+
+      # Default 1 AM for every 20 executor threads.
+      # The second part of the min calculates based on mem required for DEFAULT_EXECUTOR_TO_AM_RATIO executors + 1 AM,
+      # making use of total memory. However, it's possible that total memory will not be used - and the numExecutors is
+      # instead limited by #CPUs. Use maxPerNode to factor this in.
+      llap_concurreny_limit = min(floor(max_executors_per_node * num_llap_nodes_requested / DEFAULT_EXECUTOR_TO_AM_RATIO), MAX_CONCURRENT_QUERIES)
+      self.logger.info("DBG: Calculated 'llap_concurreny_limit' : {0}, using following : max_executors_per_node : {1}, num_llap_nodes_requested : {2}, DEFAULT_EXECUTOR_TO_AM_RATIO "
+                  ": {3}, MAX_CONCURRENT_QUERIES : {4}".format(llap_concurreny_limit, max_executors_per_node, num_llap_nodes_requested, DEFAULT_EXECUTOR_TO_AM_RATIO, MAX_CONCURRENT_QUERIES))
+      llap_concurrency = min(llap_concurreny_limit, floor(llap_mem_for_tezAm_and_daemons / (DEFAULT_EXECUTOR_TO_AM_RATIO * mem_per_thread_for_llap + normalized_tez_am_container_size)))
+      self.logger.info("DBG: Calculated 'llap_concurrency' : {0}, using following : llap_concurreny_limit : {1}, llap_mem_for_tezAm_and_daemons : "
+                  "{2}, DEFAULT_EXECUTOR_TO_AM_RATIO : {3}, mem_per_thread_for_llap : {4}, normalized_tez_am_container_size : "
+                  "{5}".format(llap_concurrency, llap_concurreny_limit, llap_mem_for_tezAm_and_daemons, DEFAULT_EXECUTOR_TO_AM_RATIO,
+                               mem_per_thread_for_llap, normalized_tez_am_container_size))
+      if llap_concurrency == 0:
+        llap_concurrency = 1
+        self.logger.info("DBG: Readjusted 'llap_concurrency' to : 1. Earlier calculated value : 0")
+
+      if llap_concurrency * normalized_tez_am_container_size > hive_tez_am_cap_available:
+        llap_concurrency = long(math.floor(hive_tez_am_cap_available / normalized_tez_am_container_size))
+        self.logger.info("DBG: Readjusted 'llap_concurrency' to : {0}, as llap_concurrency({1}) * normalized_tez_am_container_size({2}) > hive_tez_am_cap_available({3}))"
+                    .format(llap_concurrency, llap_concurrency, normalized_tez_am_container_size, hive_tez_am_cap_available))
+
+        if llap_concurrency <= 0:
+          self.logger.warning("DBG: Calculated 'LLAP Concurrent Queries' = {0}. Expected value >= 1.".format(llap_concurrency))
+          self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+          return
+        self.logger.info("DBG: Adjusted 'llap_concurrency' : {0}, using following: hive_tez_am_cap_available : {1}, normalized_tez_am_container_size: "
+                    "{2}".format(llap_concurrency, hive_tez_am_cap_available, normalized_tez_am_container_size))
+    else:
+      # Read current value
+      if 'hive.server2.tez.sessions.per.default.queue' in hsi_site:
+        llap_concurrency = long(hsi_site['hive.server2.tez.sessions.per.default.queue'])
+        if llap_concurrency <= 0:
+          self.logger.warning("'hive.server2.tez.sessions.per.default.queue' current value : {0}. Expected value : >= 1".format(llap_concurrency))
+          self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+          return
+        self.logger.info("DBG: Read 'llap_concurrency' : {0}".format(llap_concurrency ))
+      else:
+        llap_concurrency = 1
+        self.logger.warning("Couldn't retrieve Hive Server interactive's 'hive.server2.tez.sessions.per.default.queue' config. Setting default value 1.")
+        self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+        return
+
+    # Calculate 'Max LLAP Consurrency', irrespective of whether 'llap_concurrency' was read or calculated.
+    max_llap_concurreny_limit = min(floor(max_executors_per_node * num_llap_nodes_requested / MIN_EXECUTOR_TO_AM_RATIO), MAX_CONCURRENT_QUERIES)
+    self.logger.info("DBG: Calculated 'max_llap_concurreny_limit' : {0}, using following : max_executors_per_node : {1}, num_llap_nodes_requested "
+                ": {2}, MIN_EXECUTOR_TO_AM_RATIO : {3}, MAX_CONCURRENT_QUERIES : {4}".format(max_llap_concurreny_limit, max_executors_per_node,
+                                                                                             num_llap_nodes_requested, MIN_EXECUTOR_TO_AM_RATIO,
+                                                                                             MAX_CONCURRENT_QUERIES))
+
+    # Calculate value for 'num_llap_nodes', an across cluster config.
+    tez_am_memory_required = llap_concurrency * normalized_tez_am_container_size
+    self.logger.info("DBG: Calculated 'tez_am_memory_required' : {0}, using following : llap_concurrency : {1}, normalized_tez_am_container_size : "
+                "{2}".format(tez_am_memory_required, llap_concurrency, normalized_tez_am_container_size))
+    llap_mem_daemon_size = llap_mem_for_tezAm_and_daemons - tez_am_memory_required
+
+    if llap_mem_daemon_size < yarn_min_container_size:
+      self.logger.warning("Calculated 'LLAP Daemon Size = {0}'. Expected >= 'YARN Minimum Container Size' ({1})'".format(
+        llap_mem_daemon_size, yarn_min_container_size))
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+
+    if llap_mem_daemon_size < mem_per_thread_for_llap or llap_mem_daemon_size < yarn_min_container_size:
+      self.logger.warning("Not enough memory available for executors.")
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+    self.logger.info("DBG: Calculated 'llap_mem_daemon_size' : {0}, using following : llap_mem_for_tezAm_and_daemons : {1}, tez_am_memory_required : "
+                "{2}".format(llap_mem_daemon_size, llap_mem_for_tezAm_and_daemons, tez_am_memory_required))
+
+    llap_daemon_mem_per_node = self._normalizeDown(llap_mem_daemon_size / num_llap_nodes_requested, yarn_min_container_size)
+    # This value takes into account total cluster capacity, and may not have left enough capcaity on each node to launch an AM.
+    self.logger.info("DBG: Calculated 'llap_daemon_mem_per_node' : {0}, using following : llap_mem_daemon_size : {1}, num_llap_nodes_requested : {2}, "
+                "yarn_min_container_size: {3}".format(llap_daemon_mem_per_node, llap_mem_daemon_size, num_llap_nodes_requested, yarn_min_container_size))
+    if llap_daemon_mem_per_node == 0:
+      # Small cluster. No capacity left on a node after running AMs.
+      llap_daemon_mem_per_node = self._normalizeUp(mem_per_thread_for_llap, yarn_min_container_size)
+      num_llap_nodes = floor(llap_mem_daemon_size / llap_daemon_mem_per_node)
+      self.logger.info("DBG: 'llap_daemon_mem_per_node' : 0, adjusted 'llap_daemon_mem_per_node' : {0}, 'num_llap_nodes' : {1}, using following: llap_mem_daemon_size : {2}, "
+                  "mem_per_thread_for_llap : {3}".format(llap_daemon_mem_per_node, num_llap_nodes, llap_mem_daemon_size, mem_per_thread_for_llap))
+    elif llap_daemon_mem_per_node < mem_per_thread_for_llap:
+      # Previously computed value of memory per thread may be too high. Cut the number of nodes. (Alternately reduce memory per node)
+      llap_daemon_mem_per_node = mem_per_thread_for_llap
+      num_llap_nodes = floor(llap_mem_daemon_size / mem_per_thread_for_llap)
+      self.logger.info("DBG: 'llap_daemon_mem_per_node'({0}) < mem_per_thread_for_llap({1}), adjusted 'llap_daemon_mem_per_node' "
+                  ": {2}".format(llap_daemon_mem_per_node, mem_per_thread_for_llap, llap_daemon_mem_per_node))
+    else:
+      # All good. We have a proper value for memoryPerNode.
+      num_llap_nodes = num_llap_nodes_requested
+      self.logger.info("DBG: num_llap_nodes : {0}".format(num_llap_nodes))
+
+    # Make sure we have enough memory on each node to run AMs.
+    # If nodes vs nodes_requested is different - AM memory is already factored in.
+    # If llap_node_count < total_cluster_nodes - assuming AMs can run on a different node.
+    # Else factor in min_concurrency_per_node * tez_am_size, and yarn_service_am_container_size
+    # Also needs to factor in whether num_llap_nodes = cluster_node_count
+    min_mem_reserved_per_node = 0
+    if num_llap_nodes == num_llap_nodes_requested and num_llap_nodes == node_manager_cnt:
+      min_mem_reserved_per_node = max(normalized_tez_am_container_size, yarn_service_am_container_size)
+      tez_AMs_per_node = llap_concurrency / num_llap_nodes
+      tez_AMs_per_node_low = int(math.floor(tez_AMs_per_node))
+      tez_AMs_per_node_high = int(math.ceil(tez_AMs_per_node))
+      min_mem_reserved_per_node = int(max(tez_AMs_per_node_high * normalized_tez_am_container_size, tez_AMs_per_node_low * normalized_tez_am_container_size + yarn_service_am_container_size))
+      self.logger.info("DBG: Determined 'AM reservation per node': {0}, using following : concurrency: {1}, num_llap_nodes: {2}, AMsPerNode: {3}"
+                  .format(min_mem_reserved_per_node, llap_concurrency, num_llap_nodes,  tez_AMs_per_node))
+
+    max_single_node_mem_available_for_daemon = self._normalizeDown(yarn_nm_mem_in_mb_normalized - min_mem_reserved_per_node, yarn_min_container_size)
+    if max_single_node_mem_available_for_daemon <=0 or max_single_node_mem_available_for_daemon < mem_per_thread_for_llap:
+      self.logger.warning("Not enough capacity available per node for daemons after factoring in AM memory requirements. NM Mem: {0}, "
+                     "minAMMemPerNode: {1}, available: {2}".format(yarn_nm_mem_in_mb_normalized, min_mem_reserved_per_node, max_single_node_mem_available_for_daemon))
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+
+    llap_daemon_mem_per_node = min(max_single_node_mem_available_for_daemon, llap_daemon_mem_per_node)
+    self.logger.info("DBG: Determined final memPerDaemon: {0}, using following: concurrency: {1}, numNMNodes: {2}, numLlapNodes: {3} "
+                .format(llap_daemon_mem_per_node, llap_concurrency, node_manager_cnt, num_llap_nodes))
+
+    num_executors_per_node_max = self.get_max_executors_per_node(yarn_nm_mem_in_mb_normalized, cpu_per_nm_host, mem_per_thread_for_llap)
+    if num_executors_per_node_max < 1:
+      self.logger.warning("Calculated 'Max. Executors per Node' = {0}. Expected values >= 1.".format(num_executors_per_node_max))
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+    self.logger.info("DBG: Calculated 'num_executors_per_node_max' : {0}, using following : yarn_nm_mem_in_mb_normalized : {1}, cpu_per_nm_host : {2}, "
+                "mem_per_thread_for_llap: {3}".format(num_executors_per_node_max, yarn_nm_mem_in_mb_normalized, cpu_per_nm_host, mem_per_thread_for_llap))
+
+    # NumExecutorsPerNode is not necessarily max - since some capacity would have been reserved for AMs, if this value were based on mem.
+    num_executors_per_node = min(floor(llap_daemon_mem_per_node / mem_per_thread_for_llap), num_executors_per_node_max)
+    if num_executors_per_node <= 0:
+      self.logger.warning("Calculated 'Number of Executors Per Node' = {0}. Expected value >= 1".format(num_executors_per_node))
+      self.recommendDefaultLlapConfiguration(configurations, services, hosts)
+      return
+    self.logger.info("DBG: Calculated 'num_executors_per_node' : {0}, using following : llap_daemon_mem_per_node : {1}, num_executors_per_node_max : {2}, "
+                "mem_per_thread_for_llap: {3}".format(num_executors_per_node, llap_daemon_mem_per_node, num_executors_per_node_max, mem_per_thread_for_llap))
+
+    # Now figure out how much of the memory will be used by the executors, and how much will be used by the cache.
+    total_mem_for_executors_per_node = num_executors_per_node * mem_per_thread_for_llap
+    cache_mem_per_node = llap_daemon_mem_per_node - total_mem_for_executors_per_node
+    self.logger.info("DBG: Calculated 'Cache per node' : {0}, using following : llap_daemon_mem_per_node : {1}, total_mem_for_executors_per_node : {2}"
+            .format(cache_mem_per_node, llap_daemon_mem_per_node, total_mem_for_executors_per_node))
+
+    # 'hive_auto_convert_join_noconditionaltask_size' value is in bytes. Thus, multiplying it by 1048576.
+    hive_auto_convert_join_noconditionaltask_size = (long((0.8 * mem_per_thread_for_llap) / 3)) * MB_TO_BYTES
+
+    # Calculate value for prop 'llap_heap_size'
+    llap_xmx = max(total_mem_for_executors_per_node * 0.8, total_mem_for_executors_per_node - self.get_llap_headroom_space(services, configurations))
+    self.logger.info("DBG: Calculated llap_app_heap_size : {0}, using following : total_mem_for_executors : {1}".format(llap_xmx, total_mem_for_executors_per_node))
+
+    # Calculate 'hive_heapsize' for Hive2/HiveServer2 (HSI)
+    hive_server_interactive_heapsize = None
+    hive_server_interactive_hosts = self.getHostsWithComponent("HIVE", "HIVE_SERVER_INTERACTIVE", services, hosts)
+    if hive_server_interactive_hosts is None:
+      # If its None, read the base service YARN's NODEMANAGER node memory, as are host are considered homogenous.
+      hive_server_interactive_hosts = self.getHostsWithComponent("YARN", "NODEMANAGER", services, hosts)
+    if hive_server_interactive_hosts is not None and len(hive_server_interactive_hosts) > 0:
+      host_mem = long(hive_server_interactive_hosts[0]["Hosts"]["total_mem"])
+      hive_server_interactive_heapsize = min(max(2048.0, 400.0*llap_concurrency), 3.0/8 * host_mem)
+      self.logger.info("DBG: Calculated 'hive_server_interactive_heapsize' : {0}, using following : llap_concurrency : {1}, host_mem : "
+                  "{2}".format(hive_server_interactive_heapsize, llap_concurrency, host_mem))
+
+    # Done with calculations, updating calculated configs.
+    self.logger.info("DBG: Applying the calculated values....")
+
+    if is_cluster_create_opr or changed_configs_has_enable_hive_int:
+      normalized_tez_am_container_size = long(normalized_tez_am_container_size)
+      putTezInteractiveSiteProperty('tez.am.resource.memory.mb', normalized_tez_am_container_size)
+      self.logger.info("DBG: Setting 'tez.am.resource.memory.mb' config value as : {0}".format(normalized_tez_am_container_size))
+
+    if not llap_concurrency_in_changed_configs:
+      putHiveInteractiveSiteProperty('hive.server2.tez.sessions.per.default.queue', max(long(num_executors_per_node/16), 1))
+    putHiveInteractiveSitePropertyAttribute('hive.server2.tez.sessions.per.default.queue', "maximum", max(long(num_executors_per_node/4), 1))
+
+    num_llap_nodes = long(num_llap_nodes)
+    putHiveInteractiveEnvPropertyAttribute('num_llap_nodes', "minimum", min_nodes_required)
+    putHiveInteractiveEnvPropertyAttribute('num_llap_nodes', "maximum", node_manager_cnt)
+    #TODO A single value is not being set for numNodes in case of a custom queue. Also the attribute is set to non-visible, so the UI likely ends up using an old cached value
+    if (num_llap_nodes != num_llap_nodes_requested):
+      self.logger.info("DBG: User requested num_llap_nodes : {0}, but used/adjusted value for calculations is : {1}".format(num_llap_nodes_requested, num_llap_nodes))
+    else:
+      self.logger.info("DBG: Used num_llap_nodes for calculations : {0}".format(num_llap_nodes_requested))
+
+    # Safeguard for not adding "num_llap_nodes_for_llap_daemons" if it doesnt exist in hive-interactive-site.
+    # This can happen if we upgrade from Ambari 2.4 (with HDP 2.5) to Ambari 2.5, as this config is from 2.6 stack onwards only.
+    if "hive-interactive-env" in services["configurations"] and \
+        "num_llap_nodes_for_llap_daemons" in services["configurations"]["hive-interactive-env"]["properties"]:
+      putHiveInteractiveEnvProperty('num_llap_nodes_for_llap_daemons', num_llap_nodes)
+      self.logger.info("DBG: Setting config 'num_llap_nodes_for_llap_daemons' as : {0}".format(num_llap_nodes))
+
+    llap_container_size = long(llap_daemon_mem_per_node)
+    putHiveInteractiveSiteProperty('hive.llap.daemon.yarn.container.mb', llap_container_size)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.yarn.container.mb', "minimum", yarn_min_container_size)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.yarn.container.mb', "maximum", self.__get_min_hsi_mem(services, hosts) * 0.8)
+
+    # Set 'hive.tez.container.size' only if it is read as "SET_ON_FIRST_INVOCATION", implying initialization.
+    # Else, we don't (1). Override the previous calculated value or (2). User provided value.
+    if is_cluster_create_opr or changed_configs_has_enable_hive_int:
+      mem_per_thread_for_llap = long(mem_per_thread_for_llap)
+      putHiveInteractiveSiteProperty('hive.tez.container.size', mem_per_thread_for_llap)
+      self.logger.info("DBG: Setting 'hive.tez.container.size' config value as : {0}".format(mem_per_thread_for_llap))
+
+    tez_runtime_io_sort_mb = (long((0.8 * mem_per_thread_for_llap) / 3))
+    tez_runtime_unordered_output_max_per_buffer_size_bytes=1024*1024*max(min(tez_runtime_io_sort_mb, 256), 128)
+    putTezInteractiveSiteProperty('tez.runtime.io.sort.mb', tez_runtime_io_sort_mb)
+    if "tez-site" in services["configurations"] and "tez.runtime.sorter.class" in services["configurations"]["tez-site"]["properties"]:
+      if services["configurations"]["tez-site"]["properties"]["tez.runtime.sorter.class"] == "LEGACY":
+        putTezInteractiveSitePropertyAttribute("tez.runtime.io.sort.mb", "maximum", 1800)
+
+    putTezInteractiveSiteProperty('tez.runtime.unordered.output.buffer.size-mb', tez_runtime_io_sort_mb)
+    putHiveInteractiveSiteProperty('tez.runtime.unordered.output.max-per-buffer.size-bytes', tez_runtime_unordered_output_max_per_buffer_size_bytes)
+    putHiveInteractiveSiteProperty('hive.auto.convert.join.noconditionaltask.size', hive_auto_convert_join_noconditionaltask_size)
+
+    num_executors_per_node = long(num_executors_per_node)
+    self.logger.info("DBG: Putting num_executors_per_node as {0}".format(num_executors_per_node))
+    putHiveInteractiveSiteProperty('hive.llap.daemon.num.executors', num_executors_per_node)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.num.executors', "minimum", 1)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.num.executors', "maximum", long(num_executors_per_node_max))
+
+    # 'hive.llap.io.threadpool.size' config value is to be set same as value calculated for
+    # 'hive.llap.daemon.num.executors' at all times.
+    cache_mem_per_node = long(cache_mem_per_node)
+
+    putHiveInteractiveSiteProperty('hive.llap.io.threadpool.size', num_executors_per_node)
+    putHiveInteractiveSiteProperty('hive.llap.io.memory.size', cache_mem_per_node)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.io.memory.size', 'minimum', 0)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.io.memory.size', 'maximum', self.__get_min_hsi_mem(services, hosts) * 0.8)
+
+    if hive_server_interactive_heapsize is not None:
+      putHiveInteractiveEnvProperty("hive_heapsize", int(hive_server_interactive_heapsize))
+
+    ssd_cache_on = services["configurations"]["hive-interactive-site"]["properties"]["hive.llap.io.allocator.mmap"] == "true"
+    llap_io_enabled = 'true' if long(cache_mem_per_node) >= 1024 or ssd_cache_on else 'false'
+    services["forced-configurations"].append({"type" : "hive-interactive-site", "name" : "hive.llap.io.enabled"})
+    putHiveInteractiveSiteProperty('hive.llap.io.enabled', llap_io_enabled)
+
+    putHiveInteractiveEnvProperty('llap_heap_size', long(llap_xmx))
+    self.logger.info("DBG: Done putting all configs")
+
+  def recommendDefaultLlapConfiguration(self, configurations, services, hosts):
+    self.logger.info("DBG: Something likely went wrong. recommendDefaultLlapConfiguration")
+    putHiveInteractiveSiteProperty = self.putProperty(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE, services)
+    putHiveInteractiveSitePropertyAttribute = self.putPropertyAttribute(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE)
+
+    putHiveInteractiveEnvProperty = self.putProperty(configurations, "hive-interactive-env", services)
+    putHiveInteractiveEnvPropertyAttribute = self.putPropertyAttribute(configurations, "hive-interactive-env")
+
+    yarn_min_container_size = long(self.get_yarn_min_container_size(services, configurations))
+
+    node_manager_host_list = self.getHostsForComponent(services, "YARN", "NODEMANAGER")
+    node_manager_cnt = len(node_manager_host_list)
+
+    putHiveInteractiveSiteProperty('hive.server2.tez.sessions.per.default.queue', 1)
+    putHiveInteractiveSitePropertyAttribute('hive.server2.tez.sessions.per.default.queue', "maximum", 1)
+
+    min_hsi_mem = self.__get_min_hsi_mem(services, hosts)
+
+    # Safeguard for not adding "num_llap_nodes_for_llap_daemons" if it doesnt exist in hive-interactive-site.
+    # This can happen if we upgrade from Ambari 2.4 (with HDP 2.5) to Ambari 2.5, as this config is from 2.6 stack onwards only.
+    if "hive-interactive-env" in services["configurations"] and \
+        "num_llap_nodes_for_llap_daemons" in services["configurations"]["hive-interactive-env"]["properties"]:
+      putHiveInteractiveEnvProperty('num_llap_nodes_for_llap_daemons', 1)
+    putHiveInteractiveEnvProperty('num_llap_nodes', 1)
+    putHiveInteractiveEnvPropertyAttribute('num_llap_nodes', "minimum", 0)
+    putHiveInteractiveEnvPropertyAttribute('num_llap_nodes', "maximum", node_manager_cnt)
+    putHiveInteractiveSiteProperty('hive.llap.daemon.yarn.container.mb', yarn_min_container_size)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.yarn.container.mb', "minimum", yarn_min_container_size)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.yarn.container.mb', "maximum", min_hsi_mem * 0.8)
+    putHiveInteractiveSiteProperty('hive.llap.daemon.num.executors', 1)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.num.executors', "minimum", 0)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.daemon.num.executors', "maximum", 1)
+    putHiveInteractiveSiteProperty('hive.llap.io.threadpool.size', 2)
+    putHiveInteractiveEnvProperty('llap_heap_size', 8192)
+    putHiveInteractiveSiteProperty('hive.llap.io.memory.size', min(2048, yarn_min_container_size))
+    putHiveInteractiveSitePropertyAttribute('hive.llap.io.memory.size', 'minimum', 0)
+    putHiveInteractiveSitePropertyAttribute('hive.llap.io.memory.size', 'maximum', max(min_hsi_mem * 0.5, 2048))
+    
+    ssd_cache_on = services["configurations"]["hive-interactive-site"]["properties"]["hive.llap.io.allocator.mmap"] == "true"
+    if ssd_cache_on:
+      services["forced-configurations"].append({"type" : "hive-interactive-site", "name" : "hive.llap.io.enabled"})
+      putHiveInteractiveSiteProperty("hive.llap.io.enabled", "true")
+
+  def __get_min_hsi_mem(self, services, hosts):
+    hsiHosts = self.getHostsWithComponent("HIVE", "HIVE_SERVER_INTERACTIVE", services, hosts)
+    if not hsiHosts:
+      return 0
+    min_mem = hsiHosts[0]["Hosts"]["total_mem"] / 1024
+    for hsiHost in hsiHosts:
+      host_mem = hsiHost["Hosts"]["total_mem"] / 1024
+      min_mem = min(min_mem, host_mem)
+    
+    return min_mem
+
+  def get_num_llap_nodes(self, services, configurations):
+    """
+    Returns current value of number of LLAP nodes in cluster (num_llap_nodes)
+
+    :type services: dict
+    :type configurations: dict
+    :rtype int
+    """
+    hsi_env = self.getServicesSiteProperties(services, "hive-interactive-env")
+    hsi_env_properties = self.getSiteProperties(configurations, "hive-interactive-env")
+    num_llap_nodes = 0
+
+    # Check if 'num_llap_nodes' is modified in current ST invocation.
+    if hsi_env_properties and 'num_llap_nodes' in hsi_env_properties:
+      num_llap_nodes = hsi_env_properties['num_llap_nodes']
+    elif hsi_env and 'num_llap_nodes' in hsi_env:
+      num_llap_nodes = hsi_env['num_llap_nodes']
+    else:
+      self.logger.error("Couldn't retrieve Hive Server 'num_llap_nodes' config. Setting value to {0}".format(num_llap_nodes))
+
+    return float(num_llap_nodes)
+
+  def get_max_executors_per_node(self, nm_mem_per_node_normalized, nm_cpus_per_node, mem_per_thread):
+    # TODO: This potentially takes up the entire node leaving no space for AMs.
+    return min(floor(nm_mem_per_node_normalized / mem_per_thread), nm_cpus_per_node)
+
+  def calculate_mem_per_thread_for_llap(self, services, nm_mem_per_node_normalized, cpu_per_nm_host, is_cluster_create_opr=False,
+                                        enable_hive_interactive_1st_invocation=False):
+    """
+    Calculates 'mem_per_thread_for_llap' for 1st time initialization. Else returns 'hive.tez.container.size' read value.
+    """
+    hive_tez_container_size = self.get_hive_tez_container_size(services)
+
+    if is_cluster_create_opr or enable_hive_interactive_1st_invocation:
+      if nm_mem_per_node_normalized <= 1024:
+        calculated_hive_tez_container_size = min(512, nm_mem_per_node_normalized)
+      elif nm_mem_per_node_normalized <= 4096:
+        calculated_hive_tez_container_size = 1024
+      elif nm_mem_per_node_normalized <= 10240:
+        calculated_hive_tez_container_size = 2048
+      elif nm_mem_per_node_normalized <= 24576:
+        calculated_hive_tez_container_size = 3072
+      else:
+        calculated_hive_tez_container_size = 4096
+
+      self.logger.info("DBG: Calculated and returning 'hive_tez_container_size' : {0}".format(calculated_hive_tez_container_size))
+      return calculated_hive_tez_container_size
+    else:
+      self.logger.info("DBG: Returning 'hive_tez_container_size' : {0}".format(hive_tez_container_size))
+      return hive_tez_container_size
+
+  def get_hive_tez_container_size(self, services):
+    """
+    Gets HIVE Tez container size (hive.tez.container.size).
+    """
+    hive_container_size = None
+    hsi_site = self.getServicesSiteProperties(services, YARNRecommender.HIVE_INTERACTIVE_SITE)
+    if hsi_site and 'hive.tez.container.size' in hsi_site:
+      hive_container_size = hsi_site['hive.tez.container.size']
+
+    if not hive_container_size:
+      # This can happen (1). If config is missing in hive-interactive-site or (2). its an
+      # upgrade scenario from Ambari 2.4 to Ambari 2.5 with HDP 2.5 installed. Read it
+      # from hive-site.
+      #
+      # If Ambari 2.5 after upgrade from 2.4 is managing HDP 2.6 here, this config would have
+      # already been added in hive-interactive-site as part of HDP upgrade from 2.5 to 2.6,
+      # and we wont end up in this block to look up in hive-site.
+      hive_site = self.getServicesSiteProperties(services, "hive-site")
+      if hive_site and 'hive.tez.container.size' in hive_site:
+        hive_container_size = hive_site['hive.tez.container.size']
+
+    return hive_container_size
+
+  def get_llap_headroom_space(self, services, configurations):
+    """
+    Gets HIVE Server Interactive's 'llap_headroom_space' config. (Default value set to 6144 bytes).
+    """
+    llap_headroom_space = None
+    # Check if 'llap_headroom_space' is modified in current SA invocation.
+    if 'hive-interactive-env' in configurations and 'llap_headroom_space' in configurations['hive-interactive-env']['properties']:
+      hive_container_size = float(configurations['hive-interactive-env']['properties']['llap_headroom_space'])
+      self.logger.info("'llap_headroom_space' read from configurations as : {0}".format(llap_headroom_space))
+
+    if llap_headroom_space is None:
+      # Check if 'llap_headroom_space' is input in services array.
+      if 'llap_headroom_space' in services['configurations']['hive-interactive-env']['properties']:
+        llap_headroom_space = float(services['configurations']['hive-interactive-env']['properties']['llap_headroom_space'])
+        self.logger.info("'llap_headroom_space' read from services as : {0}".format(llap_headroom_space))
+    if not llap_headroom_space or llap_headroom_space < 1:
+      llap_headroom_space = 6144 # 6GB
+      self.logger.info("Couldn't read 'llap_headroom_space' from services or configurations. Returing default value : 6144 bytes")
+
+    return llap_headroom_space
+
+  def checkAndManageLlapQueue(self, services, configurations, hosts, llap_queue_name, llap_queue_cap_perc):
+    """
+    Checks and (1). Creates 'llap' queue if only 'default' queue exist at leaf level and is consuming 100% capacity OR
+               (2). Updates 'llap' queue capacity and state, if current selected queue is 'llap', and only 2 queues exist
+                    at root level : 'default' and 'llap'.
+    """
+    self.logger.info("Determining creation/adjustment of 'capacity-scheduler' for 'llap' queue.")
+    putHiveInteractiveEnvProperty = self.putProperty(configurations, "hive-interactive-env", services)
+    putHiveInteractiveSiteProperty = self.putProperty(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE, services)
+    putHiveInteractiveEnvPropertyAttribute = self.putPropertyAttribute(configurations, "hive-interactive-env")
+    putCapSchedProperty = self.putProperty(configurations, "capacity-scheduler", services)
+    leafQueueNames = None
+    hsi_site = self.getServicesSiteProperties(services, YARNRecommender.HIVE_INTERACTIVE_SITE)
+
+    capacity_scheduler_properties, received_as_key_value_pair = self.getCapacitySchedulerProperties(services)
+    if capacity_scheduler_properties:
+      leafQueueNames = self.getAllYarnLeafQueues(capacity_scheduler_properties)
+      cap_sched_config_keys = capacity_scheduler_properties.keys()
+
+      yarn_default_queue_capacity = -1
+      if 'yarn.scheduler.capacity.root.default.capacity' in cap_sched_config_keys:
+        yarn_default_queue_capacity = float(capacity_scheduler_properties.get('yarn.scheduler.capacity.root.default.capacity'))
+
+      # Get 'llap' queue state
+      currLlapQueueState = ''
+      if 'yarn.scheduler.capacity.root.'+llap_queue_name+'.state' in cap_sched_config_keys:
+        currLlapQueueState = capacity_scheduler_properties.get('yarn.scheduler.capacity.root.'+llap_queue_name+'.state')
+
+      # Get 'llap' queue capacity
+      currLlapQueueCap = -1
+      if 'yarn.scheduler.capacity.root.'+llap_queue_name+'.capacity' in cap_sched_config_keys:
+        currLlapQueueCap = int(float(capacity_scheduler_properties.get('yarn.scheduler.capacity.root.'+llap_queue_name+'.capacity')))
+
+      updated_cap_sched_configs_str = ''
+
+      enabled_hive_int_in_changed_configs = self.isConfigPropertiesChanged(services, "hive-interactive-env", ['enable_hive_interactive'], False)
+      """
+      We create OR "modify 'llap' queue 'state and/or capacity' " based on below conditions:
+       - if only 1 queue exists at root level and is 'default' queue and has 100% cap -> Create 'llap' queue,  OR
+       - if 2 queues exists at root level ('llap' and 'default') :
+           - Queue selected is 'llap' and state is STOPPED -> Modify 'llap' queue state to RUNNING, adjust capacity, OR
+           - Queue selected is 'llap', state is RUNNING and 'llap_queue_capacity' prop != 'llap' queue current running capacity ->
+              Modify 'llap' queue capacity to 'llap_queue_capacity'
+      """
+      if 'default' in leafQueueNames and \
+          ((len(leafQueueNames) == 1 and int(yarn_default_queue_capacity) == 100) or
+               ((len(leafQueueNames) == 2 and llap_queue_name in leafQueueNames) and
+                    ((currLlapQueueState == 'STOPPED' and enabled_hive_int_in_changed_configs) or (currLlapQueueState == 'RUNNING' and currLlapQueueCap != llap_queue_cap_perc)))):
+        adjusted_default_queue_cap = str(100 - llap_queue_cap_perc)
+
+        hive_user = '*'  # Open to all
+        if 'hive_user' in services['configurations']['hive-env']['properties']:
+          hive_user = services['configurations']['hive-env']['properties']['hive_user']
+
+        llap_queue_cap_perc = str(llap_queue_cap_perc)
+
+        # If capacity-scheduler configs are received as one concatenated string, we deposit the changed configs back as
+        # one concatenated string.
+        updated_cap_sched_configs_as_dict = False
+        if not received_as_key_value_pair:
+          for prop, val in capacity_scheduler_properties.items():
+            if llap_queue_name not in prop:
+              if prop == 'yarn.scheduler.capacity.root.queues':
+                updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                                                + prop + "=default,llap\n"
+              elif prop == 'yarn.scheduler.capacity.root.default.capacity':
+                updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                                                + prop + "=" + adjusted_default_queue_cap + "\n"
+              elif prop == 'yarn.scheduler.capacity.root.default.maximum-capacity':
+                updated_cap_sched_configs_str = updated_cap_sched_configs_str \
+                                                + prop + "=" + adjusted_default_queue_cap + "\n"
+              elif prop.startswith('yarn.') and '.llap.' not in prop:
+                updated_cap_sched_configs_str = updated_cap_sched_configs_str + prop + "=" + val + "\n"
+
+          # Now, append the 'llap' queue related properties
+          updated_cap_sched_configs_str += """yarn.scheduler.capacity.root.{0}.user-limit-factor=1
+yarn.scheduler.capacity.root.{0}.state=RUNNING
+yarn.scheduler.capacity.root.{0}.ordering-policy=fifo
+yarn.scheduler.capacity.root.{0}.minimum-user-limit-percent=100
+yarn.scheduler.capacity.root.{0}.maximum-capacity={1}
+yarn.scheduler.capacity.root.{0}.capacity={1}
+yarn.scheduler.capacity.root.{0}.acl_submit_applications={2}
+yarn.scheduler.capacity.root.{0}.acl_administer_queue={2}
+yarn.scheduler.capacity.root.{0}.maximum-am-resource-percent=1""".format(llap_queue_name, llap_queue_cap_perc, hive_user)
+
+          putCapSchedProperty("capacity-scheduler", updated_cap_sched_configs_str)
+          self.logger.info("Updated 'capacity-scheduler' configs as one concatenated string.")
+        else:
+          # If capacity-scheduler configs are received as a  dictionary (generally 1st time), we deposit the changed
+          # values back as dictionary itself.
+          # Update existing configs in 'capacity-scheduler'.
+          for prop, val in capacity_scheduler_properties.items():
+            if llap_queue_name not in prop:
+              if prop == 'yarn.scheduler.capacity.root.queues':
+                putCapSchedProperty(prop, 'default,llap')
+              elif prop == 'yarn.scheduler.capacity.root.default.capacity':
+                putCapSchedProperty(prop, adjusted_default_queue_cap)
+              elif prop == 'yarn.scheduler.capacity.root.default.maximum-capacity':
+                putCapSchedProperty(prop, adjusted_default_queue_cap)
+              elif prop.startswith('yarn.') and '.llap.' not in prop:
+                putCapSchedProperty(prop, val)
+
+          # Add new 'llap' queue related configs.
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".user-limit-factor", "1")
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".state", "RUNNING")
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".ordering-policy", "fifo")
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".minimum-user-limit-percent", "100")
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".maximum-capacity", llap_queue_cap_perc)
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".capacity", llap_queue_cap_perc)
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".acl_submit_applications", hive_user)
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".acl_administer_queue", hive_user)
+          putCapSchedProperty("yarn.scheduler.capacity.root." + llap_queue_name + ".maximum-am-resource-percent", "1")
+
+          self.logger.info("Updated 'capacity-scheduler' configs as a dictionary.")
+          updated_cap_sched_configs_as_dict = True
+
+        if updated_cap_sched_configs_str or updated_cap_sched_configs_as_dict:
+          if len(leafQueueNames) == 1: # 'llap' queue didn't exist before
+            self.logger.info("Created YARN Queue : '{0}' with capacity : {1}%. Adjusted 'default' queue capacity to : {2}%" \
+                        .format(llap_queue_name, llap_queue_cap_perc, adjusted_default_queue_cap))
+          else: # Queue existed, only adjustments done.
+            self.logger.info("Adjusted YARN Queue : '{0}'. Current capacity : {1}%. State: RUNNING.".format(llap_queue_name, llap_queue_cap_perc))
+            self.logger.info("Adjusted 'default' queue capacity to : {0}%".format(adjusted_default_queue_cap))
+
+          # Update Hive 'hive.llap.daemon.queue.name' prop to use 'llap' queue.
+          putHiveInteractiveSiteProperty('hive.llap.daemon.queue.name', llap_queue_name)
+          putHiveInteractiveSiteProperty('hive.server2.tez.default.queues', llap_queue_name)
+          # Update 'hive.llap.daemon.queue.name' prop combo entries and llap capacity YARN Service visibility.
+          self.setLlapDaemonQueuePropAttributes(services, configurations)
+      else:
+        self.logger.debug("Not creating/adjusting {0} queue. Current YARN queues : {1}".format(llap_queue_name, list(leafQueueNames)))
+    else:
+      self.logger.error("Couldn't retrieve 'capacity-scheduler' properties while doing YARN queue adjustment for Hive Server Interactive.")
+
+  def checkAndStopLlapQueue(self, services, configurations, llap_queue_name):
+    """
+    Checks and sees (1). If only two leaf queues exist at root level, namely: 'default' and 'llap',
+                and (2). 'llap' is in RUNNING state.
+
+    If yes, performs the following actions:   (1). 'llap' queue state set to STOPPED,
+                                              (2). 'llap' queue capacity set to 0 %,
+                                              (3). 'default' queue capacity set to 100 %
+    """
+    putCapSchedProperty = self.putProperty(configurations, "capacity-scheduler", services)
+    putHiveInteractiveSiteProperty = self.putProperty(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE, services)
+    capacity_scheduler_properties, received_as_key_value_pair = self.getCapacitySchedulerProperties(services)
+    updated_default_queue_configs = ''
+    updated_llap_queue_configs = ''
+    if capacity_scheduler_properties:
+      # Get all leaf queues.
+      leafQueueNames = self.getAllYarnLeafQueues(capacity_scheduler_properties)
+
+      if len(leafQueueNames) == 2 and llap_queue_name in leafQueueNames and 'default' in leafQueueNames:
+        # Get 'llap' queue state
+        currLlapQueueState = 'STOPPED'
+        if 'yarn.scheduler.capacity.root.'+llap_queue_name+'.state' in capacity_scheduler_properties.keys():
+          currLlapQueueState = capacity_scheduler_properties.get('yarn.scheduler.capacity.root.'+llap_queue_name+'.state')
+        else:
+          self.logger.error("{0} queue 'state' property not present in capacity scheduler. Skipping adjusting queues.".format(llap_queue_name))
+          return
+        if currLlapQueueState == 'RUNNING':
+          DEFAULT_MAX_CAPACITY = '100'
+          for prop, val in capacity_scheduler_properties.items():
+            # Update 'default' related configs in 'updated_default_queue_configs'
+            if llap_queue_name not in prop:
+              if prop == 'yarn.scheduler.capacity.root.default.capacity':
+                # Set 'default' capacity back to maximum val
+                updated_default_queue_configs = updated_default_queue_configs \
+                                                + prop + "="+DEFAULT_MAX_CAPACITY + "\n"
+              elif prop == 'yarn.scheduler.capacity.root.default.maximum-capacity':
+                # Set 'default' max. capacity back to maximum val
+                updated_default_queue_configs = updated_default_queue_configs \
+                                                + prop + "="+DEFAULT_MAX_CAPACITY + "\n"
+              elif prop.startswith('yarn.'):
+                updated_default_queue_configs = updated_default_queue_configs + prop + "=" + val + "\n"
+            else: # Update 'llap' related configs in 'updated_llap_queue_configs'
+              if prop == 'yarn.scheduler.capacity.root.'+llap_queue_name+'.state':
+                updated_llap_queue_configs = updated_llap_queue_configs \
+                                             + prop + "=STOPPED\n"
+              elif prop == 'yarn.scheduler.capacity.root.'+llap_queue_name+'.capacity':
+                updated_llap_queue_configs = updated_llap_queue_configs \
+                                             + prop + "=0\n"
+              elif prop == 'yarn.scheduler.capacity.root.'+llap_queue_name+'.maximum-capacity':
+                updated_llap_queue_configs = updated_llap_queue_configs \
+                                             + prop + "=0\n"
+              elif prop.startswith('yarn.'):
+                updated_llap_queue_configs = updated_llap_queue_configs + prop + "=" + val + "\n"
+        else:
+          self.logger.debug("{0} queue state is : {1}. Skipping adjusting queues.".format(llap_queue_name, currLlapQueueState))
+          return
+
+        if updated_default_queue_configs and updated_llap_queue_configs:
+          putCapSchedProperty("capacity-scheduler", updated_default_queue_configs+updated_llap_queue_configs)
+          self.logger.info("Changed YARN '{0}' queue state to 'STOPPED', and capacity to 0%. Adjusted 'default' queue capacity to : {1}%" \
+                      .format(llap_queue_name, DEFAULT_MAX_CAPACITY))
+
+          # Update Hive 'hive.llap.daemon.queue.name' prop to use 'default' queue.
+          putHiveInteractiveSiteProperty('hive.llap.daemon.queue.name', YARNRecommender.YARN_ROOT_DEFAULT_QUEUE_NAME)
+          putHiveInteractiveSiteProperty('hive.server2.tez.default.queues', YARNRecommender.YARN_ROOT_DEFAULT_QUEUE_NAME)
+      else:
+        self.logger.debug("Not removing '{0}' queue as number of Queues not equal to 2. Current YARN queues : {1}".format(llap_queue_name, list(leafQueueNames)))
+    else:
+      self.logger.error("Couldn't retrieve 'capacity-scheduler' properties while doing YARN queue adjustment for Hive Server Interactive.")
+
+  def setLlapDaemonQueuePropAttributes(self, services, configurations):
+    """
+    Checks and sets the 'Hive Server Interactive' 'hive.llap.daemon.queue.name' config Property Attributes.  Takes into
+    account that 'capacity-scheduler' may have changed (got updated) in current Stack Advisor invocation.
+    """
+    self.logger.info("Determining 'hive.llap.daemon.queue.name' config Property Attributes.")
+    #TODO Determine if this is doing the right thing if some queue is setup with capacity=0, or is STOPPED. Maybe don't list it.
+    putHiveInteractiveSitePropertyAttribute = self.putPropertyAttribute(configurations, YARNRecommender.HIVE_INTERACTIVE_SITE)
+
+    capacity_scheduler_properties = dict()
+
+    # Read 'capacity-scheduler' from configurations if we modified and added recommendation to it, as part of current
+    # StackAdvisor invocation.
+    if "capacity-scheduler" in configurations:
+      cap_sched_props_as_dict = configurations["capacity-scheduler"]["properties"]
+      if 'capacity-scheduler' in cap_sched_props_as_dict:
+        cap_sched_props_as_str = configurations['capacity-scheduler']['properties']['capacity-scheduler']
+        if cap_sched_props_as_str:
+          cap_sched_props_as_str = str(cap_sched_props_as_str).split('\n')
+          if len(cap_sched_props_as_str) > 0 and cap_sched_props_as_str[0] != 'null':
+            # Got 'capacity-scheduler' configs as one "\n" separated string
+            for property in cap_sched_props_as_str:
+              key, sep, value = property.partition("=")
+              capacity_scheduler_properties[key] = value
+            self.logger.info("'capacity-scheduler' configs is set as a single '\\n' separated string in current invocation. "
+                        "count(configurations['capacity-scheduler']['properties']['capacity-scheduler']) = "
+                        "{0}".format(len(capacity_scheduler_properties)))
+          else:
+            self.logger.info("Read configurations['capacity-scheduler']['properties']['capacity-scheduler'] is : {0}".format(cap_sched_props_as_str))
+        else:
+          self.logger.info("configurations['capacity-scheduler']['properties']['capacity-scheduler'] : {0}.".format(cap_sched_props_as_str))
+
+      # if 'capacity_scheduler_properties' is empty, implies we may have 'capacity-scheduler' configs as dictionary
+      # in configurations, if 'capacity-scheduler' changed in current invocation.
+      if not capacity_scheduler_properties:
+        if isinstance(cap_sched_props_as_dict, dict) and len(cap_sched_props_as_dict) > 1:
+          capacity_scheduler_properties = cap_sched_props_as_dict
+          self.logger.info("'capacity-scheduler' changed in current Stack Advisor invocation. Retrieved the configs as dictionary from configurations.")
+        else:
+          self.logger.info("Read configurations['capacity-scheduler']['properties'] is : {0}".format(cap_sched_props_as_dict))
+    else:
+      self.logger.info("'capacity-scheduler' not modified in the current Stack Advisor invocation.")
+
+
+    # if 'capacity_scheduler_properties' is still empty, implies 'capacity_scheduler' wasn't change in current
+    # SA invocation. Thus, read it from input : 'services'.
+    if not capacity_scheduler_properties:
+      capacity_scheduler_properties, received_as_key_value_pair = self.getCapacitySchedulerProperties(services)
+      self.logger.info("'capacity-scheduler' not changed in current Stack Advisor invocation. Retrieved the configs from services.")
+
+    # Get set of current YARN leaf queues.
+    leafQueueNames = self.getAllYarnLeafQueues(capacity_scheduler_properties)
+    if leafQueueNames:
+      leafQueues = [{"label": str(queueName), "value": queueName} for queueName in leafQueueNames]
+      leafQueues = sorted(leafQueues, key=lambda q: q['value'])
+      putHiveInteractiveSitePropertyAttribute("hive.llap.daemon.queue.name", "entries", leafQueues)
+      self.logger.info("'hive.llap.daemon.queue.name' config Property Attributes set to : {0}".format(leafQueues))
+    else:
+      self.logger.error("Problem retrieving YARN queues. Skipping updating HIVE Server Interactve "
+                   "'hive.server2.tez.default.queues' property attributes.")
+
+  #TODO  Convert this to a helper. It can apply to any property. Check config, or check if in the list of changed configurations and read the latest value
+  def get_yarn_min_container_size(self, services, configurations):
+    """
+    Gets YARN's minimum container size (yarn.scheduler.minimum-allocation-mb).
+    Reads from:
+      - configurations (if changed as part of current Stack Advisor invocation (output)), and services["changed-configurations"]
+        is empty, else
+      - services['configurations'] (input).
+
+    services["changed-configurations"] would be empty if Stack Advisor call is made from Blueprints (1st invocation). Subsequent
+    Stack Advisor calls will have it non-empty. We do this because in subsequent invocations, even if Stack Advisor calculates this
+    value (configurations), it is finally not recommended, making 'input' value to survive.
+
+    :type services dict
+    :type configurations dict
+    :rtype str
+    """
+    yarn_min_container_size = None
+    yarn_min_allocation_property = "yarn.scheduler.minimum-allocation-mb"
+    yarn_site = self.getSiteProperties(configurations, "yarn-site")
+    yarn_site_properties = self.getServicesSiteProperties(services, "yarn-site")
+
+    # Check if services["changed-configurations"] is empty and 'yarn.scheduler.minimum-allocation-mb' is modified in current ST invocation.
+    if not services["changed-configurations"] and yarn_site and yarn_min_allocation_property in yarn_site:
+      yarn_min_container_size = yarn_site[yarn_min_allocation_property]
+      self.logger.info("DBG: 'yarn.scheduler.minimum-allocation-mb' read from output as : {0}".format(yarn_min_container_size))
+
+    # Check if 'yarn.scheduler.minimum-allocation-mb' is input in services array.
+    elif yarn_site_properties and yarn_min_allocation_property in yarn_site_properties:
+      yarn_min_container_size = yarn_site_properties[yarn_min_allocation_property]
+      self.logger.info("DBG: 'yarn.scheduler.minimum-allocation-mb' read from services as : {0}".format(yarn_min_container_size))
+
+    if not yarn_min_container_size:
+      self.logger.error("{0} was not found in the configuration".format(yarn_min_allocation_property))
+
+    return yarn_min_container_size
+
+  def calculate_yarn_service_am_size(self, yarn_min_container_size):
+    """
+    Calculates the YARN Service App Master size based on YARN's Minimum Container Size.
+
+    :type yarn_min_container_size int
+    """
+    if yarn_min_container_size >= 1024:
+      return 1024
+    else:
+      return 512
+
+  def get_yarn_nm_mem_in_mb(self, services, configurations):
+    """
+    Gets YARN NodeManager memory in MB (yarn.nodemanager.resource.memory-mb).
+    Reads from:
+      - configurations (if changed as part of current Stack Advisor invocation (output)), and services["changed-configurations"]
+        is empty, else
+      - services['configurations'] (input).
+
+    services["changed-configurations"] would be empty is Stack Advisor call if made from Blueprints (1st invocation). Subsequent
+    Stack Advisor calls will have it non-empty. We do this because in subsequent invocations, even if Stack Advsior calculates this
+    value (configurations), it is finally not recommended, making 'input' value to survive.
+    """
+    yarn_nm_mem_in_mb = None
+
+    yarn_site = self.getServicesSiteProperties(services, "yarn-site")
+    yarn_site_properties = self.getSiteProperties(configurations, "yarn-site")
+
+    # Check if services["changed-configurations"] is empty and 'yarn.nodemanager.resource.memory-mb' is modified in current ST invocation.
+    if not services["changed-configurations"] and yarn_site_properties and 'yarn.nodemanager.resource.memory-mb' in yarn_site_properties:
+      yarn_nm_mem_in_mb = float(yarn_site_properties['yarn.nodemanager.resource.memory-mb'])
+    elif yarn_site and 'yarn.nodemanager.resource.memory-mb' in yarn_site:
+      # Check if 'yarn.nodemanager.resource.memory-mb' is input in services array.
+      yarn_nm_mem_in_mb = float(yarn_site['yarn.nodemanager.resource.memory-mb'])
+
+    if yarn_nm_mem_in_mb <= 0.0:
+      self.logger.warning("'yarn.nodemanager.resource.memory-mb' current value : {0}. Expected value : > 0".format(yarn_nm_mem_in_mb))
+
+    return yarn_nm_mem_in_mb
+
+  def calculate_tez_am_container_size(self, services, total_cluster_capacity, is_cluster_create_opr=False,
+                                      enable_hive_interactive_1st_invocation=False):
+    """
+    Calculates Tez App Master container size (tez.am.resource.memory.mb) for tez_hive2/tez-site on initialization if values read is 0.
+    Else returns the read value.
+    """
+    tez_am_resource_memory_mb = self.get_tez_am_resource_memory_mb(services)
+    calculated_tez_am_resource_memory_mb = None
+    if is_cluster_create_opr or enable_hive_interactive_1st_invocation:
+      if total_cluster_capacity <= 4096:
+        calculated_tez_am_resource_memory_mb = 512
+      elif total_cluster_capacity > 4096 and total_cluster_capacity <= 98304:
+        calculated_tez_am_resource_memory_mb = 2048
+      elif total_cluster_capacity > 98304:
+        calculated_tez_am_resource_memory_mb = 4096
+
+      self.logger.info("DBG: Calculated and returning 'tez_am_resource_memory_mb' as : {0}".format(calculated_tez_am_resource_memory_mb))
+      return float(calculated_tez_am_resource_memory_mb)
+    else:
+      self.logger.info("DBG: Returning 'tez_am_resource_memory_mb' as : {0}".format(tez_am_resource_memory_mb))
+      return float(tez_am_resource_memory_mb)
+
+  def get_tez_am_resource_memory_mb(self, services):
+    """
+    Gets Tez's AM resource memory (tez.am.resource.memory.mb) from services.
+    """
+    tez_am_resource_memory_mb = None
+    if 'tez.am.resource.memory.mb' in services['configurations']['tez-interactive-site']['properties']:
+      tez_am_resource_memory_mb = services['configurations']['tez-interactive-site']['properties']['tez.am.resource.memory.mb']
+
+    return tez_am_resource_memory_mb
+
+  def min_queue_perc_reqd_for_llap_and_hive_app(self, services, hosts, configurations):
+    """
+    Calculate minimum queue capacity required in order to get LLAP and HIVE2 app into running state.
+    """
+    # Get queue size if sized at 20%
+    node_manager_hosts = self.getHostsForComponent(services, "YARN", "NODEMANAGER")
+    yarn_rm_mem_in_mb = self.get_yarn_nm_mem_in_mb(services, configurations)
+    total_cluster_cap = len(node_manager_hosts) * yarn_rm_mem_in_mb
+    total_queue_size_at_20_perc = 20.0 / 100 * total_cluster_cap
+
+    # Calculate based on minimum size required by containers.
+    yarn_min_container_size = long(self.get_yarn_min_container_size(services, configurations))
+    yarn_service_am_size = self.calculate_yarn_service_am_size(float(yarn_min_container_size))
+    hive_tez_container_size = long(self.get_hive_tez_container_size(services))
+    tez_am_container_size = self.calculate_tez_am_container_size(services, long(total_cluster_cap))
+    normalized_val = self._normalizeUp(yarn_service_am_size, yarn_min_container_size) \
+                     + self._normalizeUp(hive_tez_container_size, yarn_min_container_size) \
+                     + self._normalizeUp(tez_am_container_size, yarn_min_container_size)
+
+    min_required = max(total_queue_size_at_20_perc, normalized_val)
+    min_required_perc = min_required * 100 / total_cluster_cap
+
+    return int(ceil(min_required_perc))
+
+  def _normalizeDown(self, val1, val2):
+    """
+    Normalize down 'val2' with respect to 'val1'.
+    """
+    tmp = floor(val1 / val2)
+    if tmp < 1.00:
+      return val1
+    return tmp * val2
+
+  def _normalizeUp(self, val1, val2):
+    """
+    Normalize up 'val2' with respect to 'val1'.
+    """
+    tmp = ceil(val1 / val2)
+    return tmp * val2
+
+  def __getQueueStateFromCapacityScheduler(self, capacity_scheduler_properties, llap_daemon_selected_queue_name):
+    """
+    Retrieves the passed in queue's 'state' from Capacity Scheduler.
+    """
+    # Identify the key which contains the state for 'llap_daemon_selected_queue_name'.
+    cap_sched_keys = capacity_scheduler_properties.keys()
+    llap_selected_queue_state_key =  None
+    llap_selected_queue_state = None
+    for key in cap_sched_keys:
+      if key.endswith(llap_daemon_selected_queue_name+".state"):
+        llap_selected_queue_state_key = key
+        break
+    llap_selected_queue_state = capacity_scheduler_properties.get(llap_selected_queue_state_key)
+    return llap_selected_queue_state
+
+  def __getQueueAmFractionFromCapacityScheduler(self, capacity_scheduler_properties, llap_daemon_selected_queue_name):
+    """
+    Retrieves the passed in queue's 'AM fraction' from Capacity Scheduler. Returns default value of 0.1 if AM Percent
+    pertaining to passed-in queue is not present.
+    """
+    # Identify the key which contains the AM fraction for 'llap_daemon_selected_queue_name'.
+    cap_sched_keys = capacity_scheduler_properties.keys()
+    llap_selected_queue_am_percent_key = None
+    for key in cap_sched_keys:
+      if key.endswith("." + llap_daemon_selected_queue_name+".maximum-am-resource-percent"):
+        llap_selected_queue_am_percent_key = key
+        self.logger.info("AM percent key got for '{0}' queue is : '{1}'".format(llap_daemon_selected_queue_name, llap_selected_queue_am_percent_key))
+        break
+    if llap_selected_queue_am_percent_key is None:
+      self.logger.info("Returning default AM percent value : '0.1' for queue : {0}".format(llap_daemon_selected_queue_name))
+      return 0.1 # Default value to use if we couldn't retrieve queue's corresponding AM Percent key.
+    else:
+      llap_selected_queue_am_percent = capacity_scheduler_properties.get(llap_selected_queue_am_percent_key)
+      self.logger.info("Returning read value for key '{0}' as : '{1}' for queue : '{2}'".format(llap_selected_queue_am_percent_key,
+                                                                                           llap_selected_queue_am_percent,
+                                                                                           llap_daemon_selected_queue_name))
+      return llap_selected_queue_am_percent
+
+  def __getSelectedQueueTotalCap(self, capacity_scheduler_properties, llap_daemon_selected_queue_name, total_cluster_capacity):
+    """
+    Calculates the total available capacity for the passed-in YARN queue of any level based on the percentages.
+    """
+    self.logger.info("Entered __getSelectedQueueTotalCap fn() with llap_daemon_selected_queue_name= '{0}'.".format(llap_daemon_selected_queue_name))
+    available_capacity = total_cluster_capacity
+    queue_cap_key = self.__getQueueCapacityKeyFromCapacityScheduler(capacity_scheduler_properties, llap_daemon_selected_queue_name)
+    if queue_cap_key:
+      queue_cap_key = queue_cap_key.strip()
+      if len(queue_cap_key) >= 34:  # len('yarn.scheduler.capacity.<single letter queue name>.capacity') = 34
+        # Expected capacity prop key is of form : 'yarn.scheduler.capacity.<one or more queues (path)>.capacity'
+        queue_path = queue_cap_key[24:]  # Strip from beginning 'yarn.scheduler.capacity.'
+        queue_path = queue_path[0:-9]  # Strip from end '.capacity'
+        queues_list = queue_path.split('.')
+        self.logger.info("Queue list : {0}".format(queues_list))
+        if queues_list:
+          for queue in queues_list:
+            queue_cap_key = self.__getQueueCapacityKeyFromCapacityScheduler(capacity_scheduler_properties, queue)
+            queue_cap_perc = float(capacity_scheduler_properties.get(queue_cap_key))
+            available_capacity = queue_cap_perc / 100 * available_capacity
+            self.logger.info("Total capacity available for queue {0} is : {1}".format(queue, available_capacity))
+
+    # returns the capacity calculated for passed-in queue in 'llap_daemon_selected_queue_name'.
+    return available_capacity
+
+  def __getQueueCapacityKeyFromCapacityScheduler(self, capacity_scheduler_properties, llap_daemon_selected_queue_name):
+    """
+    Retrieves the passed in queue's 'capacity' related key from Capacity Scheduler.
+    """
+    # Identify the key which contains the capacity for 'llap_daemon_selected_queue_name'.
+    cap_sched_keys = capacity_scheduler_properties.keys()
+    llap_selected_queue_cap_key =  None
+    current_selected_queue_for_llap_cap = None
+    for key in cap_sched_keys:
+      # Expected capacity prop key is of form : 'yarn.scheduler.capacity.<one or more queues in path separated by '.'>.[llap_daemon_selected_queue_name].capacity'
+      if key.endswith(llap_daemon_selected_queue_name+".capacity") and key.startswith("yarn.scheduler.capacity.root"):
+        self.logger.info("DBG: Selected queue name as: " + key)
+        llap_selected_queue_cap_key = key
+        break
+    return llap_selected_queue_cap_key
+  #endregion
+
+
+class MAPREDUCE2Recommender(YARNRecommender):
+  """
+  MAPREDUCE2 Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(MAPREDUCE2Recommender, self)
+    self.as_super.__init__(*args, **kwargs)
+
+  def recommendMapReduce2ConfigurationsFromHDP206(self, configurations, clusterData, services, hosts):
+    putMapredProperty = self.putProperty(configurations, "mapred-site", services)
+    putMapredProperty('yarn.app.mapreduce.am.resource.mb', int(clusterData['amMemory']))
+    putMapredProperty('yarn.app.mapreduce.am.command-opts', "-Xmx" + str(int(round(0.8 * clusterData['amMemory']))) + "m")
+    putMapredProperty('mapreduce.map.memory.mb', clusterData['mapMemory'])
+    putMapredProperty('mapreduce.reduce.memory.mb', int(clusterData['reduceMemory']))
+    putMapredProperty('mapreduce.map.java.opts', "-Xmx" + str(int(round(0.8 * clusterData['mapMemory']))) + "m")
+    putMapredProperty('mapreduce.reduce.java.opts', "-Xmx" + str(int(round(0.8 * clusterData['reduceMemory']))) + "m")
+    putMapredProperty('mapreduce.task.io.sort.mb', min(int(round(0.4 * clusterData['mapMemory'])), 1024))
+
+    mapred_mounts = [
+      ("mapred.local.dir", ["TASKTRACKER", "NODEMANAGER"], "/hadoop/mapred", "multi")
+    ]
+
+    self.updateMountProperties("mapred-site", mapred_mounts, configurations, services, hosts)
+
+    mr_queue = self.recommendYarnQueue(services, "mapred-site", "mapreduce.job.queuename")
+    if mr_queue is not None:
+      putMapredProperty("mapreduce.job.queuename", mr_queue)
+
+  def recommendMapReduce2ConfigurationsFromHDP22(self, configurations, clusterData, services, hosts):
+    # Needs to be able to access yarn-site
+    # TODO, this is a hack that was introduced in 2015. The yarn-site configs will not actually be saved
+    # as part of MAPREDUCE2 because yarn-site doesn't belong to it according to its metainfo.xml
+    # MAPREDUCE2 Recommender first needs to call all methods from YARN Recommender
+    self.recommendYARNConfigurationsFromHDP206(configurations, clusterData, services, hosts)
+    self.recommendYARNConfigurationsFromHDP22(configurations, clusterData, services, hosts)
+
+    putMapredProperty = self.putProperty(configurations, "mapred-site", services)
+    nodemanagerMinRam = 1048576 # 1TB in mb
+    if "referenceNodeManagerHost" in clusterData:
+      nodemanagerMinRam = min(clusterData["referenceNodeManagerHost"]["total_mem"]/1024, nodemanagerMinRam)
+
+    putMapredProperty('yarn.app.mapreduce.am.resource.mb', min(max(int(clusterData['ramPerContainer']),int(configurations["yarn-site"]["properties"]["yarn.scheduler.minimum-allocation-mb"])),
+                                                               int(configurations["yarn-site"]["properties"]["yarn.scheduler.maximum-allocation-mb"])))
+    putMapredProperty('yarn.app.mapreduce.am.command-opts', "-Xmx" + str(int(0.8 * int(configurations["mapred-site"]["properties"]["yarn.app.mapreduce.am.resource.mb"]))) + "m" + " -Dhdp.version=${hdp.version}")
+    servicesList = [service["StackServices"]["service_name"] for service in services["services"]]
+    min_mapreduce_map_memory_mb = 0
+    min_mapreduce_reduce_memory_mb = 0
+    min_mapreduce_map_java_opts = 0
+    if ("PIG" in servicesList) and clusterData["totalAvailableRam"] >= 4096:
+      min_mapreduce_map_memory_mb = 1536
+      min_mapreduce_reduce_memory_mb = 1536
+      min_mapreduce_map_java_opts = 1024
+
+    putMapredProperty('mapreduce.map.memory.mb',
+                      min(int(configurations["yarn-site"]["properties"]["yarn.scheduler.maximum-allocation-mb"]),
+                          max(min_mapreduce_map_memory_mb,
+                              max(int(clusterData['ramPerContainer']),
+                                  int(configurations["yarn-site"]["properties"]["yarn.scheduler.minimum-allocation-mb"])))))
+    putMapredProperty('mapreduce.reduce.memory.mb',
+                      min(int(configurations["yarn-site"]["properties"]["yarn.scheduler.maximum-allocation-mb"]),
+                          max(max(min_mapreduce_reduce_memory_mb,
+                                  int(configurations["yarn-site"]["properties"]["yarn.scheduler.minimum-allocation-mb"])),
+                              min(2*int(clusterData['ramPerContainer']),
+                                  int(nodemanagerMinRam)))))
+
+    mapredMapXmx = int(0.8*int(configurations["mapred-site"]["properties"]["mapreduce.map.memory.mb"]))
+    putMapredProperty('mapreduce.map.java.opts', "-Xmx" + str(max(min_mapreduce_map_java_opts, mapredMapXmx)) + "m")
+    putMapredProperty('mapreduce.reduce.java.opts', "-Xmx" + str(int(0.8*int(configurations["mapred-site"]["properties"]["mapreduce.reduce.memory.mb"]))) + "m")
+    putMapredProperty('mapreduce.task.io.sort.mb', str(min(int(0.7*mapredMapXmx), 2047)))
+    # Property Attributes
+    putMapredPropertyAttribute = self.putPropertyAttribute(configurations, "mapred-site")
+    yarnMinAllocationSize = int(configurations["yarn-site"]["properties"]["yarn.scheduler.minimum-allocation-mb"])
+    yarnMaxAllocationSize = min(30 * int(configurations["yarn-site"]["properties"]["yarn.scheduler.minimum-allocation-mb"]), int(configurations["yarn-site"]["properties"]["yarn.scheduler.maximum-allocation-mb"]))
+    putMapredPropertyAttribute("mapreduce.map.memory.mb", "maximum", yarnMaxAllocationSize)
+    putMapredPropertyAttribute("mapreduce.map.memory.mb", "minimum", yarnMinAllocationSize)
+    putMapredPropertyAttribute("mapreduce.reduce.memory.mb", "maximum", yarnMaxAllocationSize)
+    putMapredPropertyAttribute("mapreduce.reduce.memory.mb", "minimum", yarnMinAllocationSize)
+    putMapredPropertyAttribute("yarn.app.mapreduce.am.resource.mb", "maximum", yarnMaxAllocationSize)
+    putMapredPropertyAttribute("yarn.app.mapreduce.am.resource.mb", "minimum", yarnMinAllocationSize)
+    # Hadoop MR limitation
+    putMapredPropertyAttribute("mapreduce.task.io.sort.mb", "maximum", "2047")
+
+    # TODO, this is repeated in 2.0.6 recommendation
+    mr_queue = self.recommendYarnQueue(services, "mapred-site", "mapreduce.job.queuename")
+    if mr_queue is not None:
+      putMapredProperty("mapreduce.job.queuename", mr_queue)
+
+  def recommendConfigurationsForSSO(self, configurations, clusterData, services, hosts):
+    ambari_configuration = self.get_ambari_configuration(services)
+    ambari_sso_details = ambari_configuration.get_ambari_sso_details() if ambari_configuration else None
+
+    if ambari_sso_details and ambari_sso_details.is_managing_services():
+      putMapRedSiteProperty = self.putProperty(configurations, "mapred-site", services)
+
+      # If SSO should be enabled for this service
+      if ambari_sso_details.should_enable_sso('MAPREDUCE2'):
+        if(self.is_kerberos_enabled(configurations, services)):
+          putMapRedSiteProperty('hadoop.http.authentication.type', "org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler")
+          putMapRedSiteProperty('hadoop.http.authentication.authentication.provider.url', ambari_sso_details.get_sso_provider_url())
+          putMapRedSiteProperty('hadoop.http.authentication.public.key.pem', ambari_sso_details.get_sso_provider_certificate(False, True))
+        else:
+          # Since Kerberos is not enabled, we can not enable SSO
+          self.logger.warn("Enabling SSO integration for MapReduce requires Kerberos, Since Kerberos is not enabled, SSO integration is not being recommended.")
+          putMapRedSiteProperty('hadoop.http.authentication.type', "simple")
+          pass
+
+      # If SSO should be disabled for this service
+      elif ambari_sso_details.should_disable_sso('MAPREDUCE2'):
+        if(self.is_kerberos_enabled(configurations, services)):
+          putMapRedSiteProperty('hadoop.http.authentication.type', "kerberos")
+        else:
+          putMapRedSiteProperty('hadoop.http.authentication.type', "simple")
+
+  def is_kerberos_enabled(self, configurations, services):
+    """
+    Tests if MapReduce has Kerberos enabled by first checking the recommended changes and then the
+    existing settings.
+    :type configurations dict
+    :type services dict
+    :rtype bool
+    """
+    return self._is_kerberos_enabled(configurations) or \
+           (services and 'configurations' in services and self._is_kerberos_enabled(services['configurations']))
+
+  def _is_kerberos_enabled(self, config):
+    """
+    Detects if MapReduce has Kerberos enabled given a dictionary of configurations.
+    :type config dict
+    :rtype bool
+    """
+    return config and \
+           (
+             (
+               "mapred-site" in config and
+               'hadoop.security.authentication' in config['mapred-site']["properties"] and
+               config['mapred-site']["properties"]['hadoop.security.authentication'] == 'kerberos'
+             ) or (
+               "core-site" in config and
+               'hadoop.security.authentication' in config['core-site']["properties"] and
+               config['core-site']["properties"]['hadoop.security.authentication'] == 'kerberos'
+             )
+           )
+
+
+class YARNValidator(service_advisor.ServiceAdvisor):
+  """
+  YARN Validator checks the correctness of properties whenever the service is first added or the user attempts to
+  change configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(YARNValidator, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    self.validators = [("yarn-site", self.validateYARNSiteConfigurationsFromHDP206),
+                       ("yarn-site", self.validateYARNSiteConfigurationsFromHDP25),
+                       ("yarn-site" , self.validateYARNSiteConfigurationsFromHDP26),
+                       ("yarn-env", self.validateYARNEnvConfigurationsFromHDP206),
+                       ("yarn-env", self.validateYARNEnvConfigurationsFromHDP22),
+                       ("ranger-yarn-plugin-properties", self.validateYARNRangerPluginConfigurationsFromHDP22)]
+
+    # **********************************************************
+    # Example of how to add a function that validates a certain config type.
+    # If the same config type has multiple functions, can keep adding tuples to self.validators
+    #self.validators.append(("hadoop-env", self.sampleValidator))
+
+  def sampleValidator(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    Example of a validator function other other Service Advisors to emulate.
+    :return: A list of configuration validation problems.
+    """
+    validationItems = []
+
+    '''
+    Item is a simple dictionary.
+    Two functions can be used to construct it depending on the log level: WARN|ERROR
+    E.g.,
+    self.getErrorItem(message) or self.getWarnItem(message)
+
+    item = {"level": "ERROR|WARN", "message": "value"}
+    '''
+    validationItems.append({"config-name": "my_config_property_name",
+                            "item": self.getErrorItem("My custom message in method %s" % inspect.stack()[0][3])})
+    return self.toConfigurationValidationProblems(validationItems, "hadoop-env")
+
+  def validateYARNSiteConfigurationsFromHDP206(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    This was copied from HDP 2.0.6; validate yarn-site
+    :return: A list of configuration validation problems.
+    """
+    clusterEnv = self.getSiteProperties(configurations, "cluster-env")
+    validationItems = [ {"config-name": 'yarn.nodemanager.resource.memory-mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'yarn.nodemanager.resource.memory-mb')},
+                        {"config-name": 'yarn.scheduler.minimum-allocation-mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'yarn.scheduler.minimum-allocation-mb')},
+                        {"config-name": 'yarn.nodemanager.linux-container-executor.group', "item": self.validatorEqualsPropertyItem(properties, "yarn.nodemanager.linux-container-executor.group", clusterEnv, "user_group")},
+                        {"config-name": 'yarn.scheduler.maximum-allocation-mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'yarn.scheduler.maximum-allocation-mb')} ]
+    return self.toConfigurationValidationProblems(validationItems, "yarn-site")
+
+  def validateYARNSiteConfigurationsFromHDP25(self, properties, recommendedDefaults, configurations, services, hosts):
+    yarn_site_properties = self.getSiteProperties(configurations, "yarn-site")
+    validationItems = []
+
+    hsi_hosts = self.getHostsForComponent(services, "HIVE", "HIVE_SERVER_INTERACTIVE")
+    if len(hsi_hosts) > 0:
+      # HIVE_SERVER_INTERACTIVE is mapped to a host
+      if 'yarn.resourcemanager.work-preserving-recovery.enabled' not in yarn_site_properties or \
+              'true' != yarn_site_properties['yarn.resourcemanager.work-preserving-recovery.enabled']:
+        validationItems.append({"config-name": "yarn.resourcemanager.work-preserving-recovery.enabled",
+                                "item": self.getWarnItem(
+                                  "While enabling HIVE_SERVER_INTERACTIVE it is recommended that you enable work preserving restart in YARN.")})
+
+    validationProblems = self.toConfigurationValidationProblems(validationItems, "yarn-site")
+    return validationProblems
+
+  def validateYARNSiteConfigurationsFromHDP26(self, properties, recommendedDefaults, configurations, services, hosts):
+    validationItems = []
+    siteProperties = services["configurations"]["yarn-site"]["properties"]
+    if services["configurations"]["yarn-site"]["properties"]["yarn.http.policy"] == 'HTTP_ONLY':
+      webapp_address = services["configurations"]["yarn-site"]["properties"]["yarn.timeline-service.webapp.address"]
+      propertyValue = "http://"+webapp_address+"/ws/v1/applicationhistory"
+    else:
+      webapp_address = services["configurations"]["yarn-site"]["properties"]["yarn.timeline-service.webapp.https.address"]
+      propertyValue = "https://"+webapp_address+"/ws/v1/applicationhistory"
+      self.logger.info("validateYarnSiteConfigurations: recommended value for webservice url"+services["configurations"]["yarn-site"]["properties"]["yarn.log.server.web-service.url"])
+    # if services["configurations"]["yarn-site"]["properties"]["yarn.log.server.web-service.url"] != propertyValue:
+    #   validationItems = [
+    #                   {"config-name": "yarn.log.server.web-service.url",
+    #                    "item": self.getWarnItem("Value should be %s" % propertyValue)}]
+
+    if "yarn_hierarchy" in services["configurations"]["container-executor"]["properties"] \
+              and "yarn.nodemanager.linux-container-executor.cgroups.hierarchy" in services["configurations"]["yarn-site"]["properties"]:
+      yn_hirch = services["configurations"]["container-executor"]["properties"]["yarn_hierarchy"]
+      yn_crp_hirch = services["configurations"]["yarn-site"]["properties"]["yarn.nodemanager.linux-container-executor.cgroups.hierarchy"]
+      if yn_hirch != yn_crp_hirch:
+        validationItems.append({"config-name": 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy',
+                              "item": self.getWarnItem(
+                                "yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value")})
+
+    return self.toConfigurationValidationProblems(validationItems, "yarn-site")
+
+  def validateYARNEnvConfigurationsFromHDP206(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    This was copied from HDP 2.0.6; validate yarn-env
+    :return: A list of configuration validation problems.
+    """
+    validationItems = [{"config-name": 'service_check.queue.name', "item": self.validatorYarnQueue(properties, recommendedDefaults, 'service_check.queue.name', services)} ]
+    return self.toConfigurationValidationProblems(validationItems, "yarn-env")
+
+  def validateYARNEnvConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    This was copied from HDP 2.2; validate yarn-env
+    :return: A list of configuration validation problems.
+    """
+    validationItems = []
+    if "yarn_cgroups_enabled" in properties:
+      yarn_cgroups_enabled = properties["yarn_cgroups_enabled"].lower() == "true"
+      core_site_properties = self.getSiteProperties(configurations, "core-site")
+      security_enabled = False
+      if core_site_properties:
+        security_enabled = core_site_properties['hadoop.security.authentication'] == 'kerberos' and core_site_properties['hadoop.security.authorization'] == 'true'
+      if not security_enabled and yarn_cgroups_enabled:
+        validationItems.append({"config-name": "yarn_cgroups_enabled",
+                                "item": self.getWarnItem("CPU Isolation should only be enabled if security is enabled")})
+    validationProblems = self.toConfigurationValidationProblems(validationItems, "yarn-env")
+    return validationProblems
+
+  def validateYARNRangerPluginConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    This was copied from HDP 2.2; validate ranger-yarn-plugin-properties
+    :return: A list of configuration validation problems.
+    """
+    validationItems = []
+    ranger_plugin_properties = self.getSiteProperties(configurations, "ranger-yarn-plugin-properties")
+    ranger_plugin_enabled = ranger_plugin_properties['ranger-yarn-plugin-enabled'] if ranger_plugin_properties else 'No'
+    if ranger_plugin_enabled.lower() == 'yes':
+      # ranger-hdfs-plugin must be enabled in ranger-env
+      ranger_env = self.getServicesSiteProperties(services, 'ranger-env')
+      if not ranger_env or not 'ranger-yarn-plugin-enabled' in ranger_env or \
+              ranger_env['ranger-yarn-plugin-enabled'].lower() != 'yes':
+        validationItems.append({"config-name": 'ranger-yarn-plugin-enabled',
+                                "item": self.getWarnItem(
+                                  "ranger-yarn-plugin-properties/ranger-yarn-plugin-enabled must correspond ranger-env/ranger-yarn-plugin-enabled")})
+    return self.toConfigurationValidationProblems(validationItems, "ranger-yarn-plugin-properties")
+
+
+class MAPREDUCE2Validator(service_advisor.ServiceAdvisor):
+  """
+  YARN Validator checks the correctness of properties whenever the service is first added or the user attempts to
+  change configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(MAPREDUCE2Validator, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    self.validators = [("mapred-site", self.validateMapReduce2SiteConfigurationsFromHDP206),
+                       ("mapred-site", self.validateMapReduce2SiteConfigurationsFromHDP22)]
+
+    # **********************************************************
+    # Example of how to add a function that validates a certain config type.
+    # If the same config type has multiple functions, can keep adding tuples to self.validators
+    #self.validators.append(("hadoop-env", self.sampleValidator))
+
+  def sampleValidator(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    Example of a validator function other other Service Advisors to emulate.
+    :return: A list of configuration validation problems.
+    """
+    validationItems = []
+
+    '''
+    Item is a simple dictionary.
+    Two functions can be used to construct it depending on the log level: WARN|ERROR
+    E.g.,
+    self.getErrorItem(message) or self.getWarnItem(message)
+
+    item = {"level": "ERROR|WARN", "message": "value"}
+    '''
+    validationItems.append({"config-name": "my_config_property_name",
+                            "item": self.getErrorItem("My custom message in method %s" % inspect.stack()[0][3])})
+    return self.toConfigurationValidationProblems(validationItems, "hadoop-env")
+
+  def validateMapReduce2SiteConfigurationsFromHDP206(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    This was copied from HDP 2.0.6; validate mapred-site
+    :return: A list of configuration validation problems.
+    """
+    validationItems = [ {"config-name": 'mapreduce.map.java.opts', "item": self.validateXmxValue(properties, recommendedDefaults, 'mapreduce.map.java.opts')},
+                        {"config-name": 'mapreduce.reduce.java.opts', "item": self.validateXmxValue(properties, recommendedDefaults, 'mapreduce.reduce.java.opts')},
+                        {"config-name": 'mapreduce.task.io.sort.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'mapreduce.task.io.sort.mb')},
+                        {"config-name": 'mapreduce.map.memory.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'mapreduce.map.memory.mb')},
+                        {"config-name": 'mapreduce.reduce.memory.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'mapreduce.reduce.memory.mb')},
+                        {"config-name": 'yarn.app.mapreduce.am.resource.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'yarn.app.mapreduce.am.resource.mb')},
+                        {"config-name": 'yarn.app.mapreduce.am.command-opts', "item": self.validateXmxValue(properties, recommendedDefaults, 'yarn.app.mapreduce.am.command-opts')},
+                        {"config-name": 'mapreduce.job.queuename', "item": self.validatorYarnQueue(properties, recommendedDefaults, 'mapreduce.job.queuename', services)} ]
+    return self.toConfigurationValidationProblems(validationItems, "mapred-site")
+
+  def validateMapReduce2SiteConfigurationsFromHDP22(self, properties, recommendedDefaults, configurations, services, hosts):
+    """
+    This was copied from HDP 2.2; validate mapred-site
+    :return: A list of configuration validation problems.
+    """
+    validationItems = [ {"config-name": 'mapreduce.map.java.opts', "item": self.validateXmxValue(properties, recommendedDefaults, 'mapreduce.map.java.opts')},
+                        {"config-name": 'mapreduce.reduce.java.opts', "item": self.validateXmxValue(properties, recommendedDefaults, 'mapreduce.reduce.java.opts')},
+                        {"config-name": 'mapreduce.task.io.sort.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'mapreduce.task.io.sort.mb')},
+                        {"config-name": 'mapreduce.map.memory.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'mapreduce.map.memory.mb')},
+                        {"config-name": 'mapreduce.reduce.memory.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'mapreduce.reduce.memory.mb')},
+                        {"config-name": 'yarn.app.mapreduce.am.resource.mb', "item": self.validatorLessThenDefaultValue(properties, recommendedDefaults, 'yarn.app.mapreduce.am.resource.mb')},
+                        {"config-name": 'yarn.app.mapreduce.am.command-opts', "item": self.validateXmxValue(properties, recommendedDefaults, 'yarn.app.mapreduce.am.command-opts')},
+                        {"config-name": 'mapreduce.job.queuename', "item": self.validatorYarnQueue(properties, recommendedDefaults, 'mapreduce.job.queuename', services)} ]
+
+    if 'mapreduce.map.java.opts' in properties and \
+        self.checkXmxValueFormat(properties['mapreduce.map.java.opts']):
+      mapreduceMapJavaOpts = self.formatXmxSizeToBytes(self.getXmxSize(properties['mapreduce.map.java.opts'])) / (1024.0 * 1024)
+      mapreduceMapMemoryMb = self.to_number(properties['mapreduce.map.memory.mb'])
+      if mapreduceMapJavaOpts > mapreduceMapMemoryMb:
+        validationItems.append({"config-name": 'mapreduce.map.java.opts', "item": self.getWarnItem("mapreduce.map.java.opts Xmx should be less than mapreduce.map.memory.mb ({0})".format(mapreduceMapMemoryMb))})
+
+    if 'mapreduce.reduce.java.opts' in properties and \
+        self.checkXmxValueFormat(properties['mapreduce.reduce.java.opts']):
+      mapreduceReduceJavaOpts = self.formatXmxSizeToBytes(self.getXmxSize(properties['mapreduce.reduce.java.opts'])) / (1024.0 * 1024)
+      mapreduceReduceMemoryMb = self.to_number(properties['mapreduce.reduce.memory.mb'])
+      if mapreduceReduceJavaOpts > mapreduceReduceMemoryMb:
+        validationItems.append({"config-name": 'mapreduce.reduce.java.opts', "item": self.getWarnItem("mapreduce.reduce.java.opts Xmx should be less than mapreduce.reduce.memory.mb ({0})".format(mapreduceReduceMemoryMb))})
+
+    if 'yarn.app.mapreduce.am.command-opts' in properties and \
+        self.checkXmxValueFormat(properties['yarn.app.mapreduce.am.command-opts']):
+      yarnAppMapreduceAmCommandOpts = self.formatXmxSizeToBytes(self.getXmxSize(properties['yarn.app.mapreduce.am.command-opts'])) / (1024.0 * 1024)
+      yarnAppMapreduceAmResourceMb = self.to_number(properties['yarn.app.mapreduce.am.resource.mb'])
+      if yarnAppMapreduceAmCommandOpts > yarnAppMapreduceAmResourceMb:
+        validationItems.append({"config-name": 'yarn.app.mapreduce.am.command-opts', "item": self.getWarnItem("yarn.app.mapreduce.am.command-opts Xmx should be less than yarn.app.mapreduce.am.resource.mb ({0})".format(yarnAppMapreduceAmResourceMb))})
+
+    return self.toConfigurationValidationProblems(validationItems, "mapred-site")
Index: ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin.py b/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin.py
--- a/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin.py	(date 1719626239000)
@@ -35,7 +35,7 @@
 def setup_ranger_plugin(component_select_name, service_name, previous_jdbc_jar,
                         downloaded_custom_connector, driver_curl_source,
                         driver_curl_target, java_home,
-                        repo_name, plugin_repo_dict, 
+                        repo_name, plugin_repo_dict,
                         ranger_env_properties, plugin_properties,
                         policy_user, policymgr_mgr_url,
                         plugin_enabled, component_user, component_group, api_version=None, skip_if_rangeradmin_down = True, **kwargs):
@@ -61,10 +61,10 @@
   stack_root = Script.get_stack_root()
   stack_version = get_stack_version(component_select_name)
   file_path = format('{stack_root}/{stack_version}/ranger-{service_name}-plugin/install.properties')
-  
+
   if not os.path.isfile(file_path):
     raise Fail(format('Ranger {service_name} plugin install.properties file does not exist at {file_path}'))
-  
+
   ModifyPropertiesFile(file_path,
     properties = plugin_properties
   )
@@ -82,18 +82,18 @@
       ranger_adm_obj = Rangeradmin(url=policymgr_mgr_url, skip_if_rangeradmin_down = skip_if_rangeradmin_down)
 
     ranger_adm_obj.create_ranger_repository(service_name, repo_name, plugin_repo_dict,
-                                            ranger_env_properties['ranger_admin_username'], ranger_env_properties['ranger_admin_password'], 
-                                            ranger_env_properties['admin_username'], ranger_env_properties['admin_password'], 
+                                            ranger_env_properties['ranger_admin_username'], ranger_env_properties['ranger_admin_password'],
+                                            ranger_env_properties['admin_username'], ranger_env_properties['admin_password'],
                                             policy_user)
   else:
     cmd = (format('disable-{service_name}-plugin.sh'),)
-    
+
   cmd_env = {'JAVA_HOME': java_home,
              'PWD': format('{stack_root}/{stack_version}/ranger-{service_name}-plugin'),
              'PATH': format('{stack_root}/{stack_version}/ranger-{service_name}-plugin')}
-  
-  Execute(cmd, 
-        environment=cmd_env, 
+
+  Execute(cmd,
+        environment=cmd_env,
         logoutput=True,
         sudo=True,
   )
Index: ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin_xml.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin_xml.py b/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin_xml.py
--- a/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin_xml.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-common/src/main/python/resource_management/libraries/functions/setup_ranger_plugin_xml.py	(date 1723875382343)
@@ -20,12 +20,15 @@
 __all__ = ["setup_ranger_plugin", "get_audit_configs", "generate_ranger_service_config"]
 
 import os
+import traceback
+
 import ambari_simplejson as json
 from datetime import datetime
+
+from resource_management import Fail
 from resource_management.libraries.functions.ranger_functions import Rangeradmin
 from resource_management.core.resources import File, Directory, Execute
 from resource_management.libraries.resources.xml_config import XmlConfig
-from resource_management.libraries.functions.format import format
 from resource_management.libraries.functions.get_stack_version import get_stack_version
 from resource_management.core.logger import Logger
 from resource_management.core.source import DownloadSource, InlineTemplate
@@ -35,6 +38,7 @@
 from resource_management.libraries.functions.format import format
 from resource_management.libraries.functions.default import default
 
+
 def setup_ranger_plugin(component_select_name, service_name, previous_jdbc_jar,
                         component_downloaded_custom_connector, component_driver_curl_source,
                         component_driver_curl_target, java_home,
@@ -47,277 +51,321 @@
                         plugin_policymgr_ssl_properties, plugin_policymgr_ssl_attributes,
                         component_list, audit_db_is_enabled, credential_file,
                         xa_audit_db_password, ssl_truststore_password,
-                        ssl_keystore_password, api_version=None, stack_version_override = None, skip_if_rangeradmin_down = True,
-                        is_security_enabled = False, is_stack_supports_ranger_kerberos = False,
-                        component_user_principal = None, component_user_keytab = None, cred_lib_path_override = None, cred_setup_prefix_override = None):
+                        ssl_keystore_password, api_version=None, stack_version_override=None,
+                        skip_if_rangeradmin_down=True,
+                        is_security_enabled=False, is_stack_supports_ranger_kerberos=False,
+                        component_user_principal=None, component_user_keytab=None, cred_lib_path_override=None,
+                        cred_setup_prefix_override=None):
 
-  if audit_db_is_enabled and component_driver_curl_source is not None and not component_driver_curl_source.endswith("/None"):
-    if previous_jdbc_jar and os.path.isfile(previous_jdbc_jar):
-      File(previous_jdbc_jar, action='delete')
+    bigtop_select_version = Script.get_stack_version().replace(".", "_")
+    ranger_component_name = format('ranger_{bigtop_select_version}-{service_name}-plugin')
+    try:
+        from resource_management import Package
+        Package(ranger_component_name)
+    except KeyError:
+        traceback.print_exc()
+
+    if audit_db_is_enabled and component_driver_curl_source is not None and not component_driver_curl_source.endswith(
+            "/None"):
+        if previous_jdbc_jar and os.path.isfile(previous_jdbc_jar):
+            File(previous_jdbc_jar, action='delete')
 
-    File(component_downloaded_custom_connector,
-      content = DownloadSource(component_driver_curl_source),
-      mode = 0644
-    )
+        File(component_downloaded_custom_connector,
+             content=DownloadSource(component_driver_curl_source),
+             mode=0644
+             )
 
-    Execute(('cp', '--remove-destination', component_downloaded_custom_connector, component_driver_curl_target),
-      path=["/bin", "/usr/bin/"],
-      sudo=True
-    )
+        Execute(('cp', '--remove-destination', component_downloaded_custom_connector, component_driver_curl_target),
+                path=["/bin", "/usr/bin/"],
+                sudo=True
+                )
 
-    File(component_driver_curl_target, mode=0644)
+        File(component_driver_curl_target, mode=0644)
 
-  if policymgr_mgr_url.endswith('/'):
-    policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
+    if policymgr_mgr_url.endswith('/'):
+        policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
 
-  if stack_version_override is None:
-    stack_version = get_stack_version(component_select_name)
-  else:
-    stack_version = stack_version_override
+    if stack_version_override is None:
+        stack_version = get_stack_version(component_select_name)
+    else:
+        stack_version = stack_version_override
 
-  component_conf_dir = conf_dict
+    component_conf_dir = conf_dict
 
-  if plugin_enabled:
+    if plugin_enabled:
 
-    service_name_exist = get_policycache_service_name(service_name, repo_name, cache_service_list)
+        service_name_exist = get_policycache_service_name(service_name, repo_name, cache_service_list)
 
-    if not service_name_exist:
-      if api_version is not None and api_version == 'v2':
-        ranger_adm_obj = RangeradminV2(url=policymgr_mgr_url, skip_if_rangeradmin_down=skip_if_rangeradmin_down)
-        ranger_adm_obj.create_ranger_repository(service_name, repo_name, plugin_repo_dict,
-                                                ranger_env_properties['ranger_admin_username'], ranger_env_properties['ranger_admin_password'],
-                                                ranger_env_properties['admin_username'], ranger_env_properties['admin_password'],
-                                                policy_user, is_security_enabled, is_stack_supports_ranger_kerberos, component_user,
-                                                component_user_principal, component_user_keytab)
-      else:
-        ranger_adm_obj = Rangeradmin(url=policymgr_mgr_url, skip_if_rangeradmin_down=skip_if_rangeradmin_down)
-        ranger_adm_obj.create_ranger_repository(service_name, repo_name, plugin_repo_dict,
-                                              ranger_env_properties['ranger_admin_username'], ranger_env_properties['ranger_admin_password'],
-                                              ranger_env_properties['admin_username'], ranger_env_properties['admin_password'],
-                                              policy_user)
+        if not service_name_exist:
+            if api_version is not None and api_version == 'v2':
+                ranger_adm_obj = RangeradminV2(url=policymgr_mgr_url, skip_if_rangeradmin_down=skip_if_rangeradmin_down)
+                ranger_adm_obj.create_ranger_repository(service_name, repo_name, plugin_repo_dict,
+                                                        ranger_env_properties['ranger_admin_username'],
+                                                        ranger_env_properties['ranger_admin_password'],
+                                                        ranger_env_properties['admin_username'],
+                                                        ranger_env_properties['admin_password'],
+                                                        policy_user, is_security_enabled,
+                                                        is_stack_supports_ranger_kerberos, component_user,
+                                                        component_user_principal, component_user_keytab)
+            else:
+                ranger_adm_obj = Rangeradmin(url=policymgr_mgr_url, skip_if_rangeradmin_down=skip_if_rangeradmin_down)
+                ranger_adm_obj.create_ranger_repository(service_name, repo_name, plugin_repo_dict,
+                                                        ranger_env_properties['ranger_admin_username'],
+                                                        ranger_env_properties['ranger_admin_password'],
+                                                        ranger_env_properties['admin_username'],
+                                                        ranger_env_properties['admin_password'],
+                                                        policy_user)
 
-    current_datetime = datetime.now()
+        current_datetime = datetime.now()
+        Directory(format("{component_conf_dir}"),
+                  owner=component_user,
+                  group=component_group,
+                  mode=0775,
+                  create_parents=True
+                  )
 
-    File(format('{component_conf_dir}/ranger-security.xml'),
-      owner = component_user,
-      group = component_group,
-      mode = 0644,
-      content = InlineTemplate(format('<ranger>\n<enabled>{current_datetime}</enabled>\n</ranger>'))
-    )
+        File(format('{component_conf_dir}/ranger-security.xml'),
+             owner=component_user,
+             group=component_group,
+             mode=0644,
+             content=InlineTemplate(format('<ranger>\n<enabled>{current_datetime}</enabled>\n</ranger>'))
+             )
 
-    Directory([os.path.join('/etc', 'ranger', repo_name), os.path.join('/etc', 'ranger', repo_name, 'policycache')],
-      owner = component_user,
-      group = component_group,
-      mode=0775,
-      create_parents = True,
-      cd_access = 'a'
-    )
+        Directory([os.path.join('/etc', 'ranger', repo_name), os.path.join('/etc', 'ranger', repo_name, 'policycache')],
+                  owner=component_user,
+                  group=component_group,
+                  mode=0775,
+                  create_parents=True,
+                  cd_access='a'
+                  )
 
-    for cache_service in cache_service_list:
-      File(os.path.join('/etc', 'ranger', repo_name, 'policycache', format('{cache_service}_{repo_name}.json')),
-        owner = component_user,
-        group = component_group,
-        mode = 0644
-      )
+        for cache_service in cache_service_list:
+            File(os.path.join('/etc', 'ranger', repo_name, 'policycache', format('{cache_service}_{repo_name}.json')),
+                 owner=component_user,
+                 group=component_group,
+                 mode=0644
+                 )
 
-    # remove plain-text password from xml configs
-    plugin_audit_password_property = 'xasecure.audit.destination.db.password'
-    plugin_audit_properties_copy = {}
-    plugin_audit_properties_copy.update(plugin_audit_properties)
+        # remove plain-text password from xml configs
+        plugin_audit_password_property = 'xasecure.audit.destination.db.password'
+        plugin_audit_properties_copy = {}
+        plugin_audit_properties_copy.update(plugin_audit_properties)
 
-    if plugin_audit_password_property in plugin_audit_properties_copy:
-      plugin_audit_properties_copy[plugin_audit_password_property] = "crypted"
+        if plugin_audit_password_property in plugin_audit_properties_copy:
+            plugin_audit_properties_copy[plugin_audit_password_property] = "crypted"
 
-    XmlConfig(format('ranger-{service_name}-audit.xml'),
-      conf_dir=component_conf_dir,
-      configurations=plugin_audit_properties_copy,
-      configuration_attributes=plugin_audit_attributes,
-      owner = component_user,
-      group = component_group,
-      mode=0744)
+        XmlConfig(format('ranger-{service_name}-audit.xml'),
+                  conf_dir=component_conf_dir,
+                  configurations=plugin_audit_properties_copy,
+                  configuration_attributes=plugin_audit_attributes,
+                  owner=component_user,
+                  group=component_group,
+                  mode=0744)
 
-    XmlConfig(format('ranger-{service_name}-security.xml'),
-      conf_dir=component_conf_dir,
-      configurations=plugin_security_properties,
-      configuration_attributes=plugin_security_attributes,
-      owner = component_user,
-      group = component_group,
-      mode=0744)
+        XmlConfig(format('ranger-{service_name}-security.xml'),
+                  conf_dir=component_conf_dir,
+                  configurations=plugin_security_properties,
+                  configuration_attributes=plugin_security_attributes,
+                  owner=component_user,
+                  group=component_group,
+                  mode=0744)
 
-    # remove plain-text password from xml configs
-    plugin_password_properties = ['xasecure.policymgr.clientssl.keystore.password', 'xasecure.policymgr.clientssl.truststore.password']
-    plugin_policymgr_ssl_properties_copy = {}
-    plugin_policymgr_ssl_properties_copy.update(plugin_policymgr_ssl_properties)
+        # remove plain-text password from xml configs
+        plugin_password_properties = ['xasecure.policymgr.clientssl.keystore.password',
+                                      'xasecure.policymgr.clientssl.truststore.password']
+        plugin_policymgr_ssl_properties_copy = {}
+        plugin_policymgr_ssl_properties_copy.update(plugin_policymgr_ssl_properties)
 
-    for prop in plugin_password_properties:
-      if prop in plugin_policymgr_ssl_properties_copy:
-        plugin_policymgr_ssl_properties_copy[prop] = "crypted"
+        for prop in plugin_password_properties:
+            if prop in plugin_policymgr_ssl_properties_copy:
+                plugin_policymgr_ssl_properties_copy[prop] = "crypted"
 
-    if str(service_name).lower() == 'yarn' :
-      XmlConfig("ranger-policymgr-ssl-yarn.xml",
-        conf_dir=component_conf_dir,
-        configurations=plugin_policymgr_ssl_properties_copy,
-        configuration_attributes=plugin_policymgr_ssl_attributes,
-        owner = component_user,
-        group = component_group,
-        mode=0744)
-    else:
-      XmlConfig("ranger-policymgr-ssl.xml",
-        conf_dir=component_conf_dir,
-        configurations=plugin_policymgr_ssl_properties_copy,
-        configuration_attributes=plugin_policymgr_ssl_attributes,
-        owner = component_user,
-        group = component_group,
-        mode=0744)
+        if str(service_name).lower() == 'yarn':
+            XmlConfig("ranger-policymgr-ssl-yarn.xml",
+                      conf_dir=component_conf_dir,
+                      configurations=plugin_policymgr_ssl_properties_copy,
+                      configuration_attributes=plugin_policymgr_ssl_attributes,
+                      owner=component_user,
+                      group=component_group,
+                      mode=0744)
+        else:
+            XmlConfig("ranger-policymgr-ssl.xml",
+                      conf_dir=component_conf_dir,
+                      configurations=plugin_policymgr_ssl_properties_copy,
+                      configuration_attributes=plugin_policymgr_ssl_attributes,
+                      owner=component_user,
+                      group=component_group,
+                      mode=0744)
 
-    setup_ranger_plugin_keystore(service_name, audit_db_is_enabled, stack_version, credential_file,
-              xa_audit_db_password, ssl_truststore_password, ssl_keystore_password,
-              component_user, component_group, java_home, cred_lib_path_override, cred_setup_prefix_override)
+        setup_ranger_plugin_keystore(service_name, audit_db_is_enabled, stack_version, credential_file,
+                                     xa_audit_db_password, ssl_truststore_password, ssl_keystore_password,
+                                     component_user, component_group, java_home, cred_lib_path_override,
+                                     cred_setup_prefix_override)
 
-  else:
-    File(format('{component_conf_dir}/ranger-security.xml'),
-      action="delete"
-    )
+    else:
+        File(format('{component_conf_dir}/ranger-security.xml'),
+             action="delete"
+             )
 
+
 def setup_ranger_plugin_jar_symblink(stack_version, service_name, component_list):
-
-  stack_root = Script.get_stack_root()
-  jar_files = os.listdir(format('{stack_root}/{stack_version}/ranger-{service_name}-plugin/lib'))
+    stack_root = Script.get_stack_root()
+    jar_files = os.listdir(format('{stack_root}/{stack_version}/usr/lib/ranger-{service_name}-plugin/lib'))
 
-  for jar_file in jar_files:
-    for component in component_list:
-      Execute(('ln','-sf',format('{stack_root}/{stack_version}/ranger-{service_name}-plugin/lib/{jar_file}'),format('{stack_root}/current/{component}/lib/{jar_file}')),
-      not_if=format('ls {stack_root}/current/{component}/lib/{jar_file}'),
-      only_if=format('ls {stack_root}/{stack_version}/ranger-{service_name}-plugin/lib/{jar_file}'),
-      sudo=True)
+    for jar_file in jar_files:
+        for component in component_list:
+            Execute(('ln', '-sf',
+                     format('{stack_root}/{stack_version}/usr/lib/ranger-{service_name}-plugin/lib/{jar_file}'),
+                     format('{stack_root}/current/{component}/lib/{jar_file}')),
+                    not_if=format('ls {stack_root}/current/{component}/lib/{jar_file}'),
+                    only_if=format(
+                        'ls {stack_root}/{stack_version}/usr/lib/ranger-{service_name}-plugin/lib/{jar_file}'),
+                    sudo=True)
 
-def setup_ranger_plugin_keystore(service_name, audit_db_is_enabled, stack_version, credential_file, xa_audit_db_password,
-                                ssl_truststore_password, ssl_keystore_password, component_user, component_group, java_home, cred_lib_path_override = None, cred_setup_prefix_override = None):
 
-  stack_root = Script.get_stack_root()
-  service_name = str(service_name).lower()
+def setup_ranger_plugin_keystore(service_name, audit_db_is_enabled, stack_version, credential_file,
+                                 xa_audit_db_password,
+                                 ssl_truststore_password, ssl_keystore_password, component_user, component_group,
+                                 java_home, cred_lib_path_override=None, cred_setup_prefix_override=None):
+    stack_root = Script.get_stack_root()
+    service_name = str(service_name).lower()
 
-  if cred_lib_path_override is not None:
-    cred_lib_path = cred_lib_path_override
-  else:
-    cred_lib_path = format('{stack_root}/{stack_version}/ranger-{service_name}-plugin/install/lib/*')
+    if cred_lib_path_override is not None:
+        cred_lib_path = cred_lib_path_override
+    else:
+        cred_lib_path = format('{stack_root}/{stack_version}/usr/lib/ranger-{service_name}-plugin/install/lib/*')
 
-  if cred_setup_prefix_override is not None:
-    cred_setup_prefix = cred_setup_prefix_override
-  else:
-    cred_setup_prefix = (format('{stack_root}/{stack_version}/ranger-{service_name}-plugin/ranger_credential_helper.py'), '-l', cred_lib_path)
+    if cred_setup_prefix_override is not None:
+        cred_setup_prefix = cred_setup_prefix_override
+    else:
+        cred_setup_prefix = (
+            format('{stack_root}/{stack_version}/usr/lib/ranger-{service_name}-plugin/ranger_credential_helper.py'),
+            '-l',
+            cred_lib_path)
 
-  if audit_db_is_enabled:
-    cred_setup = cred_setup_prefix + ('-f', credential_file, '-k', 'auditDBCred', '-v', PasswordString(xa_audit_db_password), '-c', '1')
-    Execute(cred_setup, environment={'JAVA_HOME': java_home}, logoutput=True, sudo=True)
+    if audit_db_is_enabled:
+        cred_setup = cred_setup_prefix + (
+            '-f', credential_file, '-k', 'auditDBCred', '-v', PasswordString(xa_audit_db_password), '-c', '1')
+        Execute(cred_setup, environment={'JAVA_HOME': java_home}, logoutput=True, sudo=True)
 
-  cred_setup = cred_setup_prefix + ('-f', credential_file, '-k', 'sslKeyStore', '-v', PasswordString(ssl_keystore_password), '-c', '1')
-  Execute(cred_setup, environment={'JAVA_HOME': java_home}, logoutput=True, sudo=True)
+    cred_setup = cred_setup_prefix + (
+        '-f', credential_file, '-k', 'sslKeyStore', '-v', PasswordString(ssl_keystore_password), '-c', '1')
+    Execute(cred_setup, environment={'JAVA_HOME': java_home}, logoutput=True, sudo=True)
 
-  cred_setup = cred_setup_prefix + ('-f', credential_file, '-k', 'sslTrustStore', '-v', PasswordString(ssl_truststore_password), '-c', '1')
-  Execute(cred_setup, environment={'JAVA_HOME': java_home}, logoutput=True, sudo=True)
+    cred_setup = cred_setup_prefix + (
+        '-f', credential_file, '-k', 'sslTrustStore', '-v', PasswordString(ssl_truststore_password), '-c', '1')
+    Execute(cred_setup, environment={'JAVA_HOME': java_home}, logoutput=True, sudo=True)
 
-  File(credential_file,
-    owner = component_user,
-    group = component_group,
-    mode = 0640
-  )
+    File(credential_file,
+         owner=component_user,
+         group=component_group,
+         mode=0640
+         )
 
-  dot_jceks_crc_file_path = os.path.join(os.path.dirname(credential_file), "." + os.path.basename(credential_file) + ".crc")
+    dot_jceks_crc_file_path = os.path.join(os.path.dirname(credential_file),
+                                           "." + os.path.basename(credential_file) + ".crc")
 
-  File(dot_jceks_crc_file_path,
-    owner = component_user,
-    group = component_group,
-    only_if = format("test -e {dot_jceks_crc_file_path}"),
-    mode = 0640
-  )
+    File(dot_jceks_crc_file_path,
+         owner=component_user,
+         group=component_group,
+         only_if=format("test -e {dot_jceks_crc_file_path}"),
+         mode=0640
+         )
 
+
 def setup_configuration_file_for_required_plugins(component_user, component_group, create_core_site_path,
-                                                  configurations={}, configuration_attributes={}, file_name='core-site.xml',
+                                                  configurations={}, configuration_attributes={},
+                                                  file_name='core-site.xml',
                                                   xml_include_file=None, xml_include_file_content=None):
-  XmlConfig(file_name,
-    conf_dir = create_core_site_path,
-    configurations = configurations,
-    configuration_attributes = configuration_attributes,
-    owner = component_user,
-    group = component_group,
-    mode = 0644,
-    xml_include_file = xml_include_file
-  )
+    XmlConfig(file_name,
+              conf_dir=create_core_site_path,
+              configurations=configurations,
+              configuration_attributes=configuration_attributes,
+              owner=component_user,
+              group=component_group,
+              mode=0644,
+              xml_include_file=xml_include_file
+              )
 
-  if xml_include_file_content:
-    File(xml_include_file,
-         owner=component_user,
-         group=component_group,
-         content=xml_include_file_content
-         )
+    if xml_include_file_content:
+        File(xml_include_file,
+             owner=component_user,
+             group=component_group,
+             content=xml_include_file_content
+             )
 
 
 def get_audit_configs(config):
-  xa_audit_db_flavor = config['configurations']['admin-properties']['DB_FLAVOR'].lower()
-  xa_db_host = config['configurations']['admin-properties']['db_host']
-  xa_audit_db_name = default('/configurations/admin-properties/audit_db_name', 'ranger_audits')
+    xa_audit_db_flavor = config['configurations']['admin-properties']['DB_FLAVOR'].lower()
+    xa_db_host = config['configurations']['admin-properties']['db_host']
+    xa_audit_db_name = default('/configurations/admin-properties/audit_db_name', 'ranger_audits')
 
-  if xa_audit_db_flavor == 'mysql':
-    jdbc_jar_name = default("/ambariLevelParams/custom_mysql_jdbc_name", None)
-    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mysql_jdbc_name", None)
-    audit_jdbc_url = format('jdbc:mysql://{xa_db_host}/{xa_audit_db_name}')
-    jdbc_driver = "com.mysql.jdbc.Driver"
-  elif xa_audit_db_flavor == 'oracle':
-    jdbc_jar_name = default("/ambariLevelParams/custom_oracle_jdbc_name", None)
-    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_oracle_jdbc_name", None)
-    colon_count = xa_db_host.count(':')
-    if colon_count == 2 or colon_count == 0:
-      audit_jdbc_url = format('jdbc:oracle:thin:@{xa_db_host}')
-    else:
-      audit_jdbc_url = format('jdbc:oracle:thin:@//{xa_db_host}')
-    jdbc_driver = "oracle.jdbc.OracleDriver"
-  elif xa_audit_db_flavor == 'postgres':
-    jdbc_jar_name = default("/ambariLevelParams/custom_postgres_jdbc_name", None)
-    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_postgres_jdbc_name", None)
-    audit_jdbc_url = format('jdbc:postgresql://{xa_db_host}/{xa_audit_db_name}')
-    jdbc_driver = "org.postgresql.Driver"
-  elif xa_audit_db_flavor == 'mssql':
-    jdbc_jar_name = default("/ambariLevelParams/custom_mssql_jdbc_name", None)
-    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mssql_jdbc_name", None)
-    audit_jdbc_url = format('jdbc:sqlserver://{xa_db_host};databaseName={xa_audit_db_name}')
-    jdbc_driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
-  elif xa_audit_db_flavor == 'sqla':
-    jdbc_jar_name = default("/ambariLevelParams/custom_sqlanywhere_jdbc_name", None)
-    previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_sqlanywhere_jdbc_name", None)
-    audit_jdbc_url = format('jdbc:sqlanywhere:database={xa_audit_db_name};host={xa_db_host}')
-    jdbc_driver = "sap.jdbc4.sqlanywhere.IDriver"
-  else: raise Fail(format("'{xa_audit_db_flavor}' db flavor not supported."))
+    if xa_audit_db_flavor == 'mysql':
+        jdbc_jar_name = default("/ambariLevelParams/custom_mysql_jdbc_name", None)
+        previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mysql_jdbc_name", None)
+        audit_jdbc_url = format('jdbc:mysql://{xa_db_host}/{xa_audit_db_name}')
+        jdbc_driver = "com.mysql.jdbc.Driver"
+    elif xa_audit_db_flavor == 'oracle':
+        jdbc_jar_name = default("/ambariLevelParams/custom_oracle_jdbc_name", None)
+        previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_oracle_jdbc_name", None)
+        colon_count = xa_db_host.count(':')
+        if colon_count == 2 or colon_count == 0:
+            audit_jdbc_url = format('jdbc:oracle:thin:@{xa_db_host}')
+        else:
+            audit_jdbc_url = format('jdbc:oracle:thin:@//{xa_db_host}')
+        jdbc_driver = "oracle.jdbc.OracleDriver"
+    elif xa_audit_db_flavor == 'postgres':
+        jdbc_jar_name = default("/ambariLevelParams/custom_postgres_jdbc_name", None)
+        previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_postgres_jdbc_name", None)
+        audit_jdbc_url = format('jdbc:postgresql://{xa_db_host}/{xa_audit_db_name}')
+        jdbc_driver = "org.postgresql.Driver"
+    elif xa_audit_db_flavor == 'mssql':
+        jdbc_jar_name = default("/ambariLevelParams/custom_mssql_jdbc_name", None)
+        previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mssql_jdbc_name", None)
+        audit_jdbc_url = format('jdbc:sqlserver://{xa_db_host};databaseName={xa_audit_db_name}')
+        jdbc_driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
+    elif xa_audit_db_flavor == 'sqla':
+        jdbc_jar_name = default("/ambariLevelParams/custom_sqlanywhere_jdbc_name", None)
+        previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_sqlanywhere_jdbc_name", None)
+        audit_jdbc_url = format('jdbc:sqlanywhere:database={xa_audit_db_name};host={xa_db_host}')
+        jdbc_driver = "sap.jdbc4.sqlanywhere.IDriver"
+    else:
+        raise Fail(format("'{xa_audit_db_flavor}' db flavor not supported."))
 
-  return jdbc_jar_name, previous_jdbc_jar_name, audit_jdbc_url, jdbc_driver
+    return jdbc_jar_name, previous_jdbc_jar_name, audit_jdbc_url, jdbc_driver
 
+
 def generate_ranger_service_config(ranger_plugin_properties):
-  custom_service_config_dict = {}
-  ranger_plugin_properties_copy = {}
-  ranger_plugin_properties_copy.update(ranger_plugin_properties)
+    custom_service_config_dict = {}
+    ranger_plugin_properties_copy = {}
+    ranger_plugin_properties_copy.update(ranger_plugin_properties)
 
-  for key, value in ranger_plugin_properties_copy.iteritems():
-    if key.startswith("ranger.service.config.param."):
-      modify_key_name = key.replace("ranger.service.config.param.","")
-      custom_service_config_dict[modify_key_name] = value
+    for key, value in ranger_plugin_properties_copy.iteritems():
+        if key.startswith("ranger.service.config.param."):
+            modify_key_name = key.replace("ranger.service.config.param.", "")
+            custom_service_config_dict[modify_key_name] = value
 
-  return custom_service_config_dict
+    return custom_service_config_dict
 
+
 def get_policycache_service_name(service_name, repo_name, cache_service_list):
-  service_name_exist_flag = False
-  policycache_path = os.path.join('/etc', 'ranger', repo_name, 'policycache')
-  try:
-    for cache_service in cache_service_list:
-      policycache_json_file = format('{policycache_path}/{cache_service}_{repo_name}.json')
-      if os.path.isfile(policycache_json_file) and os.path.getsize(policycache_json_file) > 0:
-        with open(policycache_json_file) as json_file:
-          json_data = json.load(json_file)
-          if 'serviceName' in json_data and json_data['serviceName'] == repo_name:
-            Logger.info("Skipping Ranger API calls, as policy cache file exists for {0}".format(service_name))
-            Logger.warning("If service name for {0} is not created on Ranger Admin, then to re-create it delete policy cache file: {1}".format(service_name, policycache_json_file))
-            service_name_exist_flag = True
-            break
-  except Exception, err:
-    Logger.error("Error occurred while fetching service name from policy cache file.\nError: {0}".format(err))
+    service_name_exist_flag = False
+    policycache_path = os.path.join('/etc', 'ranger', repo_name, 'policycache')
+    try:
+        for cache_service in cache_service_list:
+            policycache_json_file = format('{policycache_path}/{cache_service}_{repo_name}.json')
+            if os.path.isfile(policycache_json_file) and os.path.getsize(policycache_json_file) > 0:
+                with open(policycache_json_file) as json_file:
+                    json_data = json.load(json_file)
+                    if 'serviceName' in json_data and json_data['serviceName'] == repo_name:
+                        Logger.info(
+                            "Skipping Ranger API calls, as policy cache file exists for {0}".format(service_name))
+                        Logger.warning(
+                            "If service name for {0} is not created on Ranger Admin, then to re-create it delete policy cache file: {1}".format(
+                                service_name, policycache_json_file))
+                        service_name_exist_flag = True
+                        break
+    except Exception, err:
+        Logger.error("Error occurred while fetching service name from policy cache file.\nError: {0}".format(err))
 
-  return service_name_exist_flag
+    return service_name_exist_flag
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/scripts/job_history_server.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/scripts/job_history_server.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/scripts/job_history_server.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/scripts/job_history_server.py	(date 1719626239000)
@@ -0,0 +1,93 @@
+#!/usr/bin/python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import sys
+import os
+
+from resource_management.libraries.script.script import Script
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions.copy_tarball import copy_to_hdfs
+from resource_management.libraries.functions.check_process_status import check_process_status
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions.constants import StackFeature
+from resource_management.core.logger import Logger
+from resource_management.core import shell
+from setup_flink import *
+from flink_service import flink_service
+
+
+class JobHistoryServer(Script):
+
+  def install(self, env):
+    import params
+    env.set_params(params)
+    
+    self.install_packages(env)
+    
+  def configure(self, env, upgrade_type=None, config_dir=None):
+    import params
+    env.set_params(params)
+    
+    setup_flink(env, 'server', upgrade_type=upgrade_type, action = 'config')
+    
+  def start(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+    
+    self.configure(env)
+    flink_service('jobhistoryserver', upgrade_type=upgrade_type, action='start')
+
+  def stop(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+    
+    flink_service('jobhistoryserver', upgrade_type=upgrade_type, action='stop')
+
+  def status(self, env):
+    import status_params
+    env.set_params(status_params)
+
+    check_process_status(status_params.flink_history_server_pid_file)
+    
+
+  def pre_upgrade_restart(self, env, upgrade_type=None):
+    import params
+
+    env.set_params(params)
+    if params.version and check_stack_feature(StackFeature.ROLLING_UPGRADE, params.version):
+      Logger.info("Executing Flink Job History Server Stack Upgrade pre-restart")
+      stack_select.select_packages(params.version)
+
+ 
+          
+  def get_log_folder(self):
+    import params
+    return params.flink_log_dir
+  
+  def get_user(self):
+    import params
+    return params.flink_user
+
+  def get_pid_files(self):
+    import status_params
+    return [status_params.flink_history_server_pid_file]
+
+if __name__ == "__main__":
+  JobHistoryServer().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/templates/test_init.sql.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/templates/test_init.sql.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/templates/test_init.sql.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/package/templates/test_init.sql.j2	(date 1719626239000)
@@ -0,0 +1,35 @@
+CREATE CATALOG hive_catalog WITH (
+  'type' = 'hive',
+  'hive-conf-dir' = '/etc/hive/conf/'
+);
+
+CREATE CATALOG iceberg_hive_catalog WITH (
+  'type'='iceberg',
+  'catalog-type'='hive',
+  'hive-conf-dir' = '/etc/hive/conf',
+  'uri'='{{hive_metastore_uris}}',
+  'clients'='5',
+  'property-version'='1',
+  'warehouse'='{{hive_metastore_warehouse_dir_hdfs}}/tablespace/managed/hive'
+);
+
+CREATE CATALOG iceberg_hadoop_catalog WITH (
+  'type'='iceberg',
+  'catalog-type'='hadoop',  
+  'property-version'='1',
+  'warehouse'='{{hive_metastore_warehouse_dir_hdfs}}/tablespace/managed/hive'
+);
+
+CREATE CATALOG paimon_hive_catalog WITH (
+  'type' = 'paimon',
+  'metastore' = 'hive',
+  'uri' = '{{hive_metastore_uris}}',
+  'hive-conf-dir' = '/etc/hive/conf',
+  'warehouse' = '{{hive_metastore_warehouse_dir_hdfs}}/tablespace/managed/hive'
+);
+
+CREATE CATALOG paimon_hadoop_catalog WITH (
+    'type'='paimon',
+    'warehouse'='{{hive_metastore_warehouse_dir_hdfs}}/tablespace/managed/hive'
+);
+
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback.xml	(date 1719626239000)
@@ -0,0 +1,94 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <property>
+    <name>content</name>
+    <description>Flink-logback-Xml</description>
+    <value><![CDATA[
+<!--
+  ~ Licensed to the Apache Software Foundation (ASF) under one
+  ~ or more contributor license agreements.  See the NOTICE file
+  ~ distributed with this work for additional information
+  ~ regarding copyright ownership.  The ASF licenses this file
+  ~ to you under the Apache License, Version 2.0 (the
+  ~ "License"); you may not use this file except in compliance
+  ~ with the License.  You may obtain a copy of the License at
+  ~
+  ~     http://www.apache.org/licenses/LICENSE-2.0
+  ~
+  ~ Unless required by applicable law or agreed to in writing, software
+  ~ distributed under the License is distributed on an "AS IS" BASIS,
+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  ~ See the License for the specific language governing permissions and
+  ~ limitations under the License.
+  -->
+
+<configuration>
+    <appender name="file" class="ch.qos.logback.core.FileAppender">
+        <file>${log.file}</file>
+        <append>false</append>
+        <encoder>
+            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
+        </encoder>
+    </appender>
+
+    <!-- This affects logging for both user code and Flink -->
+    <root level="INFO">
+        <appender-ref ref="file"/>
+    </root>
+
+    <!-- Uncomment this if you want to only change Flink's logging -->
+    <!--<logger name="org.apache.flink" level="INFO">-->
+        <!--<appender-ref ref="file"/>-->
+    <!--</logger>-->
+
+    <!-- The following lines keep the log level of common libraries/connectors on
+         log level INFO. The root logger does not override this. You have to manually
+         change the log levels here. -->
+    <logger name="akka" level="INFO">
+        <appender-ref ref="file"/>
+    </logger>
+    <logger name="org.apache.kafka" level="INFO">
+        <appender-ref ref="file"/>
+    </logger>
+    <logger name="org.apache.hadoop" level="INFO">
+        <appender-ref ref="file"/>
+    </logger>
+    <logger name="org.apache.zookeeper" level="INFO">
+        <appender-ref ref="file"/>
+    </logger>
+
+    <!-- Suppress the irrelevant (wrong) warnings from the Netty channel handler -->
+    <logger name="org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR">
+        <appender-ref ref="file"/>
+    </logger>
+</configuration>
+
+
+]]>
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="true"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/solr.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/solr.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/solr.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/solr.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/solr.py	(date 1723880898292)
@@ -1,3 +1,4 @@
+# coding=utf-8
 """
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
@@ -18,6 +19,8 @@
 """
 
 import sys
+import time
+
 from ambari_commons.repo_manager import ManagerFactory
 from ambari_commons.shell import RepoCallContext
 from resource_management.core.logger import Logger
@@ -36,128 +39,136 @@
 from resource_management.libraries.functions.stack_features import check_stack_feature
 
 from setup_solr import setup_solr, setup_solr_znode_env
+import setup_ranger_config_for_solr
+
 
 class Solr(Script):
-  def install(self, env):
-    import params
-    env.set_params(params)
-    self.install_packages(env)
+    def install(self, env):
+        import params
+        env.set_params(params)
+        self.install_packages(env)
 
-  def configure(self, env, upgrade_type=None):
-    import params
-    env.set_params(params)
+    def configure(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
 
-    setup_solr(name = 'server')
+        setup_solr(name='server')
 
-  def start(self, env, upgrade_type=None):
-    import params
-    env.set_params(params)
-    self.configure(env)
-    setup_solr_znode_env()
-    
-    start_cmd = format('{solr_bindir}/solr start -cloud -noprompt -s {solr_datadir} -z {zookeeper_quorum}{solr_znode} -Dsolr.kerberos.name.rules=\'{solr_kerberos_name_rules}\' 2>&1') \
-            if params.security_enabled else format('{solr_bindir}/solr start -cloud -noprompt -s {solr_datadir} -z {zookeeper_quorum}{solr_znode}  2>&1')
+    def start(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        self.configure(env)
+        setup_solr_znode_env()
+
+        start_cmd = format(
+            '{solr_bindir}/solr start -cloud -noprompt -s {solr_datadir} -z {zookeeper_quorum}{solr_znode} -Dsolr.kerberos.name.rules=\'{solr_kerberos_name_rules}\' 2>&1') \
+            if params.security_enabled else format(
+            '{solr_bindir}/solr start -cloud -noprompt -s {solr_datadir} -z {zookeeper_quorum}{solr_znode}  2>&1')
 
-    check_process = format("{sudo} test -f {solr_pidfile} && {sudo} pgrep -F {solr_pidfile}")
+        check_process = format("{sudo} test -f {solr_pidfile} && {sudo} pgrep -F {solr_pidfile}")
 
-    piped_start_cmd = format('{start_cmd} | tee {solr_log}') + '; (exit "${PIPESTATUS[0]}")'
-    Execute(
-      piped_start_cmd,
-      environment={'SOLR_INCLUDE': format('{solr_conf}/solr-env.sh')},
-      user=params.solr_user,
-      not_if=check_process,
-      logoutput=True
-    )
+        piped_start_cmd = format('{start_cmd} | tee {solr_log}') + '; (exit "${PIPESTATUS[0]}")'
+        Execute(
+            piped_start_cmd,
+            environment={'SOLR_INCLUDE': format('{solr_conf}/solr-env.sh')},
+            user=params.solr_user,
+            not_if=check_process,
+            logoutput=True
+        )
 
-  def stop(self, env, upgrade_type=None):
-    import params
-    env.set_params(params)
+        time.sleep(10)
+        setup_ranger_config_for_solr.setup_ranger_collection()
+    def stop(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
 
-    try:
-      stop_cmd=format('{solr_bindir}/solr stop -all')
-      piped_stop_cmd=format('{stop_cmd} | tee {solr_log}') + '; (exit "${PIPESTATUS[0]}")'
-      Execute(piped_stop_cmd,
-              environment={'SOLR_INCLUDE': format('{solr_conf}/solr-env.sh')},
-              user=params.solr_user,
-              logoutput=True
-              )
+        try:
+            stop_cmd = format('{solr_bindir}/solr stop -all')
+            piped_stop_cmd = format('{stop_cmd} | tee {solr_log}') + '; (exit "${PIPESTATUS[0]}")'
+            Execute(piped_stop_cmd,
+                    environment={'SOLR_INCLUDE': format('{solr_conf}/solr-env.sh')},
+                    user=params.solr_user,
+                    logoutput=True
+                    )
 
-      File(params.prev_solr_pidfile,
-           action="delete"
-           )
-    except:
-      Logger.warning("Could not stop solr:" + str(sys.exc_info()[1]) + "\n Trying to kill it")
-      self.kill_process(params.prev_solr_pidfile, params.solr_user, params.solr_log_dir)
+            File(params.prev_solr_pidfile,
+                 action="delete"
+                 )
+        except:
+            Logger.warning("Could not stop solr:" + str(sys.exc_info()[1]) + "\n Trying to kill it")
+            self.kill_process(params.prev_solr_pidfile, params.solr_user, params.solr_log_dir)
 
-  def status(self, env):
-    import status_params
-    env.set_params(status_params)
+    def status(self, env):
+        import status_params
+        env.set_params(status_params)
 
-    check_process_status(status_params.solr_pidfile)
+        check_process_status(status_params.solr_pidfile)
 
-  def kill_process(self, pid_file, user, log_dir):
-    """
-    Kill the process by pid file, then check the process is running or not. If the process is still running after the kill
-    command, it will try to kill with -9 option (hard kill)
-    """
-    pid = get_user_call_output(format("cat {pid_file}"), user=user, is_checked_call=False)[1]
-    process_id_exists_command = format("ls {pid_file} >/dev/null 2>&1 && ps -p {pid} >/dev/null 2>&1")
+    def kill_process(self, pid_file, user, log_dir):
+        """
+        Kill the process by pid file, then check the process is running or not. If the process is still running after the kill
+        command, it will try to kill with -9 option (hard kill)
+        """
+        pid = get_user_call_output(format("cat {pid_file}"), user=user, is_checked_call=False)[1]
+        process_id_exists_command = format("ls {pid_file} >/dev/null 2>&1 && ps -p {pid} >/dev/null 2>&1")
 
-    kill_cmd = format("{sudo} kill {pid}")
-    Execute(kill_cmd,
-          not_if=format("! ({process_id_exists_command})"))
-    wait_time = 5
+        kill_cmd = format("{sudo} kill {pid}")
+        Execute(kill_cmd,
+                not_if=format("! ({process_id_exists_command})"))
+        wait_time = 5
 
-    hard_kill_cmd = format("{sudo} kill -9 {pid}")
-    Execute(hard_kill_cmd,
-          not_if=format("! ({process_id_exists_command}) || ( sleep {wait_time} && ! ({process_id_exists_command}) )"),
-          ignore_failures=True)
-    try:
-      Execute(format("! ({process_id_exists_command})"),
-            tries=20,
-            try_sleep=3,
-            )
-    except:
-      show_logs(log_dir, user)
-      raise
+        hard_kill_cmd = format("{sudo} kill -9 {pid}")
+        Execute(hard_kill_cmd,
+                not_if=format(
+                    "! ({process_id_exists_command}) || ( sleep {wait_time} && ! ({process_id_exists_command}) )"),
+                ignore_failures=True)
+        try:
+            Execute(format("! ({process_id_exists_command})"),
+                    tries=20,
+                    try_sleep=3,
+                    )
+        except:
+            show_logs(log_dir, user)
+            raise
 
-    File(pid_file,
-       action="delete"
-       )
+        File(pid_file,
+             action="delete"
+             )
 
-  def disable_security(self, env):
-    import params
-    if not params.solr_znode:
-      Logger.info("Skipping reverting ACL")
-      return
-    zkmigrator = ZkMigrator(
-      zk_host=params.zk_quorum,
-      java_exec=params.java_exec,
-      java_home=params.java64_home,
-      jaas_file=params.solr_jaas_file,
-      user=params.solr_user)
-    zkmigrator.set_acls(params.solr_znode, 'world:anyone:crdwa')
+    def disable_security(self, env):
+        import params
+        if not params.solr_znode:
+            Logger.info("Skipping reverting ACL")
+            return
+        zkmigrator = ZkMigrator(
+            zk_host=params.zk_quorum,
+            java_exec=params.java_exec,
+            java_home=params.java64_home,
+            jaas_file=params.solr_jaas_file,
+            user=params.solr_user)
+        zkmigrator.set_acls(params.solr_znode, 'world:anyone:crdwa')
 
-  def get_log_folder(self):
-    import params
-    return params.solr_log_dir
+    def get_log_folder(self):
+        import params
+        return params.solr_log_dir
 
-  def get_user(self):
-    import params
-    return params.solr_user
+    def get_user(self):
+        import params
+        return params.solr_user
 
-  def get_pid_files(self):
-    import status_params
-    return [status_params.solr_pidfile]
+    def get_pid_files(self):
+        import status_params
+        return [status_params.solr_pidfile]
 
-  def pre_upgrade_restart(self, env, upgrade_type=None):
-    Logger.info("Executing Solr Stack Upgrade pre-restart")
-    import params
+    def pre_upgrade_restart(self, env, upgrade_type=None):
+        Logger.info("Executing Solr Stack Upgrade pre-restart")
+        import params
 
-    env.set_params(params)
+        env.set_params(params)
 
-    if params.stack_version and check_stack_feature(StackFeature.ROLLING_UPGRADE, params.stack_version):
-      stack_select.select_packages(params.stack_version)
+        if params.stack_version and check_stack_feature(StackFeature.ROLLING_UPGRADE, params.stack_version):
+            stack_select.select_packages(params.stack_version)
 
+
 if __name__ == "__main__":
-  Solr().execute()
\ No newline at end of file
+    Solr().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-console.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-console.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-console.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-console.xml	(date 1719626239000)
@@ -0,0 +1,99 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <property>
+    <name>content</name>
+    <description>Flink-logback-console-Xml</description>
+    <value><![CDATA[
+<!--
+  ~ Licensed to the Apache Software Foundation (ASF) under one
+  ~ or more contributor license agreements.  See the NOTICE file
+  ~ distributed with this work for additional information
+  ~ regarding copyright ownership.  The ASF licenses this file
+  ~ to you under the Apache License, Version 2.0 (the
+  ~ "License"); you may not use this file except in compliance
+  ~ with the License.  You may obtain a copy of the License at
+  ~
+  ~     http://www.apache.org/licenses/LICENSE-2.0
+  ~
+  ~ Unless required by applicable law or agreed to in writing, software
+  ~ distributed under the License is distributed on an "AS IS" BASIS,
+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  ~ See the License for the specific language governing permissions and
+  ~ limitations under the License.
+  -->
+
+<configuration>
+    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
+        <encoder>
+            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
+        </encoder>
+    </appender>
+
+    <appender name="rolling" class="ch.qos.logback.core.rolling.RollingFileAppender">
+        <file>${log.file}</file>
+        <append>false</append>
+
+        <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
+            <fileNamePattern>${log.file}.%i</fileNamePattern>
+            <minIndex>1</minIndex>
+            <maxIndex>10</maxIndex>
+        </rollingPolicy>
+
+        <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
+            <maxFileSize>100MB</maxFileSize>
+        </triggeringPolicy>
+
+        <encoder>
+            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
+        </encoder>
+    </appender>
+
+    <!-- This affects logging for both user code and Flink -->
+    <root level="INFO">
+        <appender-ref ref="console"/>
+        <appender-ref ref="rolling"/>
+    </root>
+
+    <!-- Uncomment this if you want to only change Flink's logging -->
+    <!--<logger name="org.apache.flink" level="INFO"/>-->
+
+    <!-- The following lines keep the log level of common libraries/connectors on
+         log level INFO. The root logger does not override this. You have to manually
+         change the log levels here. -->
+    <logger name="akka" level="INFO"/>
+    <logger name="org.apache.kafka" level="INFO"/>
+    <logger name="org.apache.hadoop" level="INFO"/>
+    <logger name="org.apache.zookeeper" level="INFO"/>
+
+    <!-- Suppress the irrelevant (wrong) warnings from the Netty channel handler -->
+    <logger name="org.jboss.netty.channel.DefaultChannelPipeline" level="ERROR"/>
+</configuration>
+
+]]>
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="true"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/params.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/params.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/params.py	(date 1719626239000)
@@ -64,6 +64,8 @@
 
 solr_conf = "/etc/solr/conf"
 
+ranger_audit_conf = format('{solr_conf}/audit_conf')
+
 solr_port = status_params.solr_port
 solr_piddir = status_params.solr_piddir
 solr_pidfile = status_params.solr_pidfile
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-session.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-session.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-session.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/configuration/flink-logback-session.xml	(date 1719626239000)
@@ -0,0 +1,75 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <property>
+    <name>content</name>
+    <description>Flink-logback-session-Xml</description>
+    <value><![CDATA[
+<!--
+  ~ Licensed to the Apache Software Foundation (ASF) under one
+  ~ or more contributor license agreements.  See the NOTICE file
+  ~ distributed with this work for additional information
+  ~ regarding copyright ownership.  The ASF licenses this file
+  ~ to you under the Apache License, Version 2.0 (the
+  ~ "License"); you may not use this file except in compliance
+  ~ with the License.  You may obtain a copy of the License at
+  ~
+  ~     http://www.apache.org/licenses/LICENSE-2.0
+  ~
+  ~ Unless required by applicable law or agreed to in writing, software
+  ~ distributed under the License is distributed on an "AS IS" BASIS,
+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  ~ See the License for the specific language governing permissions and
+  ~ limitations under the License.
+  -->
+
+<configuration>
+    <appender name="file" class="ch.qos.logback.core.FileAppender">
+        <file>${log.file}</file>
+        <append>false</append>
+        <encoder>
+            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
+        </encoder>
+    </appender>
+
+    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
+        <encoder>
+            <pattern>%d{yyyy-MM-dd HH:mm:ss} %-5level %logger{60} %X{sourceThread} - %msg%n</pattern>
+        </encoder>
+    </appender>
+
+    <logger name="ch.qos.logback" level="WARN" />
+    <root level="INFO">
+        <appender-ref ref="file"/>
+        <appender-ref ref="console"/>
+    </root>
+</configuration>
+
+
+]]>
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="true"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_solr.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_solr.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_solr.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_solr.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/SOLR/package/scripts/setup_solr.py	(date 1723880161534)
@@ -1,3 +1,4 @@
+# coding=utf-8
 """
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
@@ -26,103 +27,132 @@
 from resource_management.libraries.functions.decorator import retry
 from resource_management.libraries.functions.format import format
 
-def setup_solr(name = None):
-  import params
+
+def setup_solr(name=None):
+    import params
 
-  if name == 'server':
-    Directory([params.solr_log_dir, params.solr_piddir,
-               params.solr_datadir, params.solr_data_resources_dir],
-              mode=0755,
-              cd_access='a',
-              create_parents=True,
-              owner=params.solr_user,
-              group=params.user_group
-              )
+    if name == 'server':
+        Directory([params.solr_log_dir, params.solr_piddir,
+                   params.solr_datadir, params.solr_data_resources_dir],
+                  mode=0755,
+                  cd_access='a',
+                  create_parents=True,
+                  owner=params.solr_user,
+                  group=params.user_group
+                  )
 
-    Directory([params.solr_dir, params.solr_conf],
-              mode=0755,
-              cd_access='a',
-              owner=params.solr_user,
-              group=params.user_group,
-              create_parents=True,
-              recursive_ownership=True
-              )
+        Directory([params.solr_dir, params.solr_conf],
+                  mode=0755,
+                  cd_access='a',
+                  owner=params.solr_user,
+                  group=params.user_group,
+                  create_parents=True,
+                  recursive_ownership=True
+                  )
 
-    File(params.solr_log,
-         mode=0644,
-         owner=params.solr_user,
-         group=params.user_group,
-         content=''
-         )
+        File(params.solr_log,
+             mode=0644,
+             owner=params.solr_user,
+             group=params.user_group,
+             content=''
+             )
 
-    File(format("{solr_conf}/solr-env.sh"),
-         content=InlineTemplate(params.solr_env_content),
-         mode=0755,
-         owner=params.solr_user,
-         group=params.user_group
-         )
+        File(format("{solr_conf}/solr-env.sh"),
+             content=InlineTemplate(params.solr_env_content),
+             mode=0755,
+             owner=params.solr_user,
+             group=params.user_group
+             )
 
-    File(format("{solr_datadir}/solr.xml"),
-         content=InlineTemplate(params.solr_xml_content),
-         owner=params.solr_user,
-         group=params.user_group
-         )
+        File(format("{solr_datadir}/solr.xml"),
+             content=InlineTemplate(params.solr_xml_content),
+             owner=params.solr_user,
+             group=params.user_group
+             )
 
-    File(format("{solr_conf}/log4j2.xml"),
-         content=InlineTemplate(params.solr_log4j_content),
-         owner=params.solr_user,
-         group=params.user_group
-         )
+        File(format("{solr_conf}/log4j2.xml"),
+             content=InlineTemplate(params.solr_log4j_content),
+             owner=params.solr_user,
+             group=params.user_group
+             )
 
-    custom_security_json_location = format("{solr_conf}/custom-security.json")
-    File(custom_security_json_location,
-         content=InlineTemplate(params.solr_security_json_content),
-         owner=params.solr_user,
-         group=params.user_group,
-         mode=0640
-         )
+        custom_security_json_location = format("{solr_conf}/custom-security.json")
+        File(custom_security_json_location,
+             content=InlineTemplate(params.solr_security_json_content),
+             owner=params.solr_user,
+             group=params.user_group,
+             mode=0640
+             )
 
-    if params.security_enabled:
-      File(format("{solr_jaas_file}"),
-           content=Template("solr_jaas.conf.j2"),
-           owner=params.solr_user)
+        if params.security_enabled:
+            File(format("{solr_jaas_file}"),
+                 content=Template("solr_jaas.conf.j2"),
+                 owner=params.solr_user)
 
-      File(format("{solr_conf}/security.json"),
-           content=Template("solr-security.json.j2"),
-           owner=params.solr_user,
-           group=params.user_group,
-           mode=0640)
-    if os.path.exists(params.limits_conf_dir):
-      File(os.path.join(params.limits_conf_dir, 'solr.conf'),
-           owner='root',
-           group='root',
-           mode=0644,
-           content=Template("solr.conf.j2")
-      )
+            File(format("{solr_conf}/security.json"),
+                 content=Template("solr-security.json.j2"),
+                 owner=params.solr_user,
+                 group=params.user_group,
+                 mode=0640)
+        if os.path.exists(params.limits_conf_dir):
+            File(os.path.join(params.limits_conf_dir, 'solr.conf'),
+                 owner='root',
+                 group='root',
+                 mode=0644,
+                 content=Template("solr.conf.j2")
+                 )
+        # 创建根目录
+        Directory(format('{ranger_audit_conf}'),
+                  mode=0755,
+                  create_parents=True,
+                  owner=params.solr_user,
+                  group=params.user_group
+                  )
+
+        File(format("{ranger_audit_conf}/elevate.xml"),
+             content=Template("ranger_audit/elevate.xml.j2"),
+             mode=0755,
+             owner=params.solr_user,
+             group=params.user_group
+             )
+        File(format("{ranger_audit_conf}/managed-schema"),
+             content=Template("ranger_audit/managed-schema.j2"),
+             mode=0755,
+             owner=params.solr_user,
+             group=params.user_group
+             )
+        File(format("{ranger_audit_conf}/solrconfig.xml"),
+             content=Template("ranger_audit/solrconfig.xml.j2"),
+             mode=0755,
+             owner=params.solr_user,
+             group=params.user_group
+             )
 
-  elif name == 'client':
-    solr_cloud_util.setup_solr_client(params.config)
+    elif name == 'client':
+        solr_cloud_util.setup_solr_client(params.config)
 
-  else :
-    raise Fail('Nor client or server were selected to install.')
+    else:
+        raise Fail('Nor client or server were selected to install.')
 
+
 def setup_solr_znode_env():
-  """
-  Setup SSL, ACL and authentication / authorization related Zookeeper settings for Solr (checkout: /clustersprops.json and /security.json)
-  """
-  import params
+    """
+    Setup SSL, ACL and authentication / authorization related Zookeeper settings for Solr (checkout: /clustersprops.json and /security.json)
+    """
+    import params
 
-  custom_security_json_location = format("{solr_conf}/custom-security.json")
-  jaas_file = params.solr_jaas_file if params.security_enabled else None
-  java_opts = params.zk_security_opts if params.security_enabled else None
-  url_scheme = 'https' if params.solr_ssl_enabled else 'http'
+    custom_security_json_location = format("{solr_conf}/custom-security.json")
+    jaas_file = params.solr_jaas_file if params.security_enabled else None
+    java_opts = params.zk_security_opts if params.security_enabled else None
+    url_scheme = 'https' if params.solr_ssl_enabled else 'http'
 
-  security_json_file_location = custom_security_json_location \
-    if params.solr_security_json_content and str(params.solr_security_json_content).strip() \
-    else format("{solr_conf}/security.json") # security.json file to upload
+    security_json_file_location = custom_security_json_location \
+        if params.solr_security_json_content and str(params.solr_security_json_content).strip() \
+        else format("{solr_conf}/security.json")  # security.json file to upload
 
-  create_solr_znode(java_opts, jaas_file)
+    create_solr_znode(java_opts, jaas_file)
 
+
 #   solr_cloud_util.set_cluster_prop(
 #     zookeeper_quorum=params.zk_quorum,
 #     solr_znode=params.solr_znode,
@@ -154,27 +184,28 @@
 #     )
 
 def create_solr_znode(java_opts, jaas_file):
-  import params
+    import params
 
-  create_cmd = format('{solr_bindir}/solr zk mkroot {solr_znode} -z {zookeeper_quorum} -Dsolr.kerberos.name.rules=\'{solr_kerberos_name_rules}\' 2>&1') \
-            if params.security_enabled else format('{solr_bindir}/solr zk mkroot {solr_znode} -z {zookeeper_quorum} 2>&1')
+    create_cmd = format(
+        '{solr_bindir}/solr zk mkroot {solr_znode} -z {zookeeper_quorum} -Dsolr.kerberos.name.rules=\'{solr_kerberos_name_rules}\' 2>&1') \
+        if params.security_enabled else format('{solr_bindir}/solr zk mkroot {solr_znode} -z {zookeeper_quorum} 2>&1')
 
-  try:
-    Execute(
-      create_cmd,
-      environment={'SOLR_INCLUDE': format('{solr_conf}/solr-env.sh')},
-      user=params.solr_user
-    )
-  except ExecutionFailed as e:
-    if (format("NodeExists for {solr_znode}") in str(e)):
-      Logger.info(format("Node {solr_znode} already exists."))
-    else:
-      raise e
-  except Exception as e:
-    raise e
+    try:
+        Execute(
+            create_cmd,
+            environment={'SOLR_INCLUDE': format('{solr_conf}/solr-env.sh')},
+            user=params.solr_user
+        )
+    except ExecutionFailed as e:
+        if (format("NodeExists for {solr_znode}") in str(e)):
+            Logger.info(format("Node {solr_znode} already exists."))
+        else:
+            raise e
+    except Exception as e:
+        raise e
 
-  # solr_cloud_util.create_znode(
-  #   zookeeper_quorum=params.zk_quorum,
-  #   solr_znode=params.solr_znode,
-  #   java64_home=params.java64_home,
-  #   retry=30, interval=5, java_opts=java_opts, jaas_file=jaas_file)
\ No newline at end of file
+    # solr_cloud_util.create_znode(
+    #   zookeeper_quorum=params.zk_quorum,
+    #   solr_znode=params.solr_znode,
+    #   java64_home=params.java64_home,
+    #   retry=30, interval=5, java_opts=java_opts, jaas_file=jaas_file)
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/service_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/service_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/service_advisor.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/FLINK/service_advisor.py	(date 1719626239000)
@@ -0,0 +1,188 @@
+#!/usr/bin/env ambari-python-wrap
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+# Python imports
+import imp
+import os
+import traceback
+import re
+import socket
+import fnmatch
+
+
+from resource_management.core.logger import Logger
+
+SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
+STACKS_DIR = os.path.join(SCRIPT_DIR, '../../../../../stacks/')
+PARENT_FILE = os.path.join(STACKS_DIR, 'service_advisor.py')
+
+try:
+  if "BASE_SERVICE_ADVISOR" in os.environ:
+    PARENT_FILE = os.environ["BASE_SERVICE_ADVISOR"]
+  with open(PARENT_FILE, 'rb') as fp:
+    service_advisor = imp.load_module('service_advisor', fp, PARENT_FILE, ('.py', 'rb', imp.PY_SOURCE))
+except Exception as e:
+  traceback.print_exc()
+  print "Failed to load parent"
+
+
+#DB_TYPE_DEFAULT_PORT_MAP = {"MYSQL":"3306", "ORACLE":"1521", "POSTGRES":"5432", "MSSQL":"1433", "SQLA":"2638"}
+
+class FlinkServiceAdvisor(service_advisor.ServiceAdvisor):
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(FlinkServiceAdvisor, self)
+    self.as_super.__init__(*args, **kwargs)
+
+    # Always call these methods
+    self.modifyMastersWithMultipleInstances()
+    self.modifyCardinalitiesDict()
+    self.modifyHeapSizeProperties()
+    self.modifyNotValuableComponents()
+    self.modifyComponentsNotPreferableOnServer()
+    self.modifyComponentLayoutSchemes()
+
+  def modifyMastersWithMultipleInstances(self):
+    """
+    Modify the set of masters with multiple instances.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyCardinalitiesDict(self):
+    """
+    Modify the dictionary of cardinalities.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyHeapSizeProperties(self):
+    """
+    Modify the dictionary of heap size properties.
+    Must be overriden in child class.
+    """
+    pass
+
+  def modifyNotValuableComponents(self):
+    """
+    Modify the set of components whose host assignment is based on other services.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentsNotPreferableOnServer(self):
+    """
+    Modify the set of components that are not preferable on the server.
+    Must be overriden in child class.
+    """
+    # Nothing to do
+    pass
+
+  def modifyComponentLayoutSchemes(self):
+    """
+    Modify layout scheme dictionaries for components.
+    The scheme dictionary basically maps the number of hosts to
+    host index where component should exist.
+    Must be overriden in child class.
+    """
+
+   # Nothing to do
+    pass
+
+
+  def getServiceComponentLayoutValidations(self, services, hosts):
+    """
+    Get a list of errors.
+    Must be overriden in child class.
+    """
+
+    return self.getServiceComponentCardinalityValidations(services, hosts, "OZONE")
+
+  def getServiceConfigurationRecommendations(self, configurations, clusterData, services, hosts):
+    """
+    Entry point.
+    Must be overriden in child class.
+    """
+    # Logger.info("Class: %s, Method: %s. Recommending Service Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    recommender = FlinkRecommender()
+    recommender.recommendFlinkConfigurationsFromHDP33(configurations, clusterData, services, hosts)
+
+
+  # def getServiceConfigurationRecommendationsForSSO(self, configurations, clusterData, services, hosts):
+  #   """
+  #   Entry point.
+  #   Must be overriden in child class.
+  #   """
+  #   recommender = FlinkRecommender()
+  #   recommender.recommendConfigurationsForSSO(configurations, clusterData, services, hosts)
+
+  def getServiceConfigurationsValidationItems(self, configurations, recommendedDefaults, services, hosts):
+    """
+    Entry point.
+    Validate configurations for the service. Return a list of errors.
+    The code for this function should be the same for each Service Advisor.
+    """
+    # Logger.info("Class: %s, Method: %s. Validating Configurations." %
+    #            (self.__class__.__name__, inspect.stack()[0][3]))
+
+    return []
+
+  @staticmethod
+  def isKerberosEnabled(services, configurations):
+    """
+    Determines if security is enabled by testing the value of core-site/hadoop.security.authentication enabled.
+    If the property exists and is equal to "kerberos", then is it enabled; otherwise is it assumed to be
+    disabled.
+
+    :type services: dict
+    :param services: the dictionary containing the existing configuration values
+    :type configurations: dict
+    :param configurations: the dictionary containing the updated configuration values
+    :rtype: bool
+    :return: True or False
+    """
+    if configurations and "core-site" in configurations and \
+            "hadoop.security.authentication" in configurations["core-site"]["properties"]:
+      return configurations["core-site"]["properties"]["hadoop.security.authentication"].lower() == "kerberos"
+    elif services and "core-site" in services["configurations"] and \
+            "hadoop.security.authentication" in services["configurations"]["core-site"]["properties"]:
+      return services["configurations"]["core-site"]["properties"]["hadoop.security.authentication"].lower() == "kerberos"
+    else:
+      return False
+
+
+class FlinkRecommender(service_advisor.ServiceAdvisor):
+  """
+  Flink Recommender suggests properties when adding the service for the first time or modifying configs via the UI.
+  """
+
+  def __init__(self, *args, **kwargs):
+    self.as_super = super(FlinkRecommender, self)
+    self.as_super.__init__(*args, **kwargs)
+
+  def recommendFlinkConfigurationsFromHDP33(self, configurations, clusterData, services, hosts):
+    """
+    Recommend configurations for this service based on HDP 3.3.
+    """
+    #empty
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_service.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_service.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_service.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_service.py	(date 1719626239000)
@@ -0,0 +1,56 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import errno
+from resource_management.core.logger import Logger
+from resource_management.core.resources.system import Execute
+from resource_management.core.resources.system import File
+from resource_management.libraries.functions import check_process_status, format
+
+# Note: Phoenix Query Server is only applicable to phoenix version stacks and above.
+def phoenix_service(action = 'start'): # 'start', 'stop', 'status'
+    # Note: params should already be imported before calling phoenix_service()
+    import status_params
+    pid_file = status_params.phoenix_pid_file
+    no_op_test = format("ls {pid_file} >/dev/null 2>&1 && ps -p `cat {pid_file}` >/dev/null 2>&1")
+
+    if action == "status":
+      check_process_status(pid_file)
+    else:
+      env = {'JAVA_HOME': format("{java64_home}"), 'HBASE_CONF_DIR': format("{hbase_conf_dir}")}
+      daemon_cmd = format("{phx_daemon_script} {action}")
+      if action == 'start':
+        Execute(daemon_cmd,
+                user=format("{hbase_user}"),
+                environment=env)
+  
+      elif action == 'stop':
+        Execute(daemon_cmd,
+                user=format("{hbase_user}"),
+                environment=env
+        )
+        try:
+          File(pid_file, action = "delete")
+        except OSError as exc:
+          # OSError: [Errno 2] No such file or directory
+          if exc.errno == errno.ENOENT:
+            Logger.info("Did not remove '{0}' as it did not exist".format(pid_file))
+          else:
+            raise
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_thriftserver.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_thriftserver.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_thriftserver.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_thriftserver.py	(date 1719626239000)
@@ -0,0 +1,68 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements. See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership. The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License. You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import sys
+from resource_management import *
+from resource_management.libraries.functions.security_commons import build_expectations, \
+cached_kinit_executor, get_params_from_filesystem, validate_security_config_properties, \
+FILE_TYPE_XML
+from hbase import hbase
+from hbase_service import hbase_service
+import upgrade
+from setup_ranger_hbase import setup_ranger_hbase
+from ambari_commons import OSCheck, OSConst
+from ambari_commons.os_family_impl import OsFamilyImpl
+from resource_management.libraries.functions.check_process_status import check_process_status
+
+class HbaseThrift(Script):
+
+  def install(self, env):
+    import params
+    env.set_params(params)
+    self.install_packages(env)
+    pass
+
+  def configure(self, env):
+    import params
+    env.set_params(params)
+    pass
+
+  def start(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+    self.configure(env)
+    hbase_service('thrift', action = 'start')
+
+  def stop(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+    hbase_service('thrift', action = 'stop')
+
+  def status(self, env):
+    import status_params
+    env.set_params(status_params)
+    hbase_thrift_pid_file = format("{pid_dir}/hbase-{hbase_user}-thrift.pid")
+    check_process_status(hbase_thrift_pid_file)
+
+  def get_component_name(self):
+    return "hbase-thriftserver"
+
+if __name__ == "__main__":
+  HbaseThrift().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_queryserver.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_queryserver.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_queryserver.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/phoenix_queryserver.py	(date 1719626239000)
@@ -0,0 +1,91 @@
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions import StackFeature
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.script import Script
+from phoenix_service import phoenix_service
+from hbase import hbase
+from resource_management.core.exceptions import Fail
+from resource_management.libraries.functions.decorator import retry
+
+# Note: Phoenix Query Server is only applicable to stack version supporting Phoenix.
+class PhoenixQueryServer(Script):
+
+  def install(self, env):
+    import params
+    env.set_params(params)
+    self.install_packages(env)
+
+
+  def configure(self, env):
+    import params
+    env.set_params(params)
+    hbase(name='queryserver')
+
+
+  def start(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+    self.configure(env)
+    phoenix_service('start')
+
+  @retry(times=3, sleep_time=5, err_class=Fail)
+  def post_start(self, env=None):
+    return super(PhoenixQueryServer, self).post_start(env)
+
+  def stop(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+    phoenix_service('stop')
+
+
+  def pre_upgrade_restart(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+
+    if params.stack_version_formatted and check_stack_feature(StackFeature.PHOENIX, params.stack_version_formatted):     
+      # phoenix uses hbase configs
+      stack_select.select_packages(params.version)
+
+
+  def status(self, env):
+    import status_params
+    env.set_params(status_params)
+    phoenix_service('status')
+
+
+  def security_status(self, env):
+    self.put_structured_out({"securityState": "UNSECURED"})
+    
+  def get_log_folder(self):
+    import params
+    return params.log_dir
+  
+  def get_user(self):
+    import params
+    return params.hbase_user
+
+  def get_pid_files(self):
+    import status_params
+    return [status_params.phoenix_pid_file]
+
+if __name__ == "__main__":
+  PhoenixQueryServer().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-atlas-application-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-atlas-application-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-atlas-application-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/configuration/hbase-atlas-application-properties.xml	(date 1719626239000)
@@ -0,0 +1,61 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false">
+  <!-- These are the Atlas Hooks properties specific to this service. This file is then merged with common properties
+  that apply to all services. -->
+  <property>
+    <name>atlas.hook.hbase.synchronous</name>
+    <value>false</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.hook.hbase.numRetries</name>
+    <value>3</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.hook.hbase.minThreads</name>
+    <value>5</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.hook.hbase.maxThreads</name>
+    <value>5</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.hook.hbase.keepAliveTime</name>
+    <value>10</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.hook.hbase.queueSize</name>
+    <value>1000</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/setup_ranger_kafka.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/setup_ranger_kafka.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/setup_ranger_kafka.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/setup_ranger_kafka.py	(date 1719626239000)
@@ -0,0 +1,177 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+import os
+
+from resource_management.core.logger import Logger
+from resource_management.core.resources import File, Execute, Link
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.setup_ranger_plugin_xml import \
+    setup_configuration_file_for_required_plugins
+
+
+def setup_ranger_kafka():
+    import params
+
+    if params.enable_ranger_kafka:
+
+        from resource_management.libraries.functions.setup_ranger_plugin_xml import setup_ranger_plugin
+
+        if params.retryAble:
+            Logger.info("Kafka: Setup ranger: command retry enables thus retrying if ranger admin is down !")
+        else:
+            Logger.info("Kafka: Setup ranger: command retry not enabled thus skipping if ranger admin is down !")
+
+        if params.has_namenode and params.xa_audit_hdfs_is_enabled:
+            try:
+                params.HdfsResource("/ranger/audit",
+                                    type="directory",
+                                    action="create_on_execute",
+                                    owner=params.hdfs_user,
+                                    group=params.hdfs_user,
+                                    mode=0755,
+                                    recursive_chmod=True
+                                    )
+                params.HdfsResource("/ranger/audit/kafka",
+                                    type="directory",
+                                    action="create_on_execute",
+                                    owner=params.kafka_user,
+                                    group=params.kafka_user,
+                                    mode=0700,
+                                    recursive_chmod=True
+                                    )
+                params.HdfsResource(None, action="execute")
+                if params.is_ranger_kms_ssl_enabled:
+                    Logger.info('Ranger KMS is ssl enabled, configuring ssl-client for hdfs audits.')
+                    setup_configuration_file_for_required_plugins(component_user=params.kafka_user,
+                                                                  component_group=params.user_group,
+                                                                  create_core_site_path=params.conf_dir,
+                                                                  configurations=params.config['configurations'][
+                                                                      'ssl-client'],
+                                                                  configuration_attributes=
+                                                                  params.config['configurationAttributes'][
+                                                                      'ssl-client'], file_name='ssl-client.xml')
+                else:
+                    Logger.info('Ranger KMS is not ssl enabled, skipping ssl-client for hdfs audits.')
+            except Exception, err:
+                Logger.exception(
+                    "Audit directory creation in HDFS for KAFKA Ranger plugin failed with error:\n{0}".format(err))
+
+        setup_ranger_plugin('kafka-broker', 'kafka', params.previous_jdbc_jar,
+                            params.downloaded_custom_connector, params.driver_curl_source,
+                            params.driver_curl_target, params.java64_home,
+                            params.repo_name, params.kafka_ranger_plugin_repo,
+                            params.ranger_env, params.ranger_plugin_properties,
+                            params.policy_user, params.policymgr_mgr_url,
+                            params.enable_ranger_kafka, conf_dict=params.conf_dir,
+                            component_user=params.kafka_user, component_group=params.user_group,
+                            cache_service_list=['kafka'],
+                            plugin_audit_properties=params.ranger_kafka_audit,
+                            plugin_audit_attributes=params.ranger_kafka_audit_attrs,
+                            plugin_security_properties=params.ranger_kafka_security,
+                            plugin_security_attributes=params.ranger_kafka_security_attrs,
+                            plugin_policymgr_ssl_properties=params.ranger_kafka_policymgr_ssl,
+                            plugin_policymgr_ssl_attributes=params.ranger_kafka_policymgr_ssl_attrs,
+                            component_list=['kafka-broker'], audit_db_is_enabled=params.xa_audit_db_is_enabled,
+                            credential_file=params.credential_file, xa_audit_db_password=params.xa_audit_db_password,
+                            ssl_truststore_password=params.ssl_truststore_password,
+                            ssl_keystore_password=params.ssl_keystore_password,
+                            api_version='v2', skip_if_rangeradmin_down=not params.retryAble,
+                            is_security_enabled=params.kerberos_security_enabled,
+                            is_stack_supports_ranger_kerberos=params.stack_supports_ranger_kerberos,
+                            component_user_principal=params.kafka_jaas_principal if params.kerberos_security_enabled else None,
+                            component_user_keytab=params.kafka_keytab_path if params.kerberos_security_enabled else None)
+
+        if params.enable_ranger_kafka and params.stack_supports_kafka_env_include_ranger_script:
+            Execute(
+                ('cp', '--remove-destination', params.setup_ranger_env_sh_source, params.setup_ranger_env_sh_target),
+                not_if=format("test -f {setup_ranger_env_sh_target}"),
+                sudo=True
+                )
+            File(params.setup_ranger_env_sh_target,
+                 owner=params.kafka_user,
+                 group=params.user_group,
+                 mode=0755
+                 )
+        elif not params.stack_supports_kafka_env_include_ranger_script:
+            File(format("{params.setup_ranger_env_sh_target}"),
+                 action="delete"
+                 )
+        if params.stack_supports_core_site_for_ranger_plugin and params.enable_ranger_kafka and params.kerberos_security_enabled:
+            # sometimes this is a link for missing /etc/hdp directory, just remove link/file and create regular file.
+            Execute(('rm', '-f', os.path.join(params.conf_dir, "core-site.xml")), sudo=True)
+
+            if params.has_namenode and params.stack_supports_kafka_env_include_ranger_script:
+                Logger.info(
+                    "Stack supports core-site.xml creation for Ranger plugin and Namenode is installed, creating create core-site.xml from namenode configurations")
+                setup_configuration_file_for_required_plugins(component_user=params.kafka_user,
+                                                              component_group=params.user_group,
+                                                              create_core_site_path=params.conf_dir,
+                                                              configurations=params.config['configurations'][
+                                                                  'core-site'],
+                                                              configuration_attributes=
+                                                              params.config['configurationAttributes']['core-site'],
+                                                              file_name='core-site.xml',
+                                                              xml_include_file=params.mount_table_xml_inclusion_file_full_path,
+                                                              xml_include_file_content=params.mount_table_content)
+            elif params.has_namenode and not params.stack_supports_kafka_env_include_ranger_script:
+                Logger.info(
+                    "Stack supports core-site.xml creation for Ranger plugin and create core-site and hdfs-site in the ranger-kafka-plugin-impl diretory.")
+
+                Link(format('{ranger_kafka_plugin_impl_path}/core-site.xml'),
+                     only_if=os.path.islink(format('{ranger_kafka_plugin_impl_path}/core-site.xml')),
+                     action="delete")
+                setup_configuration_file_for_required_plugins(component_user=params.kafka_user,
+                                                              component_group=params.user_group,
+                                                              create_core_site_path=params.ranger_kafka_plugin_impl_path,
+                                                              configurations=params.config['configurations'][
+                                                                  'core-site'],
+                                                              configuration_attributes=
+                                                              params.config['configurationAttributes']['core-site'],
+                                                              file_name='core-site.xml',
+                                                              xml_include_file=params.mount_table_xml_inclusion_file_full_path,
+                                                              xml_include_file_content=params.mount_table_content)
+
+                Link(format('{ranger_kafka_plugin_impl_path}/hdfs-site.xml'),
+                     only_if=os.path.islink(format('{ranger_kafka_plugin_impl_path}/hdfs-site.xml')),
+                     action="delete")
+                setup_configuration_file_for_required_plugins(component_user=params.kafka_user,
+                                                              component_group=params.user_group,
+                                                              create_core_site_path=params.ranger_kafka_plugin_impl_path,
+                                                              configurations=params.config['configurations'][
+                                                                  'hdfs-site'],
+                                                              configuration_attributes=
+                                                              params.config['configurationAttributes']['hdfs-site'],
+                                                              file_name='hdfs-site.xml',
+                                                              xml_include_file=params.mount_table_xml_inclusion_file_full_path,
+                                                              xml_include_file_content=params.mount_table_content)
+                Link(format('{ranger_kafka_plugin_impl_path}/conf'),
+                     to=format('{conf_dir}'))
+            else:
+                Logger.info(
+                    "Stack supports core-site.xml creation for Ranger plugin and Namenode is not installed, creating create core-site.xml from default configurations")
+                setup_configuration_file_for_required_plugins(component_user=params.kafka_user,
+                                                              component_group=params.user_group,
+                                                              create_core_site_path=params.conf_dir, configurations={
+                        'hadoop.security.authentication': 'kerberos' if params.kerberos_security_enabled else 'simple'},
+                                                              configuration_attributes={}, file_name='core-site.xml')
+                Link(format('{ranger_kafka_plugin_impl_path}/conf'),
+                     to=format('{conf_dir}'))
+        else:
+            Logger.info(
+                "Stack does not support core-site.xml creation for Ranger plugin, skipping core-site.xml configurations")
+    else:
+        Logger.info('Ranger Kafka plugin is not enabled')
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-audit.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-audit.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-audit.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-audit.xml	(date 1719626239000)
@@ -0,0 +1,130 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.audit.is.enabled</name>
+    <value>true</value>
+    <description>Is Audit enabled?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs</name>
+    <value>true</value>
+    <display-name>Audit to HDFS</display-name>
+    <description>Is Audit to HDFS enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.dir</name>
+    <value>hdfs://NAMENODE_HOSTNAME:8020/ranger/audit</value>
+    <description>HDFS folder to write audit to, make sure the service user has requried permissions</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.hdfs.dir</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.batch.filespool.dir</name>
+    <value>/var/log/kafka/audit/hdfs/spool</value>
+    <description>/var/log/kafka/audit/hdfs/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr</name>
+    <value>false</value>
+    <display-name>Audit to SOLR</display-name>
+    <description>Is Solr audit enabled?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>xasecure.audit.destination.solr</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.urls</name>
+    <value/>
+    <description>Solr URL</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.urls</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.zookeepers</name>
+    <value>NONE</value>
+    <description>Solr Zookeeper string</description>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.audit.solr.zookeepers</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr.batch.filespool.dir</name>
+    <value>/var/log/kafka/audit/solr/spool</value>
+    <description>/var/log/kafka/audit/solr/spool</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.provider.summary.enabled</name>
+    <value>true</value>
+    <display-name>Audit provider summary enabled</display-name>
+    <description>Enable Summary audit?</description>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kafka.ambari.cluster.name</name>
+    <value>{{cluster_name}}</value>
+    <description>Capture cluster name from where Ranger kafka plugin is enabled.</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-security.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-security.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-security.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-security.xml	(date 1719626239000)
@@ -0,0 +1,64 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>ranger.plugin.kafka.service.name</name>
+    <value>{{repo_name}}</value>
+    <description>Name of the Ranger service containing policies for this Kafka instance</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kafka.policy.source.impl</name>
+    <value>org.apache.ranger.admin.client.RangerAdminRESTClient</value>
+    <description>Class to retrieve policies from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kafka.policy.rest.url</name>
+    <value>{{policymgr_mgr_url}}</value>
+    <description>URL to Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>policymgr_external_url</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>ranger.plugin.kafka.policy.rest.ssl.config.file</name>
+    <value>/etc/kafka/conf/ranger-policymgr-ssl.xml</value>
+    <description>Path to the file containing SSL details to contact Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kafka.policy.pollIntervalMs</name>
+    <value>30000</value>
+    <description>How often to poll for changes in policies?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.plugin.kafka.policy.cache.dir</name>
+    <value>/etc/ranger/{{repo_name}}/policycache</value>
+    <description>Directory where Ranger policies are cached after successful retrieval from the source</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-policymgr-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-policymgr-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-policymgr-ssl.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-policymgr-ssl.xml	(date 1719626239000)
@@ -0,0 +1,72 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <description>password for keystore</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>java truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file/{{credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file/{{credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-plugin-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-plugin-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-plugin-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/ranger-kafka-plugin-properties.xml	(date 1719626239000)
@@ -0,0 +1,154 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+  <property>
+    <name>policy_user</name>
+    <value>ambari-qa</value>
+    <display-name>Policy user for KAFKA</display-name>
+    <description>This user must be system user and also present at Ranger admin portal</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>hadoop.rpc.protection</name>
+    <value/>
+    <description>Used for repository creation on ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>common.name.for.certificate</name>
+    <value/>
+    <description>Common name for certificate, this value should match what is specified in repo within ranger admin</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>zookeeper.connect</name>
+    <value>localhost:2181</value>
+    <description>Used for repository creation on ranger admin</description>
+    <depends-on>
+      <property>
+        <type>zoo.cfg</type>
+        <name>clientPort</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-kafka-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Enable Ranger for KAFKA</display-name>
+    <description>Enable ranger kafka plugin</description>
+    <depends-on>
+      <property>
+        <type>ranger-env</type>
+        <name>ranger-kafka-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <value-attributes>
+      <type>boolean</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_PASSWORD</name>
+    <value>kafka</value>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger repository config password</display-name>
+    <description>Used for repository creation on ranger admin</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+
+
+  <property>
+    <name>external_admin_username</name>
+    <value></value>
+    <display-name>External Ranger admin username</display-name>
+    <description>Add ranger default admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_admin_password</name>
+    <value></value>
+    <display-name>External Ranger admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_username</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin username</display-name>
+    <description>Add ranger default ambari admin username if want to communicate to external ranger</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>external_ranger_admin_password</name>
+    <value></value>
+    <display-name>External Ranger Ambari admin password</display-name>
+    <property-type>PASSWORD</property-type>
+    <description>Add ranger default ambari admin password if want to communicate to external ranger</description>
+    <value-attributes>
+      <type>password</type>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>REPOSITORY_CONFIG_USERNAME</name>
+    <value>kafka</value>
+    <display-name>Ranger repository config user</display-name>
+    <description>Used for repository creation on ranger admin</description>
+    <depends-on>
+      <property>
+        <type>ranger-kafka-plugin-properties</type>
+        <name>ranger-kafka-plugin-enabled</name>
+      </property>
+      <property>
+        <type>kafka-env</type>
+        <name>kafka_user</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/templates/hbase_grant_permissions.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/templates/hbase_grant_permissions.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/templates/hbase_grant_permissions.j2
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/templates/hbase_grant_permissions.j2	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/templates/hbase_grant_permissions.j2	(date 1719626239000)
@@ -36,6 +36,8 @@
 # under the License.
 #
 #
+#grant '{{smoke_test_user}}', '{{smokeuser_permissions}}'
+#exit
 
 echo "grant '{{smoke_test_user}}', '{{smokeuser_permissions}}'" | {{hbase_cmd}} --config {{hbase_conf_dir}} shell > {{exec_tmp_dir}}/hbase_grant_result
 cat {{exec_tmp_dir}}/hbase_grant_result
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/database.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/database.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/database.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/database.json	(date 1719626239000)
@@ -0,0 +1,286 @@
+{
+  "name": "database",
+  "configuration": {
+    "placement": {
+      "configuration-layout": "database",
+      "configs": [
+        {
+          "config": "admin-properties/DB_FLAVOR",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "admin-properties/db_name",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "admin-properties/db_user",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "ranger-admin-site/ranger.jpa.jdbc.url",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "admin-properties/db_host",
+          "subsection-name": "subsection-ranger-db-row1-col2"
+        },
+        {
+          "config": "ranger-admin-site/ranger.jpa.jdbc.driver",
+          "subsection-name": "subsection-ranger-db-row1-col2"
+        },
+        {
+          "config": "admin-properties/db_password",
+          "subsection-name": "subsection-ranger-db-row1-col2"
+        },
+        {
+          "config": "ranger-env/create_db_dbuser",
+          "subsection-name": "subsection-ranger-db-row2"
+        },
+        {
+          "config": "ranger-env/test_db_connection",
+          "subsection-name": "subsection-ranger-db-row2",
+          "property_value_attributes": {
+            "ui_only_property": true
+          },
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/create_db_dbuser"
+              ],
+              "if": "${ranger-env/create_db_dbuser}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "admin-properties/db_root_user",
+          "subsection-name": "subsection-ranger-db-root-user-col1"
+        },
+        {
+          "config": "ranger-env/ranger_privelege_user_jdbc_url",
+          "subsection-name": "subsection-ranger-db-root-user-col1"
+        },
+        {
+          "config": "admin-properties/db_root_password",
+          "subsection-name": "subsection-ranger-db-root-user-col2"
+        },
+        {
+          "config": "ranger-env/test_root_db_connection",
+          "subsection-name": "subsection-ranger-db-root-user-col1",
+          "property_value_attributes": {
+            "ui_only_property": true
+          }
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "admin-properties/DB_FLAVOR",
+        "widget": {
+          "type": "combo"
+        }
+      },
+      {
+        "config": "admin-properties/db_user",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_name",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.jpa.jdbc.url",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.jpa.jdbc.driver",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_host",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "ranger-env/test_db_connection",
+        "widget": {
+          "type": "test-db-connection",
+          "display-name": "Test Connection",
+          "required-properties": {
+            "jdbc.driver.class": "ranger-admin-site/ranger.jpa.jdbc.driver",
+            "jdbc.driver.url": "ranger-admin-site/ranger.jpa.jdbc.url",
+            "db.connection.source.host": "ranger-site/ranger_admin_hosts",
+            "db.type": "admin-properties/DB_FLAVOR",
+            "db.connection.destination.host": "admin-properties/db_host",
+            "db.connection.user": "admin-properties/db_user",
+            "db.connection.password": "admin-properties/db_password"
+          }
+        }
+      },
+      {
+        "config": "ranger-env/create_db_dbuser",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger_privelege_user_jdbc_url",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_root_user",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_root_password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "ranger-env/test_root_db_connection",
+        "widget": {
+          "type": "test-db-connection",
+          "display-name": "Test Connection",
+          "required-properties": {
+            "jdbc.driver.class": "ranger-admin-site/ranger.jpa.jdbc.driver",
+            "jdbc.driver.url": "ranger-env/ranger_privelege_user_jdbc_url",
+            "db.connection.source.host": "ranger-site/ranger_admin_hosts",
+            "db.type": "admin-properties/DB_FLAVOR",
+            "db.connection.destination.host": "admin-properties/db_host",
+            "db.connection.user": "admin-properties/db_root_user",
+            "db.connection.password": "admin-properties/db_root_password"
+          }
+        }
+      }
+    ],
+    "layouts": [
+      {
+        "name": "database",
+        "tabs": [
+          {
+            "name": "ranger_database",
+            "display-name": "Ranger",
+            "layout": {
+              "tab-columns": "2",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "section-ranger-admin",
+                  "removed" : false,
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "3",
+                  "column-span": "2",
+                  "section-columns": "2",
+                  "section-rows": "3",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-db-row1-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-ranger-db-row1-col2",
+                      "row-index": "0",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-ranger-db-row2",
+                      "row-index": "1",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "2"
+                    },
+                    {
+                      "name": "subsection-ranger-db-root-user-col1",
+                      "row-index": "2",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "depends-on": [
+                        {
+                          "configs":[
+                            "ranger-env/create_db_dbuser"
+                          ],
+                          "if": "${ranger-env/create_db_dbuser}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    },
+                    {
+                      "name": "subsection-ranger-db-root-user-col2",
+                      "row-index": "2",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "depends-on": [
+                        {
+                          "configs":[
+                            "ranger-env/create_db_dbuser"
+                          ],
+                          "if": "${ranger-env/create_db_dbuser}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/credentials.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/credentials.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/credentials.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/credentials.json	(date 1719626239000)
@@ -0,0 +1,91 @@
+{
+  "name": "credentials",
+  "configuration": {
+    "layouts": [
+      {
+        "name": "credentials",
+        "tabs": [
+          {
+            "name": "credentials",
+            "layout": {
+              "sections": [
+                {
+                  "name": "credentials",
+                  "subsections": [
+                    {
+                      "name" : "subsection-ranger-admin-credential",
+                      "display-name": "Ranger Admin User Credentials"
+                    },
+                    {
+                      "name" : "subsection-ranger-admin-ambari-credential",
+                      "display-name": "Ranger Admin Credentials for Ambari User"
+                    },
+                    {
+                      "name" : "subsection-ranger-db-credential",
+                      "display-name": "Ranger Admin DB Credentials"
+                    },
+                    {
+                      "name" : "subsection-ranger-usersync-credential",
+                      "display-name": "Ranger Usersync user's password"
+                    },
+                    {
+                      "name" : "subsection-ranger-tagsync-credential",
+                      "display-name": "Ranger Tagsync user's password"
+                    },
+                    {
+                      "name" : "subsection-ranger-keyadmin-credential",
+                      "display-name": "Ranger KMS keyadmin user's password"
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ],
+    "placement": {
+      "configuration-layout": "credentials",
+      "configs": [
+        {
+          "config": "ranger-env/admin_username",
+          "subsection-name": "subsection-ranger-admin-credential"
+        },
+        {
+          "config": "ranger-env/admin_password",
+          "subsection-name": "subsection-ranger-admin-credential"
+        },
+        {
+          "config": "ranger-env/ranger_admin_username",
+          "subsection-name": "subsection-ranger-admin-ambari-credential"
+        },
+        {
+          "config": "ranger-env/ranger_admin_password",
+          "subsection-name": "subsection-ranger-admin-ambari-credential"
+        },
+        {
+          "config": "admin-properties/db_user",
+          "subsection-name": "subsection-ranger-db-credential"
+        },
+        {
+          "config": "admin-properties/db_password",
+          "subsection-name": "subsection-ranger-db-credential"
+        },
+        {
+          "config": "ranger-env/rangerusersync_user_password",
+          "subsection-name": "subsection-ranger-usersync-credential"
+        },
+        {
+          "config": "ranger-env/rangertagsync_user_password",
+          "subsection-name": "subsection-ranger-tagsync-credential"
+        },
+        {
+          "config": "ranger-env/keyadmin_user_password",
+          "subsection-name": "subsection-ranger-keyadmin-credential"
+        }
+      ]
+    },
+    "widgets": [
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/directories.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/directories.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/directories.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/directories.json	(date 1719626239000)
@@ -0,0 +1,108 @@
+{
+  "name": "directories",
+  "description": "Directories theme for RANGER service",
+  "configuration": {
+    "layouts": [
+      {
+        "name": "directories",
+        "tabs": [
+          {
+            "name": "directories",
+            "display-name": "Directories",
+            "layout": {
+              "tab-columns": "1",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "subsection-log-dirs",
+                  "display-name": "LOG DIRS",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-log-dirs",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                },
+                {
+                  "name": "subsection-pid-dirs",
+                  "display-name": "PID DIR",
+                  "row-index": "2",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-pid-dirs",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ],
+    "placement": {
+      "configuration-layout": "directories",
+      "configs": [
+        {
+          "config": "ranger-admin-site/ranger.logs.base.dir",
+          "subsection-name": "subsection-log-dirs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.logdir",
+          "subsection-name": "subsection-log-dirs"
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.logdir",
+          "subsection-name": "subsection-log-dirs"
+        },
+        {
+          "config": "ranger-env/ranger_pid_dir",
+          "subsection-name": "subsection-pid-dirs"
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "ranger-admin-site/ranger.logs.base.dir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.logdir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.logdir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-env/ranger_pid_dir",
+        "widget": {
+          "type": "text-field"
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/theme_version_2.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/theme_version_2.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/theme_version_2.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/themes/theme_version_2.json	(date 1719626239000)
@@ -0,0 +1,1976 @@
+{
+  "name": "default",
+  "description": "Default theme for Ranger service",
+  "configuration": {
+    "layouts": [
+      {
+        "name": "default",
+        "tabs": [
+          {
+            "name": "ranger_admin_settings",
+            "display-name": "Ranger Admin",
+            "layout": {
+              "tab-columns": "2",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "section-ranger-admin",
+                  "display-name": "Ranger Admin",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "3",
+                  "column-span": "2",
+                  "section-columns": "2",
+                  "section-rows": "3",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-db-row1-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-ranger-db-row1-col2",
+                      "row-index": "0",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-ranger-db-row2",
+                      "row-index": "1",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "2"
+                    },
+                    {
+                      "name": "subsection-ranger-db-root-user-col1",
+                      "row-index": "2",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "depends-on": [
+                        {
+                          "configs":[
+                            "ranger-env/create_db_dbuser"
+                          ],
+                          "if": "${ranger-env/create_db_dbuser}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    },
+                    {
+                      "name": "subsection-ranger-db-root-user-col2",
+                      "row-index": "2",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "depends-on": [
+                        {
+                          "configs":[
+                            "ranger-env/create_db_dbuser"
+                          ],
+                          "if": "${ranger-env/create_db_dbuser}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    }
+                  ]
+                }
+              ]
+            }
+          },
+          {
+            "name": "ranger_user_info",
+            "display-name": "Ranger User Info",
+            "layout": {
+              "tab-columns": "1",
+              "tab-rows": "1",
+              "sections": [
+                {
+                  "name": "section-user-info",
+                  "display-name": "Ranger User Info",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "2",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "2",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-user-row1-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "subsection-ranger-user-row2-col1",
+                      "row-index": "1",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1",
+                      "subsection-tabs": [
+                        {
+                          "name": "ldap-common-configs",
+                          "display-name": "Common Configs",
+                          "depends-on": [
+                            {
+                              "configs": [
+                                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+                              ],
+                              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder",
+                              "then": {
+                                "property_value_attributes": {
+                                  "visible": true
+                                }
+                              },
+                              "else": {
+                                "property_value_attributes": {
+                                  "visible": false
+                                }
+                              }
+                            }
+                          ]
+                        },
+                        {
+                          "name": "ldap-user-configs",
+                          "display-name": "User Configs",
+                          "depends-on": [
+                            {
+                              "configs": [
+                                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+                              ],
+                              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder",
+                              "then": {
+                                "property_value_attributes": {
+                                  "visible": true
+                                }
+                              },
+                              "else": {
+                                "property_value_attributes": {
+                                  "visible": false
+                                }
+                              }
+                            }
+                          ]
+                        },
+                        {
+                          "name": "ldap-group-configs",
+                          "display-name": "Group Configs",
+                          "depends-on": [
+                            {
+                              "configs": [
+                                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+                              ],
+                              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder",
+                              "then": {
+                                "property_value_attributes": {
+                                  "visible": true
+                                }
+                              },
+                              "else": {
+                                "property_value_attributes": {
+                                  "visible": false
+                                }
+                              }
+                            }
+                          ]
+                        }
+                      ],
+                      "depends-on": [
+                        {
+                          "configs": [
+                            "ranger-ugsync-site/ranger.usersync.enabled"
+                          ],
+                          "if": "${ranger-ugsync-site/ranger.usersync.enabled}",
+                          "then": {
+                            "property_value_attributes": {
+                              "visible": true
+                            }
+                          },
+                          "else": {
+                            "property_value_attributes": {
+                              "visible": false
+                            }
+                          }
+                        }
+                      ]
+                    }
+                  ]
+                }
+              ]
+            }
+          },
+          {
+            "name": "ranger_plugin",
+            "display-name": "Ranger Plugin",
+            "layout": {
+              "tab-columns": "1",
+              "tab-rows": "1",
+              "sections": [
+                {
+                  "name": "section-ranger-plugin",
+                  "display-name": "Ranger Plugin",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "3",
+                  "section-columns": "3",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "section-ranger-plugin-row1-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "section-ranger-plugin-row1-col2",
+                      "row-index": "0",
+                      "column-index": "1",
+                      "row-span": "1",
+                      "column-span": "1"
+                    },
+                    {
+                      "name": "section-ranger-plugin-row1-col3",
+                      "row-index": "0",
+                      "column-index": "2",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                }
+              ]
+            }
+          },
+          {
+            "name": "ranger_audit_settings",
+            "display-name": "Ranger Audit",
+            "layout": {
+              "tab-columns": "2",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "section-ranger-audit-solr",
+                  "display-name": "Audit to Solr",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-solr-row1-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                },
+                {
+                  "name": "section-ranger-audit-hdfs",
+                  "display-name": "Audit to HDFS",
+                  "row-index": "0",
+                  "column-index": "1",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-hdfs-row1-col2",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                }
+              ]
+            }
+          },
+          {
+            "name": "ranger_tagsync",
+            "display-name": "Ranger Tagsync",
+            "layout": {
+              "tab-columns": "2",
+              "tab-rows": "2",
+              "sections": [
+                {
+                  "name": "section-tagsync-atlas",
+                  "display-name": "Atlas Tag Source",
+                  "row-index": "0",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-tagsync-row1-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                },
+                {
+                  "name": "section-tagsync-atlasrest",
+                  "display-name": "AtlasRest Tag Source",
+                  "row-index": "0",
+                  "column-index": "1",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-tagsync-row1-col2",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                },
+                {
+                  "name": "section-tagsync-file",
+                  "display-name": "File Tag Source",
+                  "row-index": "1",
+                  "column-index": "0",
+                  "row-span": "1",
+                  "column-span": "1",
+                  "section-columns": "1",
+                  "section-rows": "1",
+                  "subsections": [
+                    {
+                      "name": "subsection-ranger-tagsync-row2-col1",
+                      "row-index": "0",
+                      "column-index": "0",
+                      "row-span": "1",
+                      "column-span": "1"
+                    }
+                  ]
+                }
+              ]
+            }
+          }
+        ]
+      }
+    ],
+    "placement": {
+      "configuration-layout": "default",
+      "configs": [
+        {
+          "config": "admin-properties/DB_FLAVOR",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "admin-properties/db_name",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "admin-properties/db_user",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "ranger-admin-site/ranger.jpa.jdbc.url",
+          "subsection-name": "subsection-ranger-db-row1-col1"
+        },
+        {
+          "config": "admin-properties/db_host",
+          "subsection-name": "subsection-ranger-db-row1-col2"
+        },
+        {
+          "config": "ranger-admin-site/ranger.jpa.jdbc.driver",
+          "subsection-name": "subsection-ranger-db-row1-col2"
+        },
+        {
+          "config": "admin-properties/db_password",
+          "subsection-name": "subsection-ranger-db-row1-col2"
+        },
+        {
+          "config": "ranger-env/create_db_dbuser",
+          "subsection-name": "subsection-ranger-db-row2"
+        },
+        {
+          "config": "ranger-env/test_db_connection",
+          "subsection-name": "subsection-ranger-db-row2",
+          "property_value_attributes": {
+            "ui_only_property": true
+          },
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/create_db_dbuser"
+              ],
+              "if": "${ranger-env/create_db_dbuser}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "admin-properties/db_root_user",
+          "subsection-name": "subsection-ranger-db-root-user-col1"
+        },
+        {
+          "config": "ranger-env/ranger_privelege_user_jdbc_url",
+          "subsection-name": "subsection-ranger-db-root-user-col1"
+        },
+        {
+          "config": "admin-properties/db_root_password",
+          "subsection-name": "subsection-ranger-db-root-user-col2"
+        },
+        {
+          "config": "ranger-env/test_root_db_connection",
+          "subsection-name": "subsection-ranger-db-root-user-col1",
+          "property_value_attributes": {
+            "ui_only_property": true
+          }
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.enabled",
+          "subsection-name": "subsection-ranger-user-row1-col1"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.source.impl.class",
+          "subsection-name": "subsection-ranger-user-row2-col1"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.unix.minUserId",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.unixusersync.process.UnixUserGroupBuilder",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.unix.password.file",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.unixusersync.process.UnixUserGroupBuilder",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.unix.group.file",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.unixusersync.process.UnixUserGroupBuilder",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.filesource.file",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.unixusersync.process.FileSourceUserGroupBuilder",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.filesource.text.delimiter",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.source.impl.class"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.source.impl.class} === org.apache.ranger.unixusersync.process.FileSourceUserGroupBuilder",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.url",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-common-configs"
+        },
+        {
+          "config": "ranger-env/bind_anonymous",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-common-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.binddn",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-common-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/bind_anonymous"
+              ],
+              "if": "${ranger-env/bind_anonymous}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.ldapbindpassword",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-common-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/bind_anonymous"
+              ],
+              "if": "${ranger-env/bind_anonymous}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.deltasync",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-common-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.starttls",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-common-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.user.nameattribute",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.user.objectclass",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.user.searchbase",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.user.searchfilter",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.user.searchscope",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.user.groupnameattribute",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.usermapsyncenabled",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.user.searchenabled",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-user-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.search.first.enabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.search.first.enabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.searchenabled",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.memberattributename",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.searchenabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.searchenabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.nameattribute",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.searchenabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.searchenabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.objectclass",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.searchenabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.searchenabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.searchbase",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.searchenabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.searchenabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.searchfilter",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.searchenabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.searchenabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.group.search.first.enabled",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-ugsync-site/ranger.usersync.group.searchenabled"
+              ],
+              "if": "${ranger-ugsync-site/ranger.usersync.group.searchenabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/is_nested_groupsync_enabled",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs"
+        },
+        {
+          "config": "ranger-ugsync-site/ranger.usersync.ldap.grouphierarchylevels",
+          "subsection-name": "subsection-ranger-user-row2-col1",
+          "subsection-tab-name": "ldap-group-configs",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/is_nested_groupsync_enabled"
+              ],
+              "if": "${ranger-env/is_nested_groupsync_enabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-hdfs-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "HDFS",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-yarn-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "YARN",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-hive-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "HIVE",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-spark3-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "SPARK3",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-ozone-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "OZONE",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-doris-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "DORIS",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-kudu-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col3",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "KUDU",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-hbase-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col2",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "HBASE",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-storm-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col2",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "STORM",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-knox-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col3",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "KNOX",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-kafka-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col3",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "KAFKA",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-atlas-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col3",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "ATLAS",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/ranger-nifi-plugin-enabled",
+          "subsection-name": "section-ranger-plugin-row1-col1",
+          "depends-on": [
+            {
+              "resource": "service",
+              "if": "NIFI",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/xasecure.audit.destination.solr",
+          "subsection-name": "subsection-ranger-solr-row1-col1"
+        },
+        {
+          "config": "ranger-env/is_solrCloud_enabled",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/xasecure.audit.destination.solr"
+              ],
+              "if": "${ranger-env/xasecure.audit.destination.solr}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/is_external_solrCloud_enabled",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/xasecure.audit.destination.solr",
+                "ranger-env/is_solrCloud_enabled"
+              ],
+              "if": "${ranger-env/xasecure.audit.destination.solr} && ${ranger-env/is_solrCloud_enabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/is_external_solrCloud_kerberos",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/xasecure.audit.destination.solr",
+                "ranger-env/is_solrCloud_enabled",
+                "ranger-env/is_external_solrCloud_enabled"
+              ],
+              "if": "${ranger-env/xasecure.audit.destination.solr} && ${ranger-env/is_solrCloud_enabled} && ${ranger-env/is_external_solrCloud_enabled}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-admin-site/ranger.audit.solr.urls",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/is_solrCloud_enabled",
+                "ranger-env/xasecure.audit.destination.solr"
+              ],
+              "if": "${ranger-env/is_solrCloud_enabled} === false && ${ranger-env/xasecure.audit.destination.solr}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-admin-site/ranger.audit.solr.zookeepers",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/is_solrCloud_enabled",
+                "ranger-env/xasecure.audit.destination.solr"
+              ],
+              "if": "${ranger-env/is_solrCloud_enabled} && ${ranger-env/xasecure.audit.destination.solr}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-admin-site/ranger.audit.solr.username",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/xasecure.audit.destination.solr"
+              ],
+              "if": "${ranger-env/xasecure.audit.destination.solr}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-admin-site/ranger.audit.solr.password",
+          "subsection-name": "subsection-ranger-solr-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/xasecure.audit.destination.solr"
+              ],
+              "if": "${ranger-env/xasecure.audit.destination.solr}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-env/xasecure.audit.destination.hdfs",
+          "subsection-name": "subsection-ranger-hdfs-row1-col2"
+        },
+        {
+          "config": "ranger-env/xasecure.audit.destination.hdfs.dir",
+          "subsection-name": "subsection-ranger-hdfs-row1-col2",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-env/xasecure.audit.destination.hdfs"
+              ],
+              "if": "${ranger-env/xasecure.audit.destination.hdfs}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.atlas",
+          "subsection-name": "subsection-ranger-tagsync-row1-col1"
+        },
+        {
+          "config": "tagsync-application-properties/atlas.kafka.bootstrap.servers",
+          "subsection-name": "subsection-ranger-tagsync-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.atlas"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.atlas}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "tagsync-application-properties/atlas.kafka.zookeeper.connect",
+          "subsection-name": "subsection-ranger-tagsync-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.atlas"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.atlas}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "tagsync-application-properties/atlas.kafka.entities.group.id",
+          "subsection-name": "subsection-ranger-tagsync-row1-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.atlas"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.atlas}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.atlasrest",
+          "subsection-name": "subsection-ranger-tagsync-row1-col2"
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.atlasrest.endpoint",
+          "subsection-name": "subsection-ranger-tagsync-row1-col2",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.atlasrest"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.atlasrest}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.atlasrest.download.interval.millis",
+          "subsection-name": "subsection-ranger-tagsync-row1-col2",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.atlasrest"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.atlasrest}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.file",
+          "subsection-name": "subsection-ranger-tagsync-row2-col1"
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.file.check.interval.millis",
+          "subsection-name": "subsection-ranger-tagsync-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.file"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.file}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        },
+        {
+          "config": "ranger-tagsync-site/ranger.tagsync.source.file.filename",
+          "subsection-name": "subsection-ranger-tagsync-row2-col1",
+          "depends-on": [
+            {
+              "configs":[
+                "ranger-tagsync-site/ranger.tagsync.source.file"
+              ],
+              "if": "${ranger-tagsync-site/ranger.tagsync.source.file}",
+              "then": {
+                "property_value_attributes": {
+                  "visible": true
+                }
+              },
+              "else": {
+                "property_value_attributes": {
+                  "visible": false
+                }
+              }
+            }
+          ]
+        }
+      ]
+    },
+    "widgets": [
+      {
+        "config": "admin-properties/DB_FLAVOR",
+        "widget": {
+          "type": "combo"
+        }
+      },
+      {
+        "config": "admin-properties/db_user",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_name",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.jpa.jdbc.url",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.jpa.jdbc.driver",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_host",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "ranger-env/test_db_connection",
+        "widget": {
+          "type": "test-db-connection",
+          "display-name": "Test Connection",
+          "required-properties": {
+            "jdbc.driver.class": "ranger-admin-site/ranger.jpa.jdbc.driver",
+            "jdbc.driver.url": "ranger-admin-site/ranger.jpa.jdbc.url",
+            "db.connection.source.host": "ranger-site/ranger_admin_hosts",
+            "db.type": "admin-properties/DB_FLAVOR",
+            "db.connection.destination.host": "admin-properties/db_host",
+            "db.connection.user": "admin-properties/db_user",
+            "db.connection.password": "admin-properties/db_password"
+          }
+        }
+      },
+      {
+        "config": "ranger-env/create_db_dbuser",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger_privelege_user_jdbc_url",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_root_user",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "admin-properties/db_root_password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "ranger-env/test_root_db_connection",
+        "widget": {
+          "type": "test-db-connection",
+          "display-name": "Test Connection",
+          "required-properties": {
+            "jdbc.driver.class": "ranger-admin-site/ranger.jpa.jdbc.driver",
+            "jdbc.driver.url": "ranger-env/ranger_privelege_user_jdbc_url",
+            "db.connection.source.host": "ranger-site/ranger_admin_hosts",
+            "db.type": "admin-properties/DB_FLAVOR",
+            "db.connection.destination.host": "admin-properties/db_host",
+            "db.connection.user": "admin-properties/db_root_user",
+            "db.connection.password": "admin-properties/db_root_password"
+          }
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.source.impl.class",
+        "widget": {
+          "type": "combo"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.url",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-env/bind_anonymous",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.binddn",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.ldapbindpassword",
+        "widget": {
+          "type": "password"
+        }
+      },
+
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.usermapsyncenabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.user.searchenabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.user.nameattribute",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.user.objectclass",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.user.searchbase",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.user.searchfilter",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.user.searchscope",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.user.groupnameattribute",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.searchenabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.memberattributename",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.nameattribute",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.objectclass",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.searchbase",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.searchfilter",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.group.search.first.enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.unix.minUserId",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.unix.password.file",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.unix.group.file",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.filesource.file",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.filesource.text.delimiter",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.deltasync",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.starttls",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/is_nested_groupsync_enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-ugsync-site/ranger.usersync.ldap.grouphierarchylevels",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-hdfs-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-hive-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+	  {
+        "config": "ranger-env/ranger-spark3-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-ozone-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-doris-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-kudu-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-hbase-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-kafka-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-knox-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-storm-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-yarn-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-atlas-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/ranger-nifi-plugin-enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/xasecure.audit.destination.solr",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/is_solrCloud_enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/is_external_solrCloud_enabled",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/is_external_solrCloud_kerberos",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.audit.solr.urls",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.audit.solr.zookeepers",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.audit.solr.username",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-admin-site/ranger.audit.solr.password",
+        "widget": {
+          "type": "password"
+        }
+      },
+      {
+        "config": "ranger-env/xasecure.audit.destination.hdfs",
+        "widget": {
+          "type": "toggle"
+        }
+      },
+      {
+        "config": "ranger-env/xasecure.audit.destination.hdfs.dir",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.file.check.interval.millis",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.file.filename",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.atlasrest.download.interval.millis",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.atlasrest.endpoint",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "tagsync-application-properties/atlas.kafka.entities.group.id",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "tagsync-application-properties/atlas.kafka.bootstrap.servers",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "tagsync-application-properties/atlas.kafka.zookeeper.connect",
+        "widget": {
+          "type": "text-field"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.atlas",
+        "widget": {
+          "type": "checkbox"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.atlasrest",
+        "widget": {
+          "type": "checkbox"
+        }
+      },
+      {
+        "config": "ranger-tagsync-site/ranger.tagsync.source.file",
+        "widget": {
+          "type": "checkbox"
+        }
+      }
+    ]
+  }
+}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/alerts/alert_ranger_admin_passwd_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/alerts/alert_ranger_admin_passwd_check.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/alerts/alert_ranger_admin_passwd_check.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/alerts/alert_ranger_admin_passwd_check.py	(date 1719626239000)
@@ -0,0 +1,195 @@
+#!/usr/bin/env python
+
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+"""
+
+import base64
+import urllib2
+import ambari_simplejson as json # simplejson is much faster comparing to Python 2.6 json module and has the same functions set.
+import logging
+from resource_management.core.environment import Environment
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions import StackFeature
+
+logger = logging.getLogger()
+RANGER_ADMIN_URL = '{{admin-properties/policymgr_external_url}}'
+ADMIN_USERNAME = '{{ranger-env/admin_username}}'
+ADMIN_PASSWORD = '{{ranger-env/admin_password}}'
+RANGER_ADMIN_USERNAME = '{{ranger-env/ranger_admin_username}}'
+RANGER_ADMIN_PASSWORD = '{{ranger-env/ranger_admin_password}}'
+SECURITY_ENABLED = '{{cluster-env/security_enabled}}'
+
+def get_tokens():
+  """
+  Returns a tuple of tokens in the format {{site/property}} that will be used
+  to build the dictionary passed into execute
+
+  :return tuple
+  """
+  return (RANGER_ADMIN_URL, ADMIN_USERNAME, ADMIN_PASSWORD, RANGER_ADMIN_USERNAME, RANGER_ADMIN_PASSWORD, SECURITY_ENABLED)
+
+
+def execute(configurations={}, parameters={}, host_name=None):
+  """
+  Returns a tuple containing the result code and a pre-formatted result label
+
+  Keyword arguments:
+  configurations (dictionary): a mapping of configuration key to value
+  parameters (dictionary): a mapping of script parameter key to value
+  host_name (string): the name of this host where the alert is running
+  """
+
+  if configurations is None:
+    return (('UNKNOWN', ['There were no configurations supplied to the script.']))
+
+  ranger_link = None
+  ranger_auth_link = None
+  ranger_get_user = None
+  admin_username = None
+  admin_password = None
+  ranger_admin_username = None
+  ranger_admin_password = None
+  security_enabled = False
+
+  stack_version_formatted = Script.get_stack_version()
+  stack_supports_ranger_kerberos = stack_version_formatted and check_stack_feature(StackFeature.RANGER_KERBEROS_SUPPORT, stack_version_formatted)
+
+  if RANGER_ADMIN_URL in configurations:
+    ranger_link = configurations[RANGER_ADMIN_URL]
+    if ranger_link.endswith('/'):
+      ranger_link = ranger_link[:-1]
+    ranger_auth_link = '{0}/{1}'.format(ranger_link, 'service/public/api/repository/count')
+    ranger_get_user = '{0}/{1}'.format(ranger_link, 'service/xusers/users')
+
+  if ADMIN_USERNAME in configurations:
+    admin_username = configurations[ADMIN_USERNAME]
+
+  if ADMIN_PASSWORD in configurations:
+    admin_password = configurations[ADMIN_PASSWORD]
+
+  if RANGER_ADMIN_USERNAME in configurations:
+    ranger_admin_username = configurations[RANGER_ADMIN_USERNAME]
+
+  if RANGER_ADMIN_PASSWORD in configurations:
+    ranger_admin_password = configurations[RANGER_ADMIN_PASSWORD]
+
+  if SECURITY_ENABLED in configurations:
+    security_enabled = str(configurations[SECURITY_ENABLED]).upper() == 'TRUE'
+
+  label = None
+  result_code = 'OK'
+
+  try:
+    if security_enabled and stack_supports_ranger_kerberos:
+      result_code = 'UNKNOWN'
+      label = 'This alert will get skipped for Ranger Admin on kerberos env'
+    else:
+      admin_http_code = check_ranger_login(ranger_auth_link, admin_username, admin_password)
+      if admin_http_code == 200:
+        get_user_code = get_ranger_user(ranger_get_user, admin_username, admin_password, ranger_admin_username)
+        if get_user_code:
+          user_http_code = check_ranger_login(ranger_auth_link, ranger_admin_username, ranger_admin_password)
+          if user_http_code == 200:
+            result_code = 'OK'
+            label = 'Login Successful for users {0} and {1}'.format(admin_username, ranger_admin_username)
+          elif user_http_code == 401:
+            result_code = 'CRITICAL'
+            label = 'User:{0} credentials on Ambari UI are not in sync with Ranger'.format(ranger_admin_username)
+          else:
+            result_code = 'WARNING'
+            label = 'Ranger Admin service is not reachable, please restart the service'
+        else:
+          result_code = 'OK'
+          label = 'Login Successful for user: {0}. User:{1} user not yet synced with Ranger'.format(admin_username, ranger_admin_username)
+      elif admin_http_code == 401:
+        result_code = 'CRITICAL'
+        label = 'User:{0} credentials on Ambari UI are not in sync with Ranger'.format(admin_username)
+      else:
+        result_code = 'WARNING'
+        label = 'Ranger Admin service is not reachable, please restart the service'
+
+  except Exception, e:
+    label = str(e)
+    result_code = 'UNKNOWN'
+    logger.exception(label)
+
+  return ((result_code, [label]))
+
+def check_ranger_login(ranger_auth_link, username, password):
+  """
+  params ranger_auth_link: ranger login url
+  params username: user credentials
+  params password: user credentials
+
+  return response code
+  """
+  try:
+    usernamepassword = '{0}:{1}'.format(username, password)
+    base_64_string = base64.encodestring(usernamepassword).replace('\n', '')
+    request = urllib2.Request(ranger_auth_link)
+    request.add_header("Content-Type", "application/json")
+    request.add_header("Accept", "application/json")
+    request.add_header("Authorization", "Basic {0}".format(base_64_string))
+    result = urllib2.urlopen(request, timeout=20)
+    response_code = result.getcode()
+    if response_code == 200:
+      response = json.loads(result.read())
+    return response_code
+  except urllib2.HTTPError, e:
+    logger.exception("Error during Ranger service authentication. Http status code - {0}. {1}".format(e.code, e.read()))
+    return e.code
+  except urllib2.URLError, e:
+    logger.exception("Error during Ranger service authentication. {0}".format(e.reason))
+    return None
+  except Exception, e:
+    return 401
+
+def get_ranger_user(ranger_get_user, username, password, user):
+  """
+  params ranger_get_user: ranger get user url
+  params username: user credentials
+  params password: user credentials
+  params user: user to be search
+  return Boolean if user exist or not
+  """
+  try:
+    url = '{0}?name={1}'.format(ranger_get_user, user)
+    usernamepassword = '{0}:{1}'.format(username, password)
+    base_64_string = base64.encodestring(usernamepassword).replace('\n', '')
+    request = urllib2.Request(url)
+    request.add_header("Content-Type", "application/json")
+    request.add_header("Accept", "application/json")
+    request.add_header("Authorization", "Basic {0}".format(base_64_string))
+    result = urllib2.urlopen(request, timeout=20)
+    response_code = result.getcode()
+    response = json.loads(result.read())
+    if response_code == 200 and len(response['vXUsers']) > 0:
+      for xuser in response['vXUsers']:
+        if xuser['name'] == user:
+          return True
+    else:
+      return False
+  except urllib2.HTTPError, e:
+    logger.exception("Error getting user from Ranger service. Http status code - {0}. {1}".format(e.code, e.read()))
+    return False
+  except urllib2.URLError, e:
+    logger.exception("Error getting user from Ranger service. {0}".format(e.reason))
+    return False
+  except Exception, e:
+    return False
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/params.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/params.py	(date 1719626239000)
@@ -0,0 +1,507 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+import os
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions.version import format_stack_version
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.is_empty import is_empty
+from resource_management.libraries.functions.constants import Direction
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions.stack_features import get_stack_feature_version
+from resource_management.libraries.functions import StackFeature, stack_select
+from resource_management.libraries.functions.get_bare_principal import get_bare_principal
+from resource_management.libraries.functions.get_kinit_path import get_kinit_path
+from resource_management.core.exceptions import Fail
+
+# a map of the Ambari role to the component name
+# for use with <stack-root>/current/<component>
+SERVER_ROLE_DIRECTORY_MAP = {
+  'RANGER_ADMIN' : 'ranger-admin',
+  'RANGER_USERSYNC' : 'ranger-usersync',
+  'RANGER_TAGSYNC' : 'ranger-tagsync'
+}
+
+component_directory = Script.get_component_from_role(SERVER_ROLE_DIRECTORY_MAP, "RANGER_ADMIN")
+
+config  = Script.get_config()
+tmp_dir = Script.get_tmp_dir()
+stack_root = Script.get_stack_root()
+
+hadoop_home = stack_select.get_hadoop_dir("home")
+
+stack_name = default("/clusterLevelParams/stack_name", None)
+version = default("/commandParams/version", None)
+
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+
+upgrade_marker_file = format("{tmp_dir}/rangeradmin_ru.inprogress")
+
+xml_configurations_supported = config['configurations']['ranger-env']['xml_configurations_supported']
+
+create_db_dbuser = config['configurations']['ranger-env']['create_db_dbuser']
+
+# get the correct version to use for checking stack features
+version_for_stack_feature_checks = get_stack_feature_version(config)
+
+stack_supports_rolling_upgrade = check_stack_feature(StackFeature.ROLLING_UPGRADE, version_for_stack_feature_checks)
+stack_supports_config_versioning = check_stack_feature(StackFeature.CONFIG_VERSIONING, version_for_stack_feature_checks)
+stack_supports_usersync_non_root = check_stack_feature(StackFeature.RANGER_USERSYNC_NON_ROOT, version_for_stack_feature_checks)
+stack_supports_ranger_tagsync = check_stack_feature(StackFeature.RANGER_TAGSYNC_COMPONENT, version_for_stack_feature_checks)
+stack_supports_ranger_audit_db = check_stack_feature(StackFeature.RANGER_AUDIT_DB_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_log4j = check_stack_feature(StackFeature.RANGER_LOG4J_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_logback = check_stack_feature(StackFeature.RANGER_LOGBACK_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_kerberos = check_stack_feature(StackFeature.RANGER_KERBEROS_SUPPORT, version_for_stack_feature_checks)
+stack_supports_usersync_passwd = check_stack_feature(StackFeature.RANGER_USERSYNC_PASSWORD_JCEKS, version_for_stack_feature_checks)
+stack_supports_infra_client = check_stack_feature(StackFeature.RANGER_INSTALL_INFRA_CLIENT, version_for_stack_feature_checks)
+stack_supports_pid = check_stack_feature(StackFeature.RANGER_PID_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_admin_password_change = check_stack_feature(StackFeature.RANGER_ADMIN_PASSWD_CHANGE, version_for_stack_feature_checks)
+stack_supports_ranger_setup_db_on_start = check_stack_feature(StackFeature.RANGER_SETUP_DB_ON_START, version_for_stack_feature_checks)
+stack_supports_ranger_tagsync_ssl_xml_support = check_stack_feature(StackFeature.RANGER_TAGSYNC_SSL_XML_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_solr_configs = check_stack_feature(StackFeature.RANGER_SOLR_CONFIG_SUPPORT, version_for_stack_feature_checks)
+stack_supports_secure_ssl_password = check_stack_feature(StackFeature.SECURE_RANGER_SSL_PASSWORD, version_for_stack_feature_checks)
+stack_supports_multiple_env_sh_files = check_stack_feature(StackFeature.MULTIPLE_ENV_SH_FILES_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_all_admin_change_default_password = check_stack_feature(StackFeature.RANGER_ALL_ADMIN_CHANGE_DEFAULT_PASSWORD, version_for_stack_feature_checks)
+stack_supports_ranger_zone_feature = check_stack_feature(StackFeature.RANGER_SUPPORT_SECURITY_ZONE_FEATURE, version_for_stack_feature_checks)
+
+upgrade_direction = default("/commandParams/upgrade_direction", None)
+
+ranger_home = format('{stack_root}/current/ranger-admin')
+ranger_conf = format('{stack_root}/current/ranger-admin/conf')
+ranger_admin_services_file = format('{stack_root}/current/ranger-admin/ews/ranger-admin-services.sh')
+
+usersync_home  = format('{stack_root}/current/ranger-usersync')
+ranger_ugsync_conf = format('{stack_root}/current/ranger-usersync/conf')
+usersync_services_file = format('{stack_root}/current/ranger-usersync/ranger-usersync-services.sh')
+
+ranger_tagsync_home  = format('{stack_root}/current/ranger-tagsync')
+ranger_tagsync_conf = format('{stack_root}/current/ranger-tagsync/conf')
+tagsync_services_file = format('{stack_root}/current/ranger-tagsync/ranger-tagsync-services.sh')
+
+security_store_path = '/etc/security/serverKeys'
+tagsync_etc_path = '/etc/ranger/tagsync'
+ranger_tagsync_credential_file = os.path.join(tagsync_etc_path, 'rangercred.jceks')
+atlas_tagsync_credential_file = os.path.join(tagsync_etc_path, 'atlascred.jceks')
+ranger_tagsync_keystore_password = config['configurations']['ranger-tagsync-policymgr-ssl']['xasecure.policymgr.clientssl.keystore.password']
+ranger_tagsync_truststore_password = config['configurations']['ranger-tagsync-policymgr-ssl']['xasecure.policymgr.clientssl.truststore.password']
+atlas_tagsync_keystore_password = config['configurations']['atlas-tagsync-ssl']['xasecure.policymgr.clientssl.keystore.password']
+atlas_tagsync_truststore_password = config['configurations']['atlas-tagsync-ssl']['xasecure.policymgr.clientssl.truststore.password']
+
+if upgrade_direction == Direction.DOWNGRADE and not check_stack_feature(StackFeature.CONFIG_VERSIONING, version_for_stack_feature_checks):
+  stack_supports_rolling_upgrade = True
+  stack_supports_config_versioning = False
+
+if upgrade_direction == Direction.DOWNGRADE and not check_stack_feature(StackFeature.RANGER_USERSYNC_NON_ROOT, version_for_stack_feature_checks):
+  stack_supports_usersync_non_root = False
+
+ranger_stop = format('{ranger_admin_services_file} stop')
+ranger_start = format('{ranger_admin_services_file} start')
+
+usersync_stop  = format('{usersync_services_file} stop')
+usersync_start = format('{usersync_services_file} start')
+
+java_home = config['ambariLevelParams']['java_home']
+unix_user  = config['configurations']['ranger-env']['ranger_user']
+unix_group = config['configurations']['ranger-env']['ranger_group']
+ranger_pid_dir = default("/configurations/ranger-env/ranger_pid_dir", "/var/run/ranger")
+usersync_log_dir = default("/configurations/ranger-ugsync-site/ranger.usersync.logdir", "/var/log/ranger/usersync")
+admin_log_dir = default("/configurations/ranger-admin-site/ranger.logs.base.dir", "/var/log/ranger/admin")
+ranger_admin_default_file = format('{ranger_conf}/ranger-admin-default-site.xml')
+security_app_context_file = format('{ranger_conf}/security-applicationContext.xml')
+ranger_ugsync_default_file = format('{ranger_ugsync_conf}/ranger-ugsync-default.xml')
+usgsync_log4j_file = format('{ranger_ugsync_conf}/log4j.xml')
+if stack_supports_ranger_log4j:
+  usgsync_log4j_file = format('{ranger_ugsync_conf}/log4j.properties')
+cred_validator_file = format('{usersync_home}/native/credValidator.uexe')
+pam_cred_validator_file = format('{usersync_home}/native/pamCredValidator.uexe')
+
+db_flavor =  (config['configurations']['admin-properties']['DB_FLAVOR']).lower()
+usersync_exturl =  config['configurations']['admin-properties']['policymgr_external_url']
+if usersync_exturl.endswith('/'):
+  usersync_exturl = usersync_exturl.rstrip('/')
+ranger_host = config['clusterHostInfo']['ranger_admin_hosts'][0]
+ugsync_host = 'localhost'
+usersync_host_info = config['clusterHostInfo']['ranger_usersync_hosts']
+if not is_empty(usersync_host_info) and len(usersync_host_info) > 0:
+  ugsync_host = config['clusterHostInfo']['ranger_usersync_hosts'][0]
+ranger_external_url = config['configurations']['admin-properties']['policymgr_external_url']
+if ranger_external_url.endswith('/'):
+  ranger_external_url = ranger_external_url.rstrip('/')
+ranger_db_name = config['configurations']['admin-properties']['db_name']
+ranger_auditdb_name = default('/configurations/admin-properties/audit_db_name', 'ranger_audits')
+
+sql_command_invoker = config['configurations']['admin-properties']['SQL_COMMAND_INVOKER']
+db_root_user = config['configurations']['admin-properties']['db_root_user']
+db_root_password = unicode(config['configurations']['admin-properties']['db_root_password'])
+db_host =  config['configurations']['admin-properties']['db_host']
+ranger_db_user = config['configurations']['admin-properties']['db_user']
+ranger_audit_db_user = default('/configurations/admin-properties/audit_db_user', 'rangerlogger')
+ranger_db_password = unicode(config['configurations']['admin-properties']['db_password'])
+
+#ranger-env properties
+oracle_home = default("/configurations/ranger-env/oracle_home", "-")
+
+#For curl command in ranger to get db connector
+jdk_location = config['ambariLevelParams']['jdk_location']
+java_share_dir = '/usr/share/java'
+jdbc_jar_name = None
+previous_jdbc_jar_name = None
+if db_flavor.lower() == 'mysql':
+  jdbc_jar_name = default("/ambariLevelParams/custom_mysql_jdbc_name", None)
+  previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mysql_jdbc_name", None)
+  audit_jdbc_url = format('jdbc:mysql://{db_host}/{ranger_auditdb_name}') if stack_supports_ranger_audit_db else None
+  jdbc_dialect = "org.eclipse.persistence.platform.database.MySQLPlatform"
+elif db_flavor.lower() == 'oracle':
+  jdbc_jar_name = default("/ambariLevelParams/custom_oracle_jdbc_name", None)
+  previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_oracle_jdbc_name", None)
+  jdbc_dialect = "org.eclipse.persistence.platform.database.OraclePlatform"
+  colon_count = db_host.count(':')
+  if colon_count == 2 or colon_count == 0:
+    audit_jdbc_url = format('jdbc:oracle:thin:@{db_host}') if stack_supports_ranger_audit_db else None
+  else:
+    audit_jdbc_url = format('jdbc:oracle:thin:@//{db_host}') if stack_supports_ranger_audit_db else None
+elif db_flavor.lower() == 'postgres':
+  jdbc_jar_name = default("/ambariLevelParams/custom_postgres_jdbc_name", None)
+  previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_postgres_jdbc_name", None)
+  audit_jdbc_url = format('jdbc:postgresql://{db_host}/{ranger_auditdb_name}') if stack_supports_ranger_audit_db else None
+  jdbc_dialect = "org.eclipse.persistence.platform.database.PostgreSQLPlatform"
+elif db_flavor.lower() == 'mssql':
+  jdbc_jar_name = default("/ambariLevelParams/custom_mssql_jdbc_name", None)
+  previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_mssql_jdbc_name", None)
+  audit_jdbc_url = format('jdbc:sqlserver://{db_host};databaseName={ranger_auditdb_name}') if stack_supports_ranger_audit_db else None
+  jdbc_dialect = "org.eclipse.persistence.platform.database.SQLServerPlatform"
+elif db_flavor.lower() == 'sqla':
+  jdbc_jar_name = default("/ambariLevelParams/custom_sqlanywhere_jdbc_name", None)
+  previous_jdbc_jar_name = default("/ambariLevelParams/previous_custom_sqlanywhere_jdbc_name", None)
+  audit_jdbc_url = format('jdbc:sqlanywhere:database={ranger_auditdb_name};host={db_host}') if stack_supports_ranger_audit_db else None
+  jdbc_dialect = "org.eclipse.persistence.platform.database.SQLAnywherePlatform"
+else: raise Fail(format("'{db_flavor}' db flavor not supported."))
+
+downloaded_custom_connector = format("{tmp_dir}/{jdbc_jar_name}")
+
+driver_curl_source = format("{jdk_location}/{jdbc_jar_name}")
+driver_curl_target = format("{java_share_dir}/{jdbc_jar_name}")
+previous_jdbc_jar = format("{java_share_dir}/{previous_jdbc_jar_name}")
+if stack_supports_config_versioning:
+  driver_curl_target = format("{ranger_home}/ews/lib/{jdbc_jar_name}")
+  previous_jdbc_jar = format("{ranger_home}/ews/lib/{previous_jdbc_jar_name}")
+
+if db_flavor.lower() == 'sqla':
+  downloaded_custom_connector = format("{tmp_dir}/sqla-client-jdbc.tar.gz")
+  jar_path_in_archive = format("{tmp_dir}/sqla-client-jdbc/java/sajdbc4.jar")
+  libs_path_in_archive = format("{tmp_dir}/sqla-client-jdbc/native/lib64/*")
+  jdbc_libs_dir = format("{ranger_home}/native/lib64")
+  ld_lib_path = format("{jdbc_libs_dir}")
+
+#for db connection
+check_db_connection_jar_name = "DBConnectionVerification.jar"
+check_db_connection_jar = format("/usr/lib/ambari-agent/{check_db_connection_jar_name}")
+ranger_jdbc_connection_url = config["configurations"]["ranger-admin-site"]["ranger.jpa.jdbc.url"]
+ranger_jdbc_driver = config["configurations"]["ranger-admin-site"]["ranger.jpa.jdbc.driver"]
+
+ranger_credential_provider_path = config["configurations"]["ranger-admin-site"]["ranger.credential.provider.path"]
+ranger_jpa_jdbc_credential_alias = config["configurations"]["ranger-admin-site"]["ranger.jpa.jdbc.credential.alias"]
+ranger_ambari_db_password = unicode(config["configurations"]["admin-properties"]["db_password"])
+
+ranger_jpa_audit_jdbc_credential_alias = default('/configurations/ranger-admin-site/ranger.jpa.audit.jdbc.credential.alias', 'rangeraudit')
+ranger_ambari_audit_db_password = ''
+if not is_empty(config["configurations"]["admin-properties"]["audit_db_password"]) and stack_supports_ranger_audit_db:
+  ranger_ambari_audit_db_password = unicode(config["configurations"]["admin-properties"]["audit_db_password"])
+
+ugsync_jceks_path = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.credstore.filename"]
+ugsync_cred_lib = os.path.join(usersync_home,"lib","*")
+cred_lib_path = os.path.join(ranger_home,"cred","lib","*")
+cred_setup_prefix = (format('{ranger_home}/ranger_credential_helper.py'), '-l', cred_lib_path)
+ranger_audit_source_type = config["configurations"]["ranger-admin-site"]["ranger.audit.source.type"]
+
+ranger_usersync_keystore_password = unicode(config["configurations"]["ranger-ugsync-site"]["ranger.usersync.keystore.password"])
+ranger_usersync_ldap_ldapbindpassword = unicode(config["configurations"]["ranger-ugsync-site"]["ranger.usersync.ldap.ldapbindpassword"])
+ranger_usersync_truststore_password = unicode(config["configurations"]["ranger-ugsync-site"]["ranger.usersync.truststore.password"])
+ranger_usersync_keystore_file = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.keystore.file"]
+default_dn_name = 'cn=unixauthservice,ou=authenticator,o=mycompany,c=US'
+
+ranger_admin_hosts = config['clusterHostInfo']['ranger_admin_hosts']
+is_ranger_ha_enabled = True if len(ranger_admin_hosts) > 1 else False
+ranger_ug_ldap_url = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.ldap.url"]
+ranger_ug_ldap_bind_dn = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.ldap.binddn"]
+ranger_ug_ldap_user_searchfilter = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.ldap.user.searchfilter"]
+ranger_ug_ldap_group_searchbase = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.group.searchbase"]
+ranger_ug_ldap_group_searchfilter = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.group.searchfilter"]
+ug_sync_source = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.source.impl.class"]
+current_host = config['agentLevelParams']['hostname']
+if current_host in ranger_admin_hosts:
+  ranger_host = current_host
+
+# ranger-tagsync
+ranger_tagsync_hosts = default("/clusterHostInfo/ranger_tagsync_hosts", [])
+has_ranger_tagsync = len(ranger_tagsync_hosts) > 0
+
+tagsync_log_dir = default("/configurations/ranger-tagsync-site/ranger.tagsync.logdir", "/var/log/ranger/tagsync")
+tagsync_jceks_path = config["configurations"]["ranger-tagsync-site"]["ranger.tagsync.keystore.filename"]
+atlas_tagsync_jceks_path = config["configurations"]["ranger-tagsync-site"]["ranger.tagsync.source.atlasrest.keystore.filename"]
+tagsync_application_properties = dict(config["configurations"]["tagsync-application-properties"]) if has_ranger_tagsync else None
+tagsync_pid_file = format('{ranger_pid_dir}/tagsync.pid')
+tagsync_cred_lib = os.path.join(ranger_tagsync_home, "lib", "*")
+
+ranger_usersync_log_maxfilesize = default('/configurations/usersync-log4j/ranger_usersync_log_maxfilesize',256)
+ranger_usersync_log_maxbackupindex = default('/configurations/usersync-log4j/ranger_usersync_log_maxbackupindex',20)
+ranger_tagsync_log_maxfilesize = default('/configurations/tagsync-log4j/ranger_tagsync_log_maxfilesize',256)
+ranger_tagsync_log_number_of_backup_files = default('/configurations/tagsync-log4j/ranger_tagsync_log_number_of_backup_files',20)
+ranger_xa_log_maxfilesize = default('/configurations/admin-log4j/ranger_xa_log_maxfilesize',256)
+ranger_xa_log_maxbackupindex = default('/configurations/admin-log4j/ranger_xa_log_maxbackupindex',20)
+
+# ranger log4j.properties
+admin_log4j = config['configurations']['admin-log4j']['content']
+usersync_log4j = config['configurations']['usersync-log4j']['content']
+tagsync_log4j = config['configurations']['tagsync-log4j']['content']
+
+# ranger logback.xml(from ranger2.3.0)
+admin_logback = config['configurations']['admin-log4j']['logback-content']
+usersync_logback = config['configurations']['usersync-log4j']['logback-content']
+tagsync_logback = config['configurations']['tagsync-log4j']['logback-content']
+
+
+# ranger kerberos
+security_enabled = config['configurations']['cluster-env']['security_enabled']
+namenode_hosts = default("/clusterHostInfo/namenode_hosts", [])
+has_namenode = len(namenode_hosts) > 0
+
+ugsync_policymgr_alias = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.policymgr.alias"]
+ugsync_policymgr_keystore = config["configurations"]["ranger-ugsync-site"]["ranger.usersync.policymgr.keystore"]
+
+# ranger solr
+audit_solr_enabled = default('/configurations/ranger-env/xasecure.audit.destination.solr', False)
+ranger_solr_config_set = config['configurations']['ranger-env']['ranger_solr_config_set']
+ranger_solr_collection_name = config['configurations']['ranger-env']['ranger_solr_collection_name']
+ranger_solr_shards = config['configurations']['ranger-env']['ranger_solr_shards']
+replication_factor = config['configurations']['ranger-env']['ranger_solr_replication_factor']
+ranger_solr_conf = format('{ranger_home}/contrib/solr_for_audit_setup/conf')
+infra_solr_hosts = default("/clusterHostInfo/infra_solr_hosts", [])
+has_infra_solr = len(infra_solr_hosts) > 0
+is_solrCloud_enabled = default('/configurations/ranger-env/is_solrCloud_enabled', False)
+is_external_solrCloud_enabled = default('/configurations/ranger-env/is_external_solrCloud_enabled', False)
+solr_znode = '/ranger_audits'
+if stack_supports_infra_client and is_solrCloud_enabled:
+  solr_znode = default('/configurations/ranger-admin-site/ranger.audit.solr.zookeepers', 'NONE')
+  if solr_znode != '' and solr_znode.upper() != 'NONE':
+    solr_znode = solr_znode.split('/')
+    if len(solr_znode) > 1 and len(solr_znode) == 2:
+      solr_znode = solr_znode[1]
+      solr_znode = format('/{solr_znode}')
+  if has_infra_solr and not is_external_solrCloud_enabled:
+    solr_znode = config['configurations']['infra-solr-env']['infra_solr_znode']
+solr_user = unix_user
+if has_infra_solr and not is_external_solrCloud_enabled:
+  solr_user = default('/configurations/infra-solr-env/infra_solr_user', unix_user)
+  infra_solr_role_ranger_admin = default('configurations/infra-solr-security-json/infra_solr_role_ranger_admin', 'ranger_user')
+  infra_solr_role_ranger_audit = default('configurations/infra-solr-security-json/infra_solr_role_ranger_audit', 'ranger_audit_user')
+  infra_solr_role_dev = default('configurations/infra-solr-security-json/infra_solr_role_dev', 'dev')
+custom_log4j = has_infra_solr and not is_external_solrCloud_enabled
+
+ranger_audit_max_retention_days = config['configurations']['ranger-solr-configuration']['ranger_audit_max_retention_days']
+ranger_audit_logs_merge_factor = config['configurations']['ranger-solr-configuration']['ranger_audit_logs_merge_factor']
+ranger_solr_config_content = config['configurations']['ranger-solr-configuration']['content']
+
+# get comma separated list of zookeeper hosts
+zookeeper_port = default('/configurations/zoo.cfg/clientPort', None)
+zookeeper_hosts = default("/clusterHostInfo/zookeeper_server_hosts", [])
+index = 0
+zookeeper_quorum = ""
+for host in zookeeper_hosts:
+  zookeeper_quorum += host + ":" + str(zookeeper_port)
+  index += 1
+  if index < len(zookeeper_hosts):
+    zookeeper_quorum += ","
+
+# solr kerberised
+solr_jaas_file = None
+is_external_solrCloud_kerberos = default('/configurations/ranger-env/is_external_solrCloud_kerberos', False)
+
+if security_enabled:
+  if has_ranger_tagsync:
+    ranger_tagsync_principal = config['configurations']['ranger-tagsync-site']['ranger.tagsync.kerberos.principal']
+    if not is_empty(ranger_tagsync_principal) and ranger_tagsync_principal != '':
+      tagsync_jaas_principal = ranger_tagsync_principal.replace('_HOST', current_host.lower())
+    tagsync_keytab_path = config['configurations']['ranger-tagsync-site']['ranger.tagsync.kerberos.keytab']
+
+  if stack_supports_ranger_kerberos:
+    ranger_admin_keytab = config['configurations']['ranger-admin-site']['ranger.admin.kerberos.keytab']
+    ranger_admin_principal = config['configurations']['ranger-admin-site']['ranger.admin.kerberos.principal']
+    if not is_empty(ranger_admin_principal) and ranger_admin_principal != '':
+      ranger_admin_jaas_principal = ranger_admin_principal.replace('_HOST', ranger_host.lower())
+      if stack_supports_infra_client and is_solrCloud_enabled and is_external_solrCloud_enabled and is_external_solrCloud_kerberos:
+        solr_jaas_file = format('{ranger_home}/conf/ranger_solr_jaas.conf')
+        solr_kerberos_principal = ranger_admin_jaas_principal
+        solr_kerberos_keytab = ranger_admin_keytab
+      if stack_supports_infra_client and is_solrCloud_enabled and not is_external_solrCloud_enabled and not is_external_solrCloud_kerberos:
+        solr_jaas_file = format('{ranger_home}/conf/ranger_solr_jaas.conf')
+        solr_kerberos_principal = ranger_admin_jaas_principal
+        solr_kerberos_keytab = ranger_admin_keytab
+
+# logic to create core-site.xml if hdfs not installed
+if stack_supports_ranger_kerberos and not has_namenode:
+  core_site_property = {
+    'hadoop.security.authentication': 'kerberos' if security_enabled else 'simple'
+  }
+
+  if security_enabled:
+    realm = 'EXAMPLE.COM'
+    ranger_admin_bare_principal = 'rangeradmin'
+    ranger_usersync_bare_principal = 'rangerusersync'
+    ranger_tagsync_bare_principal = 'rangertagsync'
+
+    ranger_usersync_principal = config['configurations']['ranger-ugsync-site']['ranger.usersync.kerberos.principal']
+    if not is_empty(ranger_admin_principal) and ranger_admin_principal != '':
+      ranger_admin_bare_principal = get_bare_principal(ranger_admin_principal)
+    if not is_empty(ranger_usersync_principal) and ranger_usersync_principal != '':
+      ranger_usersync_bare_principal = get_bare_principal(ranger_usersync_principal)
+    realm = config['configurations']['kerberos-env']['realm']
+
+    rule_dict = [
+      {'principal': ranger_admin_bare_principal, 'user': unix_user},
+      {'principal': ranger_usersync_bare_principal, 'user': 'rangerusersync'},
+    ]
+
+    if has_ranger_tagsync:
+      if not is_empty(ranger_tagsync_principal) and ranger_tagsync_principal != '':
+        ranger_tagsync_bare_principal = get_bare_principal(ranger_tagsync_principal)
+      rule_dict.append({'principal': ranger_tagsync_bare_principal, 'user': 'rangertagsync'})
+
+    core_site_auth_to_local_property = ''
+    for item in range(len(rule_dict)):
+      rule_line = 'RULE:[2:$1@$0]({0}@{1})s/.*/{2}/\n'.format(rule_dict[item]['principal'], realm, rule_dict[item]['user'])
+      core_site_auth_to_local_property = rule_line + core_site_auth_to_local_property
+
+    core_site_auth_to_local_property = core_site_auth_to_local_property + 'DEFAULT'
+    core_site_property['hadoop.security.auth_to_local'] = core_site_auth_to_local_property
+
+upgrade_type = Script.get_upgrade_type(default("/commandParams/upgrade_type", ""))
+
+# ranger service pid
+user_group = config['configurations']['cluster-env']['user_group']
+ranger_admin_pid_file = format('{ranger_pid_dir}/rangeradmin.pid')
+ranger_usersync_pid_file = format('{ranger_pid_dir}/usersync.pid')
+
+# admin credential
+admin_username = config['configurations']['ranger-env']['admin_username']
+admin_password = config['configurations']['ranger-env']['admin_password']
+default_admin_password = 'admin'
+
+ranger_is_solr_kerberised = "false"
+if audit_solr_enabled and is_solrCloud_enabled:
+  # Check internal solrCloud
+  if security_enabled and not is_external_solrCloud_enabled:
+    ranger_is_solr_kerberised = "true"
+  # Check external solrCloud
+  if is_external_solrCloud_enabled and is_external_solrCloud_kerberos:
+    ranger_is_solr_kerberised = "true"
+
+hbase_master_hosts = default("/clusterHostInfo/hbase_master_hosts", [])
+is_hbase_ha_enabled = True if len(hbase_master_hosts) > 1 else False
+is_namenode_ha_enabled = True if len(namenode_hosts) > 1 else False
+ranger_hbase_plugin_enabled = False
+ranger_hdfs_plugin_enabled = False
+
+
+if is_hbase_ha_enabled:
+  if not is_empty(config['configurations']['ranger-hbase-plugin-properties']['ranger-hbase-plugin-enabled']):
+    ranger_hbase_plugin_enabled = config['configurations']['ranger-hbase-plugin-properties']['ranger-hbase-plugin-enabled'].lower() == 'yes'
+if is_namenode_ha_enabled:
+  if not is_empty(config['configurations']['ranger-hdfs-plugin-properties']['ranger-hdfs-plugin-enabled']):
+    ranger_hdfs_plugin_enabled = config['configurations']['ranger-hdfs-plugin-properties']['ranger-hdfs-plugin-enabled'].lower() == 'yes'
+
+ranger_admin_password_properties = ['ranger.jpa.jdbc.password', 'ranger.jpa.audit.jdbc.password', 'ranger.ldap.bind.password', 'ranger.ldap.ad.bind.password']
+ranger_usersync_password_properties = ['ranger.usersync.ldap.ldapbindpassword']
+ranger_tagsync_password_properties = ['xasecure.policymgr.clientssl.keystore.password', 'xasecure.policymgr.clientssl.truststore.password']
+if stack_supports_secure_ssl_password:
+  ranger_admin_password_properties.extend(['ranger.service.https.attrib.keystore.pass', 'ranger.truststore.password'])
+  ranger_usersync_password_properties.extend(['ranger.usersync.keystore.password', 'ranger.usersync.truststore.password'])
+
+ranger_auth_method = config['configurations']['ranger-admin-site']['ranger.authentication.method']
+ranger_ldap_password_alias = default('/configurations/ranger-admin-site/ranger.ldap.binddn.credential.alias', 'ranger.ldap.bind.password')
+ranger_ad_password_alias = default('/configurations/ranger-admin-site/ranger.ldap.ad.binddn.credential.alias', 'ranger.ldap.ad.bind.password')
+ranger_https_keystore_alias = default('/configurations/ranger-admin-site/ranger.service.https.attrib.keystore.credential.alias', 'keyStoreCredentialAlias')
+ranger_truststore_alias = default('/configurations/ranger-admin-site/ranger.truststore.alias', 'trustStoreAlias')
+https_enabled = config['configurations']['ranger-admin-site']['ranger.service.https.attrib.ssl.enabled']
+http_enabled = config['configurations']['ranger-admin-site']['ranger.service.http.enabled']
+https_keystore_password = config['configurations']['ranger-admin-site']['ranger.service.https.attrib.keystore.pass']
+truststore_password = config['configurations']['ranger-admin-site']['ranger.truststore.password']
+
+# need this to capture cluster name for ranger tagsync
+cluster_name = config['clusterName']
+ranger_ldap_bind_auth_password = config['configurations']['ranger-admin-site']['ranger.ldap.bind.password']
+ranger_ad_bind_auth_password = config['configurations']['ranger-admin-site']['ranger.ldap.ad.bind.password']
+
+ranger_env_content = config['configurations']['ranger-env']['content']
+is_ranger_admin_host = 'role' in config and config['role'] == "RANGER_ADMIN"
+is_ranger_usersync_host = 'role' in config and config['role'] == "RANGER_USERSYNC"
+is_ranger_tagsync_host = 'role' in config and config['role'] == "RANGER_TAGSYNC"
+
+# zookeeper principal
+zookeeper_principal = default("/configurations/zookeeper-env/zookeeper_principal_name", "zookeeper@EXAMPLE.COM")
+zookeeper_principal_primary = get_bare_principal(zookeeper_principal)
+
+# rangerusersync user credential
+rangerusersync_username = 'rangerusersync'
+rangerusersync_user_password = config['configurations']['ranger-env']['rangerusersync_user_password']
+default_rangerusersync_user_password = 'rangerusersync'
+
+# rangertagsync user credential
+rangertagsync_username = 'rangertagsync'
+rangertagsync_user_password = config['configurations']['ranger-env']['rangertagsync_user_password']
+default_rangertagsync_user_password = 'rangertagsync'
+
+# keyadmin user credential
+keyadmin_username = 'keyadmin'
+keyadmin_user_password = config['configurations']['ranger-env']['keyadmin_user_password']
+default_keyadmin_user_password = 'keyadmin'
+
+# atlas admin user password
+atlas_admin_password = default("/configurations/atlas-env/atlas.admin.password", "admin")
+
+mount_table_content = None
+if 'viewfs-mount-table' in config['configurations']:
+  xml_inclusion_file_name = 'viewfs-mount-table.xml'
+  mount_table = config['configurations']['viewfs-mount-table']
+
+  if 'content' in mount_table and mount_table['content'].strip():
+    mount_table_content = mount_table['content']
+
+# Ranger Services maximum heap size configurations
+ranger_admin_max_heap_size=default('/configurations/ranger-env/ranger_admin_max_heap_size','1g')
+ranger_usersync_max_heap_size=default('/configurations/ranger-env/ranger_usersync_max_heap_size','1g')
+ranger_tagsync_max_heap_size=default('/configurations/ranger-env/ranger_tagsync_max_heap_size','1g')
+
+# add zoneName field in ranger_audits collection when the current stack support security zone feature during upgrade
+add_zoneName_field = {
+  "add-field" : {
+    "name" : "zoneName",
+    "type" :"key_lower_case",
+    "multiValued" : False
+  }
+}
+infra_solr_ssl_enabled = default("/configurations/infra-solr-env/infra_solr_ssl_enabled", False)
+infra_solr_protocol = "https" if infra_solr_ssl_enabled else "http"
+infra_solr_port = default("/configurations/infra-solr-env/infra_solr_port", "8886")
+if has_infra_solr:
+  infra_solr_host = infra_solr_hosts[0]
+kinit_path_local = get_kinit_path(default("/configurations/kerberos-env/executable_search_paths", None))
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_client.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_client.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_client.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_client.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/hbase_client.py	(date 1719626239000)
@@ -33,7 +33,7 @@
     import params
     env.set_params(params)
     self.install_packages(env)
-    self.configure(env)
+    #self.configure(env)
 
   def configure(self, env):
     import params
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/upgrade.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/upgrade.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/upgrade.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/upgrade.py	(date 1719626239000)
@@ -0,0 +1,29 @@
+
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.core.resources.system import Execute
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions.format import format
+
+def prestart(env):
+  import params
+
+  if params.version and params.stack_supports_rolling_upgrade:
+    stack_select.select_packages(params.version)
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_admin.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_admin.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_admin.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_admin.py	(date 1719626239000)
@@ -0,0 +1,258 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.core.exceptions import Fail, ExecutionFailed
+from resource_management.libraries.functions.check_process_status import check_process_status
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions import upgrade_summary
+from resource_management.libraries.functions.constants import Direction
+from resource_management.libraries.script import Script
+from resource_management.core.resources.system import Execute, File
+from resource_management.core.exceptions import ComponentIsNotRunning
+from resource_management.libraries.functions.format import format
+from resource_management.core.logger import Logger
+from resource_management.core import shell
+from ranger_service import ranger_service
+from resource_management.libraries.functions import solr_cloud_util
+from ambari_commons.constants import UPGRADE_TYPE_NON_ROLLING, UPGRADE_TYPE_ROLLING
+import upgrade
+import os, errno
+import ambari_simplejson as json
+import setup_ranger_xml
+
+class RangerAdmin(Script):
+
+  def install(self, env):
+    self.install_packages(env)
+    import params
+    env.set_params(params)
+
+    # taking backup of install.properties file
+    Execute(('cp', '-f', format('{ranger_home}/install.properties'), format('{ranger_home}/install-backup.properties')),
+      not_if = format('ls {ranger_home}/install-backup.properties'),
+      only_if = format('ls {ranger_home}/install.properties'),
+      sudo = True
+    )
+
+    # call config and setup db only in case of stack version < 2.6
+    if not params.stack_supports_ranger_setup_db_on_start:
+      self.configure(env, setup_db=True)
+
+  def stop(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+
+    Execute(format('{params.ranger_stop}'), environment={'JAVA_HOME': params.java_home}, user=params.unix_user)
+    if params.stack_supports_pid:
+      File(params.ranger_admin_pid_file,
+        action = "delete"
+      )
+
+  def pre_upgrade_restart(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+
+    upgrade.prestart(env)
+
+    self.set_ru_rangeradmin_in_progress(params.upgrade_marker_file)
+
+  def post_upgrade_restart(self,env, upgrade_type=None):
+    import params
+    env.set_params(params)
+
+    if os.path.isfile(params.upgrade_marker_file):
+      os.remove(params.upgrade_marker_file)
+
+    if upgrade_type and params.upgrade_direction == Direction.UPGRADE and not params.stack_supports_multiple_env_sh_files:
+      files_name_list = ['ranger-admin-env-piddir.sh', 'ranger-admin-env-logdir.sh']
+      for file_name in files_name_list:
+        File(format("{ranger_conf}/{file_name}"),
+          action = "delete"
+        )
+
+    if upgrade_type and params.upgrade_direction == Direction.UPGRADE and params.stack_supports_ranger_zone_feature:
+      if params.has_infra_solr and params.audit_solr_enabled and params.is_solrCloud_enabled and not params.is_external_solrCloud_enabled:
+        add_field_json = json.dumps(params.add_zoneName_field)
+        add_field_cmd = format("curl -k -X POST -H 'Content-type:application/json' --data-binary '{add_field_json}' {infra_solr_protocol}://{infra_solr_host}:{infra_solr_port}/solr/{ranger_solr_collection_name}/schema")
+        if params.security_enabled:
+          kinit_cmd = format("{kinit_path_local} -kt {ranger_admin_keytab} {ranger_admin_jaas_principal};")
+          add_field_cmd = format("{kinit_cmd} curl -k --negotiate -u : -X POST -H 'Content-type:application/json' --data-binary '{add_field_json}' {infra_solr_protocol}://{infra_solr_host}:{infra_solr_port}/solr/{ranger_solr_collection_name}/schema")
+        try:
+          Execute(add_field_cmd,
+            tries = 3,
+            try_sleep = 5,
+            user = params.unix_user,
+            logoutput = True
+          )
+        except ExecutionFailed as execution_exception:
+          Logger.error("Error adding field to Ranger Audits Solr Collection. Kindly check Infra Solr service to be up and running {0}".format(execution_exception))
+
+  def start(self, env, upgrade_type=None):
+    import params
+    env.set_params(params)
+
+    if upgrade_type is None:
+      setup_ranger_xml.validate_user_password()
+
+    # setup db only if in case stack version is > 2.6
+    self.configure(env, upgrade_type=upgrade_type, setup_db=params.stack_supports_ranger_setup_db_on_start)
+
+    if params.stack_supports_infra_client and params.audit_solr_enabled and params.is_solrCloud_enabled:
+      solr_cloud_util.setup_solr_client(params.config, custom_log4j = params.custom_log4j)
+      setup_ranger_xml.setup_ranger_audit_solr()
+
+    setup_ranger_xml.update_password_configs()
+    ranger_service('ranger_admin')
+
+  def status(self, env):
+    import status_params
+
+    env.set_params(status_params)
+
+    if status_params.stack_supports_pid:
+      check_process_status(status_params.ranger_admin_pid_file)
+      return
+
+    cmd = 'ps -ef | grep proc_rangeradmin | grep -v grep'
+    code, output = shell.call(cmd, timeout=20)
+
+    if code != 0:
+      if self.is_ru_rangeradmin_in_progress(status_params.upgrade_marker_file):
+        Logger.info('Ranger admin process not running - skipping as stack upgrade is in progress')
+      else:
+        Logger.debug('Ranger admin process not running')
+        raise ComponentIsNotRunning()
+    pass
+
+  def configure(self, env, upgrade_type=None, setup_db=False):
+    import params
+    env.set_params(params)
+
+    # set up db if we are not upgrading and setup_db is true
+    if setup_db and upgrade_type is None:
+      setup_ranger_xml.setup_ranger_db()
+
+    setup_ranger_xml.ranger('ranger_admin', upgrade_type=upgrade_type)
+
+    # set up java patches if we are not upgrading and setup_db is true
+    if setup_db and upgrade_type is None:
+      setup_ranger_xml.setup_java_patch()
+      if params.stack_supports_ranger_all_admin_change_default_password:
+        setup_ranger_xml.setup_ranger_all_admin_password_change(params.admin_username, params.default_admin_password, params.admin_password,
+          params.rangerusersync_username, params.default_rangerusersync_user_password, params.rangerusersync_user_password,
+          params.rangertagsync_username, params.default_rangertagsync_user_password, params.rangertagsync_user_password,
+          params.keyadmin_username, params.default_keyadmin_user_password, params.keyadmin_user_password)
+      else:
+        # Updating password for Ranger Admin user
+        setup_ranger_xml.setup_ranger_admin_passwd_change(params.admin_username, params.admin_password, params.default_admin_password)
+        # Updating password for Ranger Usersync user
+        setup_ranger_xml.setup_ranger_admin_passwd_change(params.rangerusersync_username, params.rangerusersync_user_password, params.default_rangerusersync_user_password)
+        # Updating password for Ranger Tagsync user
+        setup_ranger_xml.setup_ranger_admin_passwd_change(params.rangertagsync_username, params.rangertagsync_user_password, params.default_rangertagsync_user_password)
+        # Updating password for Ranger Keyadmin user
+        setup_ranger_xml.setup_ranger_admin_passwd_change(params.keyadmin_username, params.keyadmin_user_password, params.default_keyadmin_user_password)
+
+  def set_ru_rangeradmin_in_progress(self, upgrade_marker_file):
+    config_dir = os.path.dirname(upgrade_marker_file)
+    try:
+      msg = "Starting Upgrade"
+      if (not os.path.exists(config_dir)):
+        os.makedirs(config_dir)
+      ofp = open(upgrade_marker_file, 'w')
+      ofp.write(msg)
+      ofp.close()
+    except OSError as exc:
+      if exc.errno == errno.EEXIST and os.path.isdir(config_dir):
+        pass
+      else:
+        raise
+
+  def is_ru_rangeradmin_in_progress(self, upgrade_marker_file):
+    return os.path.isfile(upgrade_marker_file)
+
+  def setup_ranger_database(self, env):
+    import params
+    env.set_params(params)
+
+    upgrade_stack = stack_select._get_upgrade_stack()
+    if upgrade_stack is None:
+      raise Fail('Unable to determine the stack and stack version')
+
+    stack_version = upgrade_stack[1]
+
+    if params.upgrade_direction == Direction.UPGRADE:
+      target_version = upgrade_summary.get_target_version("RANGER", default_version = stack_version)
+      Logger.info(format('Setting Ranger database schema, using version {target_version}'))
+
+      setup_ranger_xml.setup_ranger_db(stack_version = target_version)
+
+  def setup_ranger_java_patches(self, env):
+    import params
+    env.set_params(params)
+
+    upgrade_stack = stack_select._get_upgrade_stack()
+    if upgrade_stack is None:
+      raise Fail('Unable to determine the stack and stack version')
+
+    stack_version = upgrade_stack[1]
+
+    if params.upgrade_direction == Direction.UPGRADE:
+      target_version = upgrade_summary.get_target_version("RANGER", default_version = stack_version)
+      Logger.info(format('Applying Ranger java patches, using version {target_version}'))
+
+      setup_ranger_xml.setup_java_patch(stack_version = target_version)
+
+  def set_pre_start(self, env):
+    import params
+    env.set_params(params)
+
+    orchestration = stack_select.PACKAGE_SCOPE_STANDARD
+    summary = upgrade_summary.get_upgrade_summary()
+
+    if summary is not None:
+      orchestration = summary.orchestration
+      if orchestration is None:
+        raise Fail("The upgrade summary does not contain an orchestration type")
+
+      if orchestration.upper() in stack_select._PARTIAL_ORCHESTRATION_SCOPES:
+        orchestration = stack_select.PACKAGE_SCOPE_PATCH
+
+    stack_select_packages = stack_select.get_packages(orchestration, service_name = "RANGER", component_name = "RANGER_ADMIN")
+    if stack_select_packages is None:
+      raise Fail("Unable to get packages for stack-select")
+
+    Logger.info("RANGER_ADMIN component will be stack-selected to version {0} using a {1} orchestration".format(params.version, orchestration.upper()))
+
+    for stack_select_package_name in stack_select_packages:
+      stack_select.select(stack_select_package_name, params.version)
+
+  def get_log_folder(self):
+    import params
+    return params.admin_log_dir
+
+  def get_user(self):
+    import params
+    return params.unix_user
+
+  def get_pid_files(self):
+    import status_params
+    return [status_params.ranger_admin_pid_file]
+
+if __name__ == "__main__":
+  RangerAdmin().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/params_linux.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/params_linux.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/params_linux.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/params_linux.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/params_linux.py	(date 1719626239000)
@@ -135,11 +135,10 @@
 
 underscored_version = stack_version_unformatted.replace('.', '_')
 dashed_version = stack_version_unformatted.replace('.', '-')
-# if OSCheck.is_redhat_family() or OSCheck.is_suse_family():
-#   phoenix_package = format("phoenix_{underscored_version}_*")
-# elif OSCheck.is_ubuntu_family():
-#   phoenix_package = format("phoenix-{dashed_version}-.*")
-phoenix_package = "phoenix"
+if OSCheck.is_redhat_family() or OSCheck.is_suse_family():
+  phoenix_package = format("phoenix_{underscored_version}*")
+elif OSCheck.is_ubuntu_family():
+  phoenix_package = format("phoenix-{dashed_version}*")
 
 pid_dir = status_params.pid_dir
 tmp_dir = config['configurations']['hbase-site']['hbase.tmp.dir']
@@ -391,6 +390,15 @@
   hbase_ranger_plugin_config['default-policy.1.policyItem.1.users'] = policy_user
   hbase_ranger_plugin_config['default-policy.1.policyItem.1.accessTypes'] = "read,write,create"
 
+  #for atlas
+  hbase_ranger_plugin_config['default-policy.2.name'] = "Atlas - table, column-family, column"
+  hbase_ranger_plugin_config['default-policy.2.resource.table'] = "ATLAS_ENTITY_AUDIT_EVENTS,atlas_janus"
+  hbase_ranger_plugin_config['default-policy.2.resource.column-family'] = "*"
+  hbase_ranger_plugin_config['default-policy.2.resource.column'] = "*"
+  hbase_ranger_plugin_config['default-policy.2.policyItem.1.users'] = "atlas"
+  hbase_ranger_plugin_config['default-policy.2.policyItem.1.accessTypes'] = "read,write,create"
+
+
   custom_ranger_service_config = generate_ranger_service_config(ranger_plugin_properties)
   if len(custom_ranger_service_config) > 0:
     hbase_ranger_plugin_config.update(custom_ranger_service_config)
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/service_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/service_check.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/service_check.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/service_check.py	(date 1719626239000)
@@ -0,0 +1,49 @@
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.script import Script
+from resource_management.core.resources.system import Execute
+from resource_management.core.exceptions import ComponentIsNotRunning
+from resource_management.libraries.functions.format import format
+from resource_management.core.logger import Logger
+import os
+
+
+class RangerServiceCheck(Script):
+
+  def service_check(self, env):
+    import params
+
+    env.set_params(params)
+    self.check_ranger_admin_service(params.ranger_external_url, params.upgrade_marker_file)
+
+  def check_ranger_admin_service(self, ranger_external_url, upgrade_marker_file):
+    if (self.is_ru_rangeradmin_in_progress(upgrade_marker_file)):
+      Logger.info('Ranger admin process not running - skipping as stack upgrade is in progress')
+    else:
+      Execute(format("curl -s -o /dev/null -w'%{{http_code}}' --negotiate -u: -k {ranger_external_url}/login.jsp | grep 200"),
+        tries = 10,
+        try_sleep=3,
+        logoutput=True)
+
+  def is_ru_rangeradmin_in_progress(self, upgrade_marker_file):
+    return os.path.isfile(upgrade_marker_file)
+
+if __name__ == "__main__":
+  RangerServiceCheck().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/status_params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/status_params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/status_params.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/status_params.py	(date 1719626239000)
@@ -0,0 +1,39 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.default import default
+from resource_management.libraries.functions.version import format_stack_version
+from resource_management.libraries.functions.stack_features import check_stack_feature
+from resource_management.libraries.functions import StackFeature
+
+config  = Script.get_config()
+tmp_dir = Script.get_tmp_dir()
+
+upgrade_marker_file = format("{tmp_dir}/rangeradmin_ru.inprogress")
+ranger_pid_dir = config['configurations']['ranger-env']['ranger_pid_dir']
+tagsync_pid_file = format('{ranger_pid_dir}/tagsync.pid')
+stack_name = default("/clusterLevelParams/stack_name", None)
+stack_version_unformatted = config['clusterLevelParams']['stack_version']
+stack_version_formatted = format_stack_version(stack_version_unformatted)
+ranger_admin_pid_file = format('{ranger_pid_dir}/rangeradmin.pid')
+ranger_usersync_pid_file = format('{ranger_pid_dir}/usersync.pid')
+stack_supports_pid = stack_version_formatted and check_stack_feature(StackFeature.RANGER_PID_SUPPORT, stack_version_formatted)
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_service.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_service.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_service.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_service.py	(date 1719626239000)
@@ -0,0 +1,61 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.show_logs import show_logs
+from resource_management.core.resources.system import Execute
+
+def ranger_service(name, action=None):
+  import params
+
+  env_dict = {'JAVA_HOME': params.java_home}
+  if params.db_flavor.lower() == 'sqla':
+    env_dict = {'JAVA_HOME': params.java_home, 'LD_LIBRARY_PATH': params.ld_lib_path}
+
+  if name == 'ranger_admin':
+    no_op_test = format('ps -ef | grep proc_rangeradmin | grep -v grep')
+    try:
+      Execute(params.ranger_start, environment=env_dict, user=params.unix_user, not_if=no_op_test)
+    except:
+      show_logs(params.admin_log_dir, params.unix_user)
+      raise
+  elif name == 'ranger_usersync':
+    no_op_test = format('ps -ef | grep proc_rangerusersync | grep -v grep')
+    try:
+      Execute(params.usersync_start,
+              environment=env_dict,
+              not_if=no_op_test,
+              user=params.unix_user
+      )
+    except:
+      show_logs(params.usersync_log_dir, params.unix_user)
+      raise
+  elif name == 'ranger_tagsync' and params.stack_supports_ranger_tagsync:
+    no_op_test = format('ps -ef | grep proc_rangertagsync | grep -v grep')
+    cmd = format('{tagsync_services_file} start')
+    try:
+      Execute(cmd,
+        environment=env_dict,
+        user=params.unix_user,
+        not_if=no_op_test
+      )
+    except:
+      show_logs(params.tagsync_log_dir, params.unix_user)
+      raise
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/service_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/service_check.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/service_check.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/service_check.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/service_check.py	(date 1719626239000)
@@ -70,15 +70,16 @@
     
     if params.security_enabled:    
       hbase_grant_premissions_file = format("{exec_tmp_dir}/hbase_grant_permissions.sh")
+      #grantprivelegecmd = format("{kinit_cmd} {hbase_cmd} shell {hbase_grant_premissions_file}")
       grantprivelegecmd = format("{kinit_cmd} {hbase_grant_premissions_file}")
-
+  
       File( hbase_grant_premissions_file,
         owner   = params.hbase_user,
         group   = params.user_group,
         mode    = 0755,
         content = Template('hbase_grant_permissions.j2')
       )
-
+      
       Execute( grantprivelegecmd,
         tries     = 6,
         try_sleep = 5,
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_tagsync.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_tagsync.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_tagsync.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_tagsync.py	(date 1719626239000)
@@ -0,0 +1,154 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions import stack_select
+from resource_management.libraries.functions import upgrade_summary
+from resource_management.libraries.functions.constants import Direction
+from resource_management.core.resources.system import Execute, File
+from resource_management.libraries.functions.check_process_status import check_process_status
+from resource_management.core.exceptions import ComponentIsNotRunning
+from resource_management.libraries.functions.format import format
+from resource_management.core.logger import Logger
+from resource_management.core import shell
+from ranger_service import ranger_service
+from resource_management.core.exceptions import Fail
+import setup_ranger_xml
+import upgrade
+import os
+
+
+class RangerTagsync(Script):
+
+    def install(self, env):
+        self.install_packages(env)
+
+    def initialize(self, env):
+        import params
+        env.set_params(params)
+        ranger_tagsync_setup_marker = os.path.join(params.ranger_tagsync_conf, "tagsync_setup")
+        if not os.path.exists(ranger_tagsync_setup_marker):
+            setup_ranger_xml.validate_user_password('rangertagsync_user_password')
+
+        setup_ranger_xml.ranger_credential_helper(params.tagsync_cred_lib, 'tagadmin.user.password',
+                                                  params.rangertagsync_user_password, params.tagsync_jceks_path)
+        File(params.tagsync_jceks_path,
+             owner=params.unix_user,
+             group=params.unix_group,
+             only_if=format("test -e {tagsync_jceks_path}"),
+             mode=0640
+             )
+
+        setup_ranger_xml.update_dot_jceks_crc_ownership(credential_provider_path=params.tagsync_jceks_path,
+                                                        user=params.unix_user, group=params.unix_group)
+
+        if params.stack_supports_ranger_tagsync_ssl_xml_support:
+            Logger.info("Stack support Atlas user for Tagsync, creating keystore for same.")
+            self.create_atlas_user_keystore(env)
+        else:
+            Logger.info("Stack does not support Atlas user for Tagsync, skipping keystore creation for same.")
+
+            File(ranger_tagsync_setup_marker,
+                 owner=params.unix_user,
+                 group=params.unix_group,
+                 mode=0o640
+                 )
+
+    def configure(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        # set up ranger_admin version for current
+        Execute('python /usr/lib/bigtop-select/distro-select set ranger-tagsync 3.2.0')
+
+        self.initialize(env)
+        setup_ranger_xml.ranger('ranger_tagsync', upgrade_type=upgrade_type)
+
+    def start(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        self.configure(env, upgrade_type=upgrade_type)
+        ranger_service('ranger_tagsync')
+
+    def stop(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        Execute(format('{tagsync_services_file} stop'), environment={'JAVA_HOME': params.java_home},
+                user=params.unix_user)
+        File(params.tagsync_pid_file,
+             action="delete"
+             )
+
+    def status(self, env):
+        import status_params
+        env.set_params(status_params)
+
+        check_process_status(status_params.tagsync_pid_file)
+
+    def pre_upgrade_restart(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        if params.stack_supports_ranger_tagsync:
+            Logger.info("Executing Ranger Tagsync Stack Upgrade pre-restart")
+            stack_select.select_packages(params.version)
+
+    def post_upgrade_restart(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        if upgrade_type and params.upgrade_direction == Direction.UPGRADE and not params.stack_supports_multiple_env_sh_files:
+            files_name_list = ['ranger-tagsync-env-piddir.sh', 'ranger-tagsync-env-logdir.sh']
+            for file_name in files_name_list:
+                File(format("{ranger_tagsync_conf}/{file_name}"),
+                     action="delete"
+                     )
+
+    def get_log_folder(self):
+        import params
+        return params.tagsync_log_dir
+
+    def get_user(self):
+        import params
+        return params.unix_user
+
+    def get_pid_files(self):
+        import status_params
+        return [status_params.tagsync_pid_file]
+
+    def create_atlas_user_keystore(self, env):
+        import params
+        env.set_params(params)
+
+        setup_ranger_xml.ranger_credential_helper(params.tagsync_cred_lib, 'atlas.user.password',
+                                                  params.atlas_admin_password, params.atlas_tagsync_jceks_path)
+        File(params.atlas_tagsync_jceks_path,
+             owner=params.unix_user,
+             group=params.unix_group,
+             only_if=format("test -e {atlas_tagsync_jceks_path}"),
+             mode=0640
+             )
+
+        setup_ranger_xml.update_dot_jceks_crc_ownership(credential_provider_path=params.atlas_tagsync_jceks_path,
+                                                        user=params.unix_user, group=params.unix_group)
+
+
+if __name__ == "__main__":
+    RangerTagsync().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/status_params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/status_params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/status_params.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/status_params.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HBASE/package/scripts/status_params.py	(date 1719626239000)
@@ -48,6 +48,7 @@
 
   hbase_master_pid_file = format("{pid_dir}/hbase-{hbase_user}-master.pid")
   regionserver_pid_file = format("{pid_dir}/hbase-{hbase_user}-regionserver.pid")
+  phoenix_pid_file = format("{pid_dir}/phoenix-{hbase_user}-queryserver.pid")
 
   # Security related/required params
   hostname = config['agentLevelParams']['hostname']
@@ -61,5 +62,7 @@
 
   hbase_conf_dir = "/etc/hbase/conf"
   limits_conf_dir = "/etc/security/limits.d"
+  if stack_version_formatted and check_stack_feature(StackFeature.ROLLING_UPGRADE, stack_version_formatted):
+    hbase_conf_dir = format("{stack_root}/current/{component_directory}/conf")
     
 stack_name = default("/clusterLevelParams/stack_name", None)
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_usersync.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_usersync.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_usersync.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/ranger_usersync.py	(date 1719626239000)
@@ -0,0 +1,121 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+from resource_management.libraries.functions.check_process_status import check_process_status
+from resource_management.libraries.script import Script
+from resource_management.core.resources.system import Execute, File
+from resource_management.core.exceptions import ComponentIsNotRunning
+from resource_management.libraries.functions.format import format
+from resource_management.core.logger import Logger
+from resource_management.core import shell
+from ranger_service import ranger_service
+from ambari_commons.constants import UPGRADE_TYPE_NON_ROLLING, UPGRADE_TYPE_ROLLING
+from resource_management.libraries.functions.constants import Direction
+import upgrade
+import setup_ranger_xml
+import os
+
+
+class RangerUsersync(Script):
+
+    def install(self, env):
+        self.install_packages(env)
+
+
+    def configure(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        setup_ranger_xml.validate_user_password('rangerusersync_user_password')
+        # change to none install
+        if params.stack_supports_usersync_passwd:
+            setup_ranger_xml.ranger_credential_helper(params.ugsync_cred_lib, params.ugsync_policymgr_alias,
+                                                      params.rangerusersync_user_password,
+                                                      params.ugsync_policymgr_keystore)
+
+            File(params.ugsync_policymgr_keystore,
+                 owner=params.unix_user,
+                 group=params.unix_group,
+                 mode=0640
+                 )
+        setup_ranger_xml.ranger('ranger_usersync', upgrade_type=upgrade_type)
+
+    def start(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        self.configure(env, upgrade_type=upgrade_type)
+        ranger_service('ranger_usersync')
+
+    def stop(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        Execute(format('{params.usersync_stop}'), environment={'JAVA_HOME': params.java_home}, user=params.unix_user)
+        if params.stack_supports_pid:
+            File(params.ranger_usersync_pid_file,
+                 action="delete"
+                 )
+
+    def status(self, env):
+        import status_params
+        env.set_params(status_params)
+
+        if status_params.stack_supports_pid:
+            check_process_status(status_params.ranger_usersync_pid_file)
+            return
+
+        cmd = 'ps -ef | grep proc_rangerusersync | grep -v grep'
+        code, output = shell.call(cmd, timeout=20)
+
+        if code != 0:
+            Logger.debug('Ranger usersync process not running')
+            raise ComponentIsNotRunning()
+        pass
+
+    def pre_upgrade_restart(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        upgrade.prestart(env)
+
+    def post_upgrade_restart(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+
+        if upgrade_type and params.upgrade_direction == Direction.UPGRADE and not params.stack_supports_multiple_env_sh_files:
+            files_name_list = ['ranger-usersync-env-piddir.sh', 'ranger-usersync-env-logdir.sh']
+            for file_name in files_name_list:
+                File(format("{ranger_ugsync_conf}/{file_name}"),
+                     action="delete"
+                     )
+
+    def get_log_folder(self):
+        import params
+        return params.usersync_log_dir
+
+    def get_user(self):
+        import params
+        return params.unix_user
+
+    def get_pid_files(self):
+        import status_params
+        return [status_params.ranger_usersync_pid_file]
+
+
+if __name__ == "__main__":
+    RangerUsersync().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/setup_ranger_xml.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/setup_ranger_xml.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/setup_ranger_xml.py
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/scripts/setup_ranger_xml.py	(date 1719626239000)
@@ -0,0 +1,969 @@
+#!/usr/bin/env python
+"""
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+"""
+import os
+import re
+from resource_management.libraries.script import Script
+from resource_management.libraries.functions.default import default
+from resource_management.core.logger import Logger
+from resource_management.core.resources.system import File, Directory, Execute, Link
+from resource_management.core.source import DownloadSource, InlineTemplate, Template
+from resource_management.libraries.resources.xml_config import XmlConfig
+from resource_management.libraries.resources.modify_properties_file import ModifyPropertiesFile
+from resource_management.libraries.resources.properties_file import PropertiesFile
+from resource_management.core.exceptions import Fail
+from resource_management.libraries.functions.decorator import retry
+from resource_management.libraries.functions.generate_logfeeder_input_config import generate_logfeeder_input_config
+from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.is_empty import is_empty
+from resource_management.core.utils import PasswordString
+from resource_management.core.shell import as_sudo
+from resource_management.libraries.functions import solr_cloud_util
+from ambari_commons.constants import UPGRADE_TYPE_NON_ROLLING, UPGRADE_TYPE_ROLLING
+from resource_management.core.exceptions import ExecutionFailed
+
+
+# This file contains functions used for setup/configure of Ranger Admin and Ranger Usersync.
+# The design is to mimic what is done by the setup.sh script bundled by Ranger component currently.
+
+def ranger(name=None, upgrade_type=None):
+    """
+    parameter name: name of ranger service component
+    """
+    if name == 'ranger_admin':
+        setup_ranger_admin(upgrade_type=upgrade_type)
+
+    if name == 'ranger_usersync':
+        setup_usersync(upgrade_type=upgrade_type)
+
+    if name == 'ranger_tagsync':
+        setup_tagsync(upgrade_type=upgrade_type)
+
+
+def setup_ranger_admin(upgrade_type=None):
+    import params
+
+    if upgrade_type is None:
+        upgrade_type = Script.get_upgrade_type(default("/commandParams/upgrade_type", ""))
+
+    ranger_home = params.ranger_home
+    ranger_conf = params.ranger_conf
+
+    Directory(ranger_conf,
+              owner=params.unix_user,
+              group=params.unix_group,
+              create_parents=True
+              )
+
+    copy_jdbc_connector(ranger_home)
+
+    File(format("/usr/lib/ambari-agent/{check_db_connection_jar_name}"),
+         content=DownloadSource(format("{jdk_location}/{check_db_connection_jar_name}")),
+         mode=0644,
+         )
+
+    generate_logfeeder_input_config('ranger', Template("input.config-ranger.json.j2", extra_imports=[default]))
+
+    cp = format("{check_db_connection_jar}")
+    if params.db_flavor.lower() == 'sqla':
+        cp = cp + os.pathsep + format("{ranger_home}/ews/lib/sajdbc4.jar")
+    else:
+        cp = cp + os.pathsep + format("{driver_curl_target}")
+    cp = cp + os.pathsep + format("{ranger_home}/ews/lib/*")
+
+    db_connection_check_command = format(
+        "{java_home}/bin/java -cp {cp} org.apache.ambari.server.DBConnectionVerification '{ranger_jdbc_connection_url}' {ranger_db_user} {ranger_db_password!p} {ranger_jdbc_driver}")
+
+    env_dict = {}
+    if params.db_flavor.lower() == 'sqla':
+        env_dict = {'LD_LIBRARY_PATH': params.ld_lib_path}
+
+    Execute(db_connection_check_command, path='/usr/sbin:/sbin:/usr/local/bin:/bin:/usr/bin', tries=5, try_sleep=10,
+            environment=env_dict)
+
+    Execute(('ln', '-sf', format('{ranger_home}/ews/webapp/WEB-INF/classes/conf'), format('{ranger_home}/conf')),
+            not_if=format("ls {ranger_home}/conf"),
+            only_if=format("ls {ranger_home}/ews/webapp/WEB-INF/classes/conf"),
+            sudo=True)
+
+    if upgrade_type is not None:
+        src_file = format('{ranger_home}/ews/webapp/WEB-INF/classes/conf.dist/ranger-admin-default-site.xml')
+        dst_file = format('{ranger_home}/conf/ranger-admin-default-site.xml')
+        Execute(('cp', '-f', src_file, dst_file), sudo=True)
+
+        src_file = format('{ranger_home}/ews/webapp/WEB-INF/classes/conf.dist/security-applicationContext.xml')
+        dst_file = format('{ranger_home}/conf/security-applicationContext.xml')
+
+        Execute(('cp', '-f', src_file, dst_file), sudo=True)
+
+    Directory(format('{ranger_home}/'),
+              owner=params.unix_user,
+              group=params.unix_group,
+              recursive_ownership=True,
+              )
+
+    Directory(params.ranger_pid_dir,
+              mode=0755,
+              owner=params.unix_user,
+              group=params.user_group,
+              cd_access="a",
+              create_parents=True
+              )
+
+    Directory(params.admin_log_dir,
+              owner=params.unix_user,
+              group=params.unix_group,
+              create_parents=True,
+              cd_access='a',
+              mode=0755
+              )
+
+    if os.path.isfile(params.ranger_admin_default_file):
+        File(params.ranger_admin_default_file, owner=params.unix_user, group=params.unix_group)
+    else:
+        Logger.warning(
+            'Required file {0} does not exist, copying the file to {1} path'.format(params.ranger_admin_default_file,
+                                                                                    ranger_conf))
+        src_file = format('{ranger_home}/ews/webapp/WEB-INF/classes/conf.dist/ranger-admin-default-site.xml')
+        dst_file = format('{ranger_home}/conf/ranger-admin-default-site.xml')
+        Execute(('cp', '-f', src_file, dst_file), sudo=True)
+        File(params.ranger_admin_default_file, owner=params.unix_user, group=params.unix_group)
+
+    if os.path.isfile(params.security_app_context_file):
+        File(params.security_app_context_file, owner=params.unix_user, group=params.unix_group)
+    else:
+        Logger.warning(
+            'Required file {0} does not exist, copying the file to {1} path'.format(params.security_app_context_file,
+                                                                                    ranger_conf))
+        src_file = format('{ranger_home}/ews/webapp/WEB-INF/classes/conf.dist/security-applicationContext.xml')
+        dst_file = format('{ranger_home}/conf/security-applicationContext.xml')
+        Execute(('cp', '-f', src_file, dst_file), sudo=True)
+        File(params.security_app_context_file, owner=params.unix_user, group=params.unix_group)
+
+    if default("/configurations/ranger-admin-site/ranger.authentication.method", "") == 'PAM':
+        d = '/etc/pam.d'
+        if os.path.isdir(d):
+            if os.path.isfile(os.path.join(d, 'ranger-admin')):
+                Logger.info('ranger-admin PAM file already exists.')
+            else:
+                File(format('{d}/ranger-admin'),
+                     content=Template('ranger_admin_pam.j2'),
+                     owner=params.unix_user,
+                     group=params.unix_group,
+                     mode=0644
+                     )
+            if os.path.isfile(os.path.join(d, 'ranger-remote')):
+                Logger.info('ranger-remote PAM file already exists.')
+            else:
+                File(format('{d}/ranger-remote'),
+                     content=Template('ranger_remote_pam.j2'),
+                     owner=params.unix_user,
+                     group=params.unix_group,
+                     mode=0644
+                     )
+        else:
+            Logger.error("Unable to use PAM authentication, /etc/pam.d/ directory does not exist.")
+
+    Execute(('ln', '-sf', format('{ranger_home}/ews/ranger-admin-services.sh'), '/usr/bin/ranger-admin'),
+            not_if=format("ls /usr/bin/ranger-admin"),
+            only_if=format("ls {ranger_home}/ews/ranger-admin-services.sh"),
+            sudo=True)
+
+    # remove plain-text password from xml configs
+
+    ranger_admin_site_copy = {}
+    ranger_admin_site_copy.update(params.config['configurations']['ranger-admin-site'])
+    for prop in params.ranger_admin_password_properties:
+        if prop in ranger_admin_site_copy:
+            ranger_admin_site_copy[prop] = "_"
+    if 'ranger.ha.spnego.kerberos.keytab' in ranger_admin_site_copy:
+        ranger_admin_site_copy['ranger.spnego.kerberos.keytab'] = ranger_admin_site_copy[
+            'ranger.ha.spnego.kerberos.keytab']
+
+    XmlConfig("ranger-admin-site.xml",
+              conf_dir=ranger_conf,
+              configurations=ranger_admin_site_copy,
+              configuration_attributes=params.config['configurationAttributes']['ranger-admin-site'],
+              owner=params.unix_user,
+              group=params.unix_group,
+              mode=0644)
+
+    Directory(os.path.join(ranger_conf, 'ranger_jaas'),
+              mode=0700,
+              owner=params.unix_user,
+              group=params.unix_group,
+              )
+
+    if params.stack_supports_ranger_log4j:
+        File(format('{ranger_home}/ews/webapp/WEB-INF/log4j.properties'),
+             owner=params.unix_user,
+             group=params.unix_group,
+             content=InlineTemplate(params.admin_log4j),
+             mode=0644
+             )
+    # ranger2.3.0
+    if params.stack_supports_ranger_logback:
+        File(format('{ranger_home}/ews/webapp/WEB-INF/classes/conf/logback.xml'),
+             owner=params.unix_user,
+             group=params.unix_group,
+             content=InlineTemplate(params.admin_logback),
+             mode=0644
+             )
+
+    do_keystore_setup(upgrade_type=upgrade_type)
+
+    create_core_site_xml(ranger_conf)
+
+    if params.stack_supports_ranger_kerberos:
+        if params.is_hbase_ha_enabled and params.ranger_hbase_plugin_enabled:
+            XmlConfig("hbase-site.xml",
+                      conf_dir=ranger_conf,
+                      configurations=params.config['configurations']['hbase-site'],
+                      configuration_attributes=params.config['configurationAttributes']['hbase-site'],
+                      owner=params.unix_user,
+                      group=params.unix_group,
+                      mode=0644
+                      )
+
+        if params.is_namenode_ha_enabled and params.ranger_hdfs_plugin_enabled:
+            XmlConfig("hdfs-site.xml",
+                      conf_dir=ranger_conf,
+                      configurations=params.config['configurations']['hdfs-site'],
+                      configuration_attributes=params.config['configurationAttributes']['hdfs-site'],
+                      owner=params.unix_user,
+                      group=params.unix_group,
+                      mode=0644
+                      )
+
+    File(format("{ranger_conf}/ranger-admin-env.sh"),
+         content=InlineTemplate(params.ranger_env_content),
+         owner=params.unix_user,
+         group=params.unix_group,
+         mode=0755
+         )
+
+
+def setup_ranger_db(stack_version=None):
+    import params
+
+    ranger_home = params.ranger_home
+
+    if stack_version is not None:
+        ranger_home = format("{stack_root}/{stack_version}/ranger-admin")
+
+    copy_jdbc_connector(ranger_home)
+
+    ModifyPropertiesFile(format("{ranger_home}/install.properties"),
+                         properties={'audit_store': params.ranger_audit_source_type},
+                         owner=params.unix_user,
+                         )
+
+    ModifyPropertiesFile(format("{ranger_home}/install.properties"),
+                         properties={'ranger_admin_max_heap_size': params.ranger_admin_max_heap_size},
+                         owner=params.unix_user,
+                         )
+
+    env_dict = {'RANGER_ADMIN_HOME': ranger_home, 'JAVA_HOME': params.java_home}
+    if params.db_flavor.lower() == 'sqla':
+        env_dict = {'RANGER_ADMIN_HOME': ranger_home, 'JAVA_HOME': params.java_home,
+                    'LD_LIBRARY_PATH': params.ld_lib_path}
+
+    # User wants us to setup the DB user and DB?
+    if params.create_db_dbuser:
+        Logger.info('Setting up Ranger DB and DB User')
+        dba_setup = format('ambari-python-wrap {ranger_home}/dba_script.py -q')
+        Execute(dba_setup,
+                environment=env_dict,
+                logoutput=True,
+                user=params.unix_user,
+                )
+    else:
+        Logger.info('Separate DBA property not set. Assuming Ranger DB and DB User exists!')
+
+    db_setup = format('ambari-python-wrap {ranger_home}/db_setup.py')
+    Execute(db_setup,
+            environment=env_dict,
+            logoutput=True,
+            user=params.unix_user,
+            )
+
+
+def setup_java_patch(stack_version=None):
+    import params
+
+    ranger_home = params.ranger_home
+    if stack_version is not None:
+        ranger_home = format("{stack_root}/{stack_version}/ranger-admin")
+
+    env_dict = {'RANGER_ADMIN_HOME': ranger_home, 'JAVA_HOME': params.java_home}
+    if params.db_flavor.lower() == 'sqla':
+        env_dict = {'RANGER_ADMIN_HOME': ranger_home, 'JAVA_HOME': params.java_home,
+                    'LD_LIBRARY_PATH': params.ld_lib_path}
+
+    setup_java_patch = format('ambari-python-wrap {ranger_home}/db_setup.py -javapatch')
+    Execute(setup_java_patch,
+            environment=env_dict,
+            logoutput=True,
+            user=params.unix_user,
+            )
+
+
+def do_keystore_setup(upgrade_type=None):
+    import params
+
+    ranger_home = params.ranger_home
+    cred_lib_path = params.cred_lib_path
+
+    ranger_credential_helper(cred_lib_path, params.ranger_jpa_jdbc_credential_alias, params.ranger_ambari_db_password,
+                             params.ranger_credential_provider_path)
+
+    if params.ranger_auth_method.upper() == "LDAP":
+        ranger_credential_helper(params.cred_lib_path, params.ranger_ldap_password_alias,
+                                 params.ranger_ldap_bind_auth_password, params.ranger_credential_provider_path)
+
+    if params.ranger_auth_method.upper() == "ACTIVE_DIRECTORY":
+        ranger_credential_helper(params.cred_lib_path, params.ranger_ad_password_alias,
+                                 params.ranger_ad_bind_auth_password, params.ranger_credential_provider_path)
+
+    if params.stack_supports_secure_ssl_password:
+        ranger_credential_helper(params.cred_lib_path, params.ranger_truststore_alias, params.truststore_password,
+                                 params.ranger_credential_provider_path)
+
+        if params.https_enabled and not params.http_enabled:
+            ranger_credential_helper(params.cred_lib_path, params.ranger_https_keystore_alias,
+                                     params.https_keystore_password, params.ranger_credential_provider_path)
+
+    File(params.ranger_credential_provider_path,
+         owner=params.unix_user,
+         group=params.unix_group,
+         only_if=format("test -e {ranger_credential_provider_path}"),
+         mode=0640
+         )
+
+    update_dot_jceks_crc_ownership(credential_provider_path=params.ranger_credential_provider_path,
+                                   user=params.unix_user, group=params.unix_group)
+
+
+def password_validation(password):
+    import params
+    if password.strip() == "":
+        raise Fail("Blank password is not allowed for Bind user. Please enter valid password.")
+    if re.search("[\\\`'\"]", password):
+        raise Fail("LDAP/AD bind password contains one of the unsupported special characters like \" ' \ `")
+    else:
+        Logger.info("password validated")
+
+
+def copy_jdbc_connector(ranger_home):
+    import params
+
+    if params.jdbc_jar_name is None and params.driver_curl_source.endswith("/None"):
+        error_message = format(
+            "{db_flavor} jdbc driver cannot be downloaded from {jdk_location}\nPlease run 'ambari-server setup --jdbc-db={db_flavor} --jdbc-driver={{path_to_jdbc}}' on ambari-server host.")
+        raise Fail(error_message)
+
+    if params.driver_curl_source and not params.driver_curl_source.endswith("/None"):
+        if params.previous_jdbc_jar and os.path.isfile(params.previous_jdbc_jar):
+            File(params.previous_jdbc_jar, action='delete')
+
+    File(params.downloaded_custom_connector,
+         content=DownloadSource(params.driver_curl_source),
+         mode=0644
+         )
+
+    driver_curl_target = format("{ranger_home}/ews/lib/{jdbc_jar_name}")
+
+    if params.db_flavor.lower() == 'sqla':
+        Execute(('tar', '-xvf', params.downloaded_custom_connector, '-C', params.tmp_dir), sudo=True)
+
+        Execute(('cp', '--remove-destination', params.jar_path_in_archive, os.path.join(ranger_home, 'ews', 'lib')),
+                path=["/bin", "/usr/bin/"],
+                sudo=True)
+
+        File(os.path.join(ranger_home, 'ews', 'lib', 'sajdbc4.jar'), mode=0644)
+
+        Directory(params.jdbc_libs_dir,
+                  cd_access="a",
+                  create_parents=True)
+
+        Execute(as_sudo(['yes', '|', 'cp', params.libs_path_in_archive, params.jdbc_libs_dir], auto_escape=False),
+                path=["/bin", "/usr/bin/"])
+    else:
+        Execute(
+            ('cp', '--remove-destination', params.downloaded_custom_connector, os.path.join(ranger_home, 'ews', 'lib')),
+            path=["/bin", "/usr/bin/"],
+            sudo=True)
+
+        File(os.path.join(ranger_home, 'ews', 'lib', params.jdbc_jar_name), mode=0644)
+
+    ModifyPropertiesFile(format("{ranger_home}/install.properties"),
+                         properties=params.config['configurations']['admin-properties'],
+                         owner=params.unix_user,
+                         )
+
+    if params.db_flavor.lower() == 'sqla':
+        ModifyPropertiesFile(format("{ranger_home}/install.properties"),
+                             properties={'SQL_CONNECTOR_JAR': format('{ranger_home}/ews/lib/sajdbc4.jar')},
+                             owner=params.unix_user,
+                             )
+    else:
+        ModifyPropertiesFile(format("{ranger_home}/install.properties"),
+                             properties={'SQL_CONNECTOR_JAR': format('{driver_curl_target}')},
+                             owner=params.unix_user,
+                             )
+
+
+def setup_usersync(upgrade_type=None):
+    import params
+
+    usersync_home = params.usersync_home
+    ranger_home = params.ranger_home
+    ranger_ugsync_conf = params.ranger_ugsync_conf
+
+    if not is_empty(
+            params.ranger_usersync_ldap_ldapbindpassword) and params.ug_sync_source == 'org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder':
+        password_validation(params.ranger_usersync_ldap_ldapbindpassword)
+
+    Directory(params.ranger_pid_dir,
+              mode=0755,
+              owner=params.unix_user,
+              group=params.user_group,
+              cd_access="a",
+              create_parents=True
+              )
+
+    Directory(params.usersync_log_dir,
+              owner=params.unix_user,
+              group=params.unix_group,
+              cd_access='a',
+              create_parents=True,
+              mode=0755,
+              recursive_ownership=True
+              )
+
+    Directory(format("{ranger_ugsync_conf}/"),
+              owner=params.unix_user
+              )
+
+    generate_logfeeder_input_config('ranger', Template("input.config-ranger.json.j2", extra_imports=[default]))
+
+    if upgrade_type is not None:
+        src_file = format('{usersync_home}/conf.dist/ranger-ugsync-default.xml')
+        dst_file = format('{usersync_home}/conf/ranger-ugsync-default.xml')
+        Execute(('cp', '-f', src_file, dst_file), sudo=True)
+
+    if params.stack_supports_ranger_log4j:
+        File(format('{usersync_home}/conf/log4j.properties'),
+             owner=params.unix_user,
+             group=params.unix_group,
+             content=InlineTemplate(params.usersync_log4j),
+             mode=0644
+             )
+    elif upgrade_type is not None and not params.stack_supports_ranger_log4j:
+        src_file = format('{usersync_home}/conf.dist/log4j.xml')
+        dst_file = format('{usersync_home}/conf/log4j.xml')
+        Execute(('cp', '-f', src_file, dst_file), sudo=True)
+
+    # ranger2.3.0
+    if params.stack_supports_ranger_logback:
+        File(format('{usersync_home}/conf/logback.xml'),
+             owner=params.unix_user,
+             group=params.unix_group,
+             content=InlineTemplate(params.usersync_logback),
+             mode=0644
+             )
+
+    # remove plain-text password from xml configs
+    ranger_ugsync_site_copy = {}
+    ranger_ugsync_site_copy.update(params.config['configurations']['ranger-ugsync-site'])
+    for prop in params.ranger_usersync_password_properties:
+        if prop in ranger_ugsync_site_copy:
+            ranger_ugsync_site_copy[prop] = "_"
+
+    XmlConfig("ranger-ugsync-site.xml",
+              conf_dir=ranger_ugsync_conf,
+              configurations=ranger_ugsync_site_copy,
+              configuration_attributes=params.config['configurationAttributes']['ranger-ugsync-site'],
+              owner=params.unix_user,
+              group=params.unix_group,
+              mode=0644)
+
+    if os.path.isfile(params.ranger_ugsync_default_file):
+        File(params.ranger_ugsync_default_file, owner=params.unix_user, group=params.unix_group)
+
+    if os.path.isfile(params.usgsync_log4j_file):
+        File(params.usgsync_log4j_file, owner=params.unix_user, group=params.unix_group)
+
+    if os.path.isfile(params.cred_validator_file):
+        File(params.cred_validator_file, group=params.unix_group, mode=0750)
+
+    if os.path.isfile(params.pam_cred_validator_file):
+        File(params.pam_cred_validator_file, group=params.unix_group, mode=0750)
+
+    ranger_credential_helper(params.ugsync_cred_lib, 'usersync.ssl.key.password',
+                             params.ranger_usersync_keystore_password, params.ugsync_jceks_path)
+
+    if not is_empty(
+            params.ranger_usersync_ldap_ldapbindpassword) and params.ug_sync_source == 'org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder':
+        ranger_credential_helper(params.ugsync_cred_lib, 'ranger.usersync.ldap.bindalias',
+                                 params.ranger_usersync_ldap_ldapbindpassword, params.ugsync_jceks_path)
+
+    ranger_credential_helper(params.ugsync_cred_lib, 'usersync.ssl.truststore.password',
+                             params.ranger_usersync_truststore_password, params.ugsync_jceks_path)
+
+    File(params.ugsync_jceks_path,
+         owner=params.unix_user,
+         group=params.unix_group,
+         only_if=format("test -e {ugsync_jceks_path}"),
+         mode=0640
+         )
+
+    update_dot_jceks_crc_ownership(credential_provider_path=params.ugsync_jceks_path, user=params.unix_user,
+                                   group=params.unix_group)
+
+    File(params.usersync_services_file,
+         mode=0755,
+         )
+
+    if not os.path.isfile(params.ranger_usersync_keystore_file):
+        cmd = format(
+            "{java_home}/bin/keytool -genkeypair -keyalg RSA -alias selfsigned -keystore '{ranger_usersync_keystore_file}' -keypass {ranger_usersync_keystore_password!p} -storepass {ranger_usersync_keystore_password!p} -validity 3600 -keysize 2048 -dname '{default_dn_name}'")
+
+        Execute(cmd, logoutput=True, user=params.unix_user)
+
+        File(params.ranger_usersync_keystore_file,
+             owner=params.unix_user,
+             group=params.user_group,
+             only_if=format("test -e {ranger_usersync_keystore_file}"),
+             mode=0640
+             )
+
+    create_core_site_xml(ranger_ugsync_conf)
+
+    File(format("{ranger_ugsync_conf}/ranger-usersync-env.sh"),
+         content=InlineTemplate(params.ranger_env_content),
+         owner=params.unix_user,
+         group=params.unix_group,
+         mode=0755
+         )
+
+
+def setup_tagsync(upgrade_type=None):
+    import params
+
+    ranger_tagsync_home = params.ranger_tagsync_home
+    ranger_home = params.ranger_home
+    ranger_tagsync_conf = params.ranger_tagsync_conf
+
+    Directory(format("{ranger_tagsync_conf}"),
+              owner=params.unix_user,
+              group=params.unix_group,
+              create_parents=True
+              )
+
+    Directory(params.ranger_pid_dir,
+              mode=0755,
+              create_parents=True,
+              owner=params.unix_user,
+              group=params.user_group,
+              cd_access="a",
+              )
+
+    Directory(params.tagsync_log_dir,
+              create_parents=True,
+              owner=params.unix_user,
+              group=params.unix_group,
+              cd_access="a",
+              mode=0755
+              )
+
+    XmlConfig("ranger-tagsync-site.xml",
+              conf_dir=ranger_tagsync_conf,
+              configurations=params.config['configurations']['ranger-tagsync-site'],
+              configuration_attributes=params.config['configurationAttributes']['ranger-tagsync-site'],
+              owner=params.unix_user,
+              group=params.unix_group,
+              mode=0644)
+
+    if params.stack_supports_ranger_tagsync_ssl_xml_support:
+        Logger.info("Stack supports tagsync-ssl configurations, performing the same.")
+        setup_tagsync_ssl_configs()
+    else:
+        Logger.info("Stack doesnt support tagsync-ssl configurations, skipping the same.")
+
+    PropertiesFile(format('{ranger_tagsync_conf}/atlas-application.properties'),
+                   properties=params.tagsync_application_properties,
+                   mode=0755,
+                   owner=params.unix_user,
+                   group=params.unix_group
+                   )
+
+    File(format('{ranger_tagsync_conf}/log4j.properties'),
+         owner=params.unix_user,
+         group=params.unix_group,
+         content=InlineTemplate(params.tagsync_log4j),
+         mode=0644
+         )
+
+    File(format('{ranger_tagsync_conf}/logback.xml'),
+         owner=params.unix_user,
+         group=params.unix_group,
+         content=InlineTemplate(params.tagsync_logback),
+         mode=0644
+         )
+
+    File(params.tagsync_services_file,
+         mode=0755,
+         )
+
+    create_core_site_xml(ranger_tagsync_conf)
+
+    File(format("{ranger_tagsync_conf}/ranger-tagsync-env.sh"),
+         content=InlineTemplate(params.ranger_env_content),
+         owner=params.unix_user,
+         group=params.unix_group,
+         mode=0755
+         )
+
+
+def ranger_credential_helper(lib_path, alias_key, alias_value, file_path):
+    import params
+
+    java_bin = format('{java_home}/bin/java')
+    file_path = format('jceks://file{file_path}')
+    cmd = (java_bin, '-cp', lib_path, 'org.apache.ranger.credentialapi.buildks', 'create', alias_key, '-value',
+           PasswordString(alias_value), '-provider', file_path)
+    Execute(cmd, environment={'JAVA_HOME': params.java_home, 'HADOOP_HOME': params.hadoop_home}, logoutput=True,
+            sudo=True)
+
+
+def create_core_site_xml(conf_dir):
+    import params
+
+    if params.stack_supports_ranger_kerberos:
+        if params.has_namenode:
+            # if there is the viewFS mount table content, create separate xml config and include in in the core-site
+            # else just create core-site
+            if params.mount_table_content:
+                XmlConfig("core-site.xml",
+                          conf_dir=conf_dir,
+                          configurations=params.config['configurations']['core-site'],
+                          configuration_attributes=params.config['configurationAttributes']['core-site'],
+                          owner=params.unix_user,
+                          group=params.unix_group,
+                          mode=0644,
+                          xml_include_file=os.path.join(conf_dir, params.xml_inclusion_file_name)
+                          )
+
+                File(os.path.join(conf_dir, params.xml_inclusion_file_name),
+                     owner=params.unix_user,
+                     group=params.unix_group,
+                     content=params.mount_table_content,
+                     mode=0644
+                     )
+            else:
+                XmlConfig("core-site.xml",
+                          conf_dir=conf_dir,
+                          configurations=params.config['configurations']['core-site'],
+                          configuration_attributes=params.config['configurationAttributes']['core-site'],
+                          owner=params.unix_user,
+                          group=params.unix_group,
+                          mode=0644
+                          )
+        else:
+            Logger.warning('HDFS service not installed. Creating core-site.xml file.')
+            XmlConfig("core-site.xml",
+                      conf_dir=conf_dir,
+                      configurations=params.core_site_property,
+                      configuration_attributes={},
+                      owner=params.unix_user,
+                      group=params.unix_group,
+                      mode=0644
+                      )
+
+
+def setup_ranger_audit_solr():
+    import params
+
+    if params.security_enabled and params.stack_supports_ranger_kerberos:
+
+        if params.solr_jaas_file is not None:
+            File(format("{solr_jaas_file}"),
+                 content=Template("ranger_solr_jaas_conf.j2"),
+                 owner=params.unix_user
+                 )
+    try:
+        check_znode()
+
+        if params.stack_supports_ranger_solr_configs:
+            Logger.info('Solr configrations supported,creating solr-configurations.')
+            File(format("{ranger_solr_conf}/solrconfig.xml"),
+                 content=InlineTemplate(params.ranger_solr_config_content),
+                 owner=params.unix_user,
+                 group=params.unix_group,
+                 mode=0644
+                 )
+
+            solr_cloud_util.upload_configuration_to_zk(
+                zookeeper_quorum=params.zookeeper_quorum,
+                solr_znode=params.solr_znode,
+                config_set=params.ranger_solr_config_set,
+                config_set_dir=params.ranger_solr_conf,
+                tmp_dir=params.tmp_dir,
+                java64_home=params.java_home,
+                solrconfig_content=InlineTemplate(params.ranger_solr_config_content),
+                jaas_file=params.solr_jaas_file,
+                retry=30, interval=5
+            )
+
+        else:
+            Logger.info('Solr configrations not supported, skipping solr-configurations.')
+            solr_cloud_util.upload_configuration_to_zk(
+                zookeeper_quorum=params.zookeeper_quorum,
+                solr_znode=params.solr_znode,
+                config_set=params.ranger_solr_config_set,
+                config_set_dir=params.ranger_solr_conf,
+                tmp_dir=params.tmp_dir,
+                java64_home=params.java_home,
+                jaas_file=params.solr_jaas_file,
+                retry=30, interval=5)
+
+        if params.security_enabled and params.has_infra_solr \
+                and not params.is_external_solrCloud_enabled and params.stack_supports_ranger_kerberos:
+            solr_cloud_util.add_solr_roles(params.config,
+                                           roles=[params.infra_solr_role_ranger_admin,
+                                                  params.infra_solr_role_ranger_audit, params.infra_solr_role_dev],
+                                           new_service_principals=[params.ranger_admin_jaas_principal])
+            service_default_principals_map = [('hdfs', 'nn'), ('hbase', 'hbase'), ('hive', 'hive'), ('kafka', 'kafka'),
+                                              ('kms', 'rangerkms'),
+                                              ('knox', 'knox'), ('nifi', 'nifi'), ('storm', 'storm'), ('yanr', 'yarn')]
+            service_principals = get_ranger_plugin_principals(service_default_principals_map)
+            solr_cloud_util.add_solr_roles(params.config,
+                                           roles=[params.infra_solr_role_ranger_audit, params.infra_solr_role_dev],
+                                           new_service_principals=service_principals)
+
+        solr_cloud_util.create_collection(
+            zookeeper_quorum=params.zookeeper_quorum,
+            solr_znode=params.solr_znode,
+            collection=params.ranger_solr_collection_name,
+            config_set=params.ranger_solr_config_set,
+            java64_home=params.java_home,
+            shards=params.ranger_solr_shards,
+            replication_factor=int(params.replication_factor),
+            jaas_file=params.solr_jaas_file)
+
+        if params.security_enabled and params.has_infra_solr \
+                and not params.is_external_solrCloud_enabled and params.stack_supports_ranger_kerberos:
+            secure_znode(format('{solr_znode}/configs/{ranger_solr_config_set}'), params.solr_jaas_file)
+            secure_znode(format('{solr_znode}/collections/{ranger_solr_collection_name}'), params.solr_jaas_file)
+    except ExecutionFailed as execution_exception:
+        Logger.error(
+            'Error when configuring Solr for Ranger, Kindly check Solr/Zookeeper services to be up and running:\n {0}'.format(
+                execution_exception))
+
+
+def setup_ranger_admin_passwd_change(username, user_password, user_default_password):
+    import params
+
+    env_dict = {'RANGER_ADMIN_HOME': params.ranger_home, 'JAVA_HOME': params.java_home}
+    if params.db_flavor.lower() == 'sqla':
+        env_dict = {'RANGER_ADMIN_HOME': params.ranger_home, 'JAVA_HOME': params.java_home,
+                    'LD_LIBRARY_PATH': params.ld_lib_path}
+
+    cmd = format(
+        "ambari-python-wrap {ranger_home}/db_setup.py -changepassword {username} {user_default_password!p} {user_password!p}")
+    Execute(cmd, environment=env_dict, user=params.unix_user, tries=3, try_sleep=5, logoutput=True)
+
+
+def setup_ranger_all_admin_password_change(admin_username, default_admin_password, admin_password,
+                                           rangerusersync_username, default_rangerusersync_user_password,
+                                           rangerusersync_user_password,
+                                           rangertagsync_username, default_rangertagsync_user_password,
+                                           rangertagsync_user_password,
+                                           keyadmin_username, default_keyadmin_user_password, keyadmin_user_password):
+    import params
+
+    env_dict = {'RANGER_ADMIN_HOME': params.ranger_home, 'JAVA_HOME': params.java_home}
+    if params.db_flavor.lower() == 'sqla':
+        env_dict = {'RANGER_ADMIN_HOME': params.ranger_home, 'JAVA_HOME': params.java_home,
+                    'LD_LIBRARY_PATH': params.ld_lib_path}
+
+    password_change_cmd = format("ambari-python-wrap {ranger_home}/db_setup.py -changepassword "
+                                 " -pair {admin_username} {default_admin_password!p} {admin_password!p} "
+                                 " -pair {rangerusersync_username} {default_rangerusersync_user_password!p} {rangerusersync_user_password!p} "
+                                 " -pair {rangertagsync_username} {default_rangertagsync_user_password!p} {rangertagsync_user_password!p} "
+                                 " -pair {keyadmin_username} {default_keyadmin_user_password!p} {keyadmin_user_password!p} ")
+    Execute(password_change_cmd, environment=env_dict, user=params.unix_user, tries=3, try_sleep=5, logoutput=True)
+
+
+@retry(times=10, sleep_time=5, err_class=Fail)
+def check_znode():
+    import params
+    solr_cloud_util.check_znode(
+        zookeeper_quorum=params.zookeeper_quorum,
+        solr_znode=params.solr_znode,
+        java64_home=params.java_home)
+
+
+def secure_znode(znode, jaasFile):
+    import params
+    solr_cloud_util.secure_znode(config=params.config, zookeeper_quorum=params.zookeeper_quorum,
+                                 solr_znode=znode,
+                                 jaas_file=jaasFile,
+                                 java64_home=params.java_home, sasl_users=[params.ranger_admin_jaas_principal])
+
+
+def get_ranger_plugin_principals(services_defaults_tuple_list):
+    """
+    Get ranger plugin user principals from service-default value maps using ranger-*-audit configurations
+    """
+    import params
+    user_principals = []
+    if len(services_defaults_tuple_list) < 1:
+        raise Exception("Services - defaults map parameter is missing.")
+
+    for (service, default_value) in services_defaults_tuple_list:
+        user_principal = default(
+            format("configurations/ranger-{service}-audit/xasecure.audit.jaas.Client.option.principal"), default_value)
+        user_principals.append(user_principal)
+    return user_principals
+
+
+def setup_tagsync_ssl_configs():
+    import params
+    Directory(params.security_store_path,
+              cd_access="a",
+              create_parents=True)
+
+    Directory(params.tagsync_etc_path,
+              cd_access="a",
+              owner=params.unix_user,
+              group=params.unix_group,
+              mode=0775,
+              create_parents=True)
+
+    # remove plain-text password from xml configs
+    ranger_tagsync_policymgr_ssl_copy = {}
+    ranger_tagsync_policymgr_ssl_copy.update(params.config['configurations']['ranger-tagsync-policymgr-ssl'])
+    for prop in params.ranger_tagsync_password_properties:
+        if prop in ranger_tagsync_policymgr_ssl_copy:
+            ranger_tagsync_policymgr_ssl_copy[prop] = "_"
+
+    XmlConfig("ranger-policymgr-ssl.xml",
+              conf_dir=params.ranger_tagsync_conf,
+              configurations=ranger_tagsync_policymgr_ssl_copy,
+              configuration_attributes=params.config['configurationAttributes']['ranger-tagsync-policymgr-ssl'],
+              owner=params.unix_user,
+              group=params.unix_group,
+              mode=0644)
+
+    ranger_credential_helper(params.tagsync_cred_lib, 'sslKeyStore', params.ranger_tagsync_keystore_password,
+                             params.ranger_tagsync_credential_file)
+    ranger_credential_helper(params.tagsync_cred_lib, 'sslTrustStore', params.ranger_tagsync_truststore_password,
+                             params.ranger_tagsync_credential_file)
+
+    File(params.ranger_tagsync_credential_file,
+         owner=params.unix_user,
+         group=params.unix_group,
+         only_if=format("test -e {ranger_tagsync_credential_file}"),
+         mode=0640
+         )
+
+    update_dot_jceks_crc_ownership(credential_provider_path=params.ranger_tagsync_credential_file,
+                                   user=params.unix_user, group=params.unix_group)
+
+    # remove plain-text password from xml configs
+    atlas_tagsync_ssl_copy = {}
+    atlas_tagsync_ssl_copy.update(params.config['configurations']['atlas-tagsync-ssl'])
+    for prop in params.ranger_tagsync_password_properties:
+        if prop in atlas_tagsync_ssl_copy:
+            atlas_tagsync_ssl_copy[prop] = "_"
+
+    XmlConfig("atlas-tagsync-ssl.xml",
+              conf_dir=params.ranger_tagsync_conf,
+              configurations=atlas_tagsync_ssl_copy,
+              configuration_attributes=params.config['configurationAttributes']['atlas-tagsync-ssl'],
+              owner=params.unix_user,
+              group=params.unix_group,
+              mode=0644)
+
+    ranger_credential_helper(params.tagsync_cred_lib, 'sslKeyStore', params.atlas_tagsync_keystore_password,
+                             params.atlas_tagsync_credential_file)
+    ranger_credential_helper(params.tagsync_cred_lib, 'sslTrustStore', params.atlas_tagsync_truststore_password,
+                             params.atlas_tagsync_credential_file)
+
+    File(params.atlas_tagsync_credential_file,
+         owner=params.unix_user,
+         group=params.unix_group,
+         only_if=format("test -e {atlas_tagsync_credential_file}"),
+         mode=0640
+         )
+
+    update_dot_jceks_crc_ownership(credential_provider_path=params.atlas_tagsync_credential_file, user=params.unix_user,
+                                   group=params.unix_group)
+
+    Logger.info("Configuring tagsync-ssl configurations done successfully.")
+
+
+def update_password_configs():
+    import params
+
+    password_configs = {'db_root_password': '_', 'db_password': '_'}
+
+    if params.stack_supports_ranger_audit_db:
+        password_configs['audit_db_password'] = '_'
+
+    ModifyPropertiesFile(format("{ranger_home}/install.properties"),
+                         properties=password_configs,
+                         owner=params.unix_user,
+                         )
+
+
+def validate_user_password(password_property=None):
+    import params
+
+    validation = []
+
+    if password_property is None:
+        ranger_password_properties = ['admin_password', 'ranger_admin_password', 'rangerusersync_user_password',
+                                      'rangertagsync_user_password', 'keyadmin_user_password']
+    else:
+        ranger_password_properties = [password_property]
+
+    for index in range(len(ranger_password_properties)):
+        password = params.config['configurations']['ranger-env'][ranger_password_properties[index]]
+        if not bool(re.search(r'^(?=.*[0-9])(?=.*[a-zA-Z]).{10,}$', password)) or bool(
+                re.search('[\\\`"\']', password)):
+            validation.append(ranger_password_properties[index])
+
+    if len(validation) > 0:
+        raise Fail("Password validation failed for : " + ", ".join(
+            validation) + ". Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are \" ' \ `")
+
+
+def update_dot_jceks_crc_ownership(credential_provider_path, user, group):
+    dot_jceks_crc_file_path = os.path.join(os.path.dirname(credential_provider_path),
+                                           "." + os.path.basename(credential_provider_path) + ".crc")
+
+    File(dot_jceks_crc_file_path,
+         owner=user,
+         group=group,
+         only_if=format("test -e {dot_jceks_crc_file_path}"),
+         mode=0640
+         )
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_admin_pam.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_admin_pam.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_admin_pam.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_admin_pam.j2	(date 1719626239000)
@@ -0,0 +1,22 @@
+{#
+ # Licensed to the Apache Software Foundation (ASF) under one
+ # or more contributor license agreements.  See the NOTICE file
+ # distributed with this work for additional information
+ # regarding copyright ownership.  The ASF licenses this file
+ # to you under the Apache License, Version 2.0 (the
+ # "License"); you may not use this file except in compliance
+ # with the License.  You may obtain a copy of the License at
+ #
+ #   http://www.apache.org/licenses/LICENSE-2.0
+ #
+ # Unless required by applicable law or agreed to in writing, software
+ # distributed under the License is distributed on an "AS IS" BASIS,
+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ # See the License for the specific language governing permissions and
+ # limitations under the License.
+ #}
+#%PAM-1.0
+auth    sufficient        pam_unix.so
+auth    sufficient        pam_sss.so
+account sufficient        pam_unix.so
+account sufficient        pam_sss.so
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_remote_pam.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_remote_pam.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_remote_pam.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_remote_pam.j2	(date 1719626239000)
@@ -0,0 +1,22 @@
+{#
+ # Licensed to the Apache Software Foundation (ASF) under one
+ # or more contributor license agreements.  See the NOTICE file
+ # distributed with this work for additional information
+ # regarding copyright ownership.  The ASF licenses this file
+ # to you under the Apache License, Version 2.0 (the
+ # "License"); you may not use this file except in compliance
+ # with the License.  You may obtain a copy of the License at
+ #
+ #   http://www.apache.org/licenses/LICENSE-2.0
+ #
+ # Unless required by applicable law or agreed to in writing, software
+ # distributed under the License is distributed on an "AS IS" BASIS,
+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ # See the License for the specific language governing permissions and
+ # limitations under the License.
+ #}
+#%PAM-1.0
+auth    sufficient        pam_unix.so
+auth    sufficient        pam_sss.so
+account sufficient        pam_unix.so
+account sufficient        pam_sss.so
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_solr_jaas_conf.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_solr_jaas_conf.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_solr_jaas_conf.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/ranger_solr_jaas_conf.j2	(date 1719626239000)
@@ -0,0 +1,26 @@
+{#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#}
+
+Client {
+  com.sun.security.auth.module.Krb5LoginModule required
+  useKeyTab=true
+  storeKey=true
+  useTicketCache=false
+  keyTab="{{solr_kerberos_keytab}}"
+  principal="{{solr_kerberos_principal}}";
+};
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/input.config-ranger.json.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/input.config-ranger.json.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/input.config-ranger.json.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/package/templates/input.config-ranger.json.j2	(date 1719626239000)
@@ -0,0 +1,79 @@
+{#
+ # Licensed to the Apache Software Foundation (ASF) under one
+ # or more contributor license agreements.  See the NOTICE file
+ # distributed with this work for additional information
+ # regarding copyright ownership.  The ASF licenses this file
+ # to you under the Apache License, Version 2.0 (the
+ # "License"); you may not use this file except in compliance
+ # with the License.  You may obtain a copy of the License at
+ #
+ #   http://www.apache.org/licenses/LICENSE-2.0
+ #
+ # Unless required by applicable law or agreed to in writing, software
+ # distributed under the License is distributed on an "AS IS" BASIS,
+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ # See the License for the specific language governing permissions and
+ # limitations under the License.
+ #}
+{
+  "input":[
+    {
+      "type":"ranger_admin",
+      "rowtype":"service",
+      "path":"{{default('/configurations/ranger-admin-site/ranger.logs.base.dir', '/var/log/ranger/admin')}}/xa_portal.log"
+    },
+    {
+      "type":"ranger_dbpatch",
+      "rowtype":"service",
+      "path":"{{default('/configurations/ranger-admin-site/ranger.logs.base.dir', '/var/log/ranger/admin')}}/ranger_db_patch.log"
+    },
+    {
+      "type":"ranger_usersync",
+      "rowtype":"service",
+      "path":"{{default('/configurations/ranger-ugsync-site/ranger.usersync.logdir', '/var/log/ranger/usersync')}}/usersync.log"
+    }
+  ],
+  "filter":[
+    {
+      "filter":"grok",
+      "conditions":{
+        "fields":{
+          "type":[
+            "ranger_admin",
+            "ranger_dbpatch"
+          ]
+        }
+      },
+      "log4j_format":"%d [%t] %-5p %C{6} (%F:%L) - %m%n",
+      "multiline_pattern":"^(%{TIMESTAMP_ISO8601:logtime})",
+      "message_pattern":"(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}\\[%{DATA:thread_name}\\]%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\(%{JAVAFILE:file}:%{INT:line_number}\\)%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}",
+      "post_map_values":{
+        "logtime":{
+          "map_date":{
+            "target_date_pattern":"yyyy-MM-dd HH:mm:ss,SSS"
+          }
+        }
+      }
+    },
+    {
+      "filter":"grok",
+      "conditions":{
+        "fields":{
+          "type":[
+            "ranger_usersync"
+          ]
+        }
+      },
+      "log4j_format":"%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %m%n",
+      "multiline_pattern":"^(%{USER_SYNC_DATE:logtime})",
+      "message_pattern":"(?m)^%{USER_SYNC_DATE:logtime}%{SPACE}%{LOGLEVEL:level}%{SPACE}%{JAVACLASS:logger_name}%{SPACE}\\[%{DATA:thread_name}\\]%{SPACE}-%{SPACE}%{GREEDYDATA:log_message}",
+      "post_map_values":{
+        "logtime":{
+          "map_date":{
+            "target_date_pattern":"dd MMM yyyy HH:mm:ss"
+          }
+        }
+      }
+    }
+  ]
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/templates/kafka.conf.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/templates/kafka.conf.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/templates/kafka.conf.j2
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/templates/kafka.conf.j2	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/templates/kafka.conf.j2	(date 1719626239000)
@@ -16,5 +16,20 @@
 # limitations under the License.
 #}
 
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 {{kafka_user}}   - nofile   {{kafka_user_nofile_limit}}
 {{kafka_user}}   - nproc    {{kafka_user_nproc_limit}}
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-env.sh.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-env.sh.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-env.sh.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-env.sh.j2	(date 1719626239000)
@@ -0,0 +1,48 @@
+#!/bin/bash
+
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#  http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Set Ranger-specific environment variables here.
+export JAVA_HOME={{java_home}}
+
+{% if is_ranger_admin_host  %}
+# Ranger Admin specific environment variables here.
+export RANGER_ADMIN_LOG_DIR={{admin_log_dir}}
+export RANGER_PID_DIR_PATH={{ranger_pid_dir}}
+export RANGER_USER={{unix_user}}
+ranger_admin_max_heap_size={{ranger_admin_max_heap_size}}
+{% if security_enabled %}
+export JAVA_OPTS=" ${JAVA_OPTS} -Dzookeeper.sasl.client.username={{zookeeper_principal_primary}} "
+{% endif %}
+{% endif %}
+
+{% if is_ranger_usersync_host  %}
+# Ranger Usersync specific environment variables here.
+export USERSYNC_CONF_DIR={{ranger_ugsync_conf}}
+export logdir={{usersync_log_dir}}
+export USERSYNC_PID_DIR_PATH={{ranger_pid_dir}}
+export UNIX_USERSYNC_USER={{unix_user}}
+ranger_usersync_max_heap_size={{ranger_usersync_max_heap_size}}
+{% endif %}
+
+{% if is_ranger_tagsync_host  %}
+# Ranger Tagsync specific environment variables here.
+export RANGER_TAGSYNC_LOG_DIR={{tagsync_log_dir}}
+export TAGSYNC_PID_DIR_PATH={{ranger_pid_dir}}
+export UNIX_TAGSYNC_USER={{unix_user}}
+ranger_tagsync_max_heap_size={{ranger_tagsync_max_heap_size}}
+{% endif %}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/admin-logback.xml.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/admin-logback.xml.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/admin-logback.xml.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/admin-logback.xml.j2	(date 1719626239000)
@@ -0,0 +1,116 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<configuration>
+  <appender name="xa_log_appender" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <!--See http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
+    <!--and http://logback.qos.ch/manual/appenders.html#TimeBasedRollingPolicy-->
+    <!--for further documentation-->
+    <file>${logdir}/ranger-admin-${hostname}-${user}.log</file>
+    <append>true</append>
+    <encoder>
+      <pattern>%date [%thread] %level{5} [%file:%line] %msg%n</pattern>
+    </encoder>
+    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
+      <fileNamePattern>${logdir}/ranger-admin-${hostname}-${user}.log.%d{yyyy-MM-dd}</fileNamePattern>
+      <maxHistory>15</maxHistory>
+      <cleanHistoryOnStart>true</cleanHistoryOnStart>
+    </rollingPolicy>
+  </appender>
+  <appender name="sql_appender" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <!--See http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
+    <!--and http://logback.qos.ch/manual/appenders.html#TimeBasedRollingPolicy-->
+    <!--for further documentation-->
+    <file>${logdir}/ranger_admin_sql.log</file>
+    <append>true</append>
+    <encoder>
+      <pattern>%d [%t] %-5p %C{6} \(%F:%L\) %msg%n</pattern>
+    </encoder>
+    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
+      <fileNamePattern>${logdir}/ranger_admin_sql.log.%d{yyyy-MM-dd}</fileNamePattern>
+      <maxHistory>15</maxHistory>
+      <cleanHistoryOnStart>true</cleanHistoryOnStart>
+    </rollingPolicy>
+  </appender>
+  <appender name="perf_appender" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <!--See http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
+    <!--and http://logback.qos.ch/manual/appenders.html#TimeBasedRollingPolicy-->
+    <!--for further documentation-->
+    <file>${logdir}/ranger_admin_perf.log</file>
+    <append>true</append>
+    <encoder>
+      <pattern>%d [%t] %msg%n</pattern>
+    </encoder>
+    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
+      <fileNamePattern>${logdir}/ranger_admin_perf.log.%d{yyyy-MM-dd}</fileNamePattern>
+      <maxHistory>15</maxHistory>
+      <cleanHistoryOnStart>true</cleanHistoryOnStart>
+    </rollingPolicy>
+  </appender>
+  <appender name="patch_logger" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <!--See http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
+    <!--and http://logback.qos.ch/manual/appenders.html#TimeBasedRollingPolicy-->
+    <!--for further documentation-->
+    <append>true</append>
+    <file>${logdir}/ranger_db_patch.log</file>
+    <encoder>
+      <pattern>%d [%t] %-5p %C{6} \(%F:%L\) %msg%n</pattern>
+    </encoder>
+    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
+      <fileNamePattern>${logdir}/ranger_db_patch.log.%d{yyyy-MM-dd}</fileNamePattern>
+      <maxHistory>15</maxHistory>
+      <cleanHistoryOnStart>true</cleanHistoryOnStart>
+    </rollingPolicy>
+  </appender>
+  <logger name="xa" additivity="false" level="info">
+    <appender-ref ref="xa_log_appender"/>
+  </logger>
+  <logger name="com.mchange" additivity="false" level="error">
+    <appender-ref ref="sql_appender"/>
+  </logger>
+  <logger name="org.apache.ranger.perf" additivity="false" level="info">
+    <appender-ref ref="perf_appender"/>
+  </logger>
+  <logger name="jdbc.audit" additivity="false" level="error">
+    <appender-ref ref="sql_appender"/>
+  </logger>
+  <logger name="org.apache.ranger.patch" additivity="false" level="info">
+    <appender-ref ref="patch_logger"/>
+  </logger>
+  <logger name="jdbc.resultset" additivity="false" level="error">
+    <appender-ref ref="sql_appender"/>
+  </logger>
+  <logger name="org.springframework" additivity="false" level="warn">
+    <appender-ref ref="patch_logger"/>
+  </logger>
+  <logger name="jdbc.sqltiming" additivity="false" level="warn">
+    <appender-ref ref="sql_appender"/>
+  </logger>
+  <logger name="org.hibernate.SQL" additivity="false" level="warn">
+    <appender-ref ref="sql_appender"/>
+  </logger>
+  <logger name="org.apache.ranger" additivity="false" level="info">
+    <appender-ref ref="xa_log_appender"/>
+  </logger>
+  <logger name="jdbc.sqlonly" additivity="false" level="error">
+    <appender-ref ref="sql_appender"/>
+  </logger>
+  <root level="warn">
+    <appender-ref ref="xa_log_appender"/>
+  </root>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/tagsync-logback.xml.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/tagsync-logback.xml.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/tagsync-logback.xml.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/tagsync-logback.xml.j2	(date 1719626239000)
@@ -0,0 +1,37 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<configuration>
+  <appender name="logFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <!--See http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
+    <!--and http://logback.qos.ch/manual/appenders.html#TimeBasedRollingPolicy-->
+    <!--for further documentation-->
+    <file>${logdir}/tagsync.log</file>
+    <encoder>
+      <pattern>%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %L %m%n</pattern>
+    </encoder>
+    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
+      <fileNamePattern>${logdir}/tagsync.log.%d{yyyy-MM-dd}</fileNamePattern>
+      <maxHistory>15</maxHistory>
+      <cleanHistoryOnStart>true</cleanHistoryOnStart>
+    </rollingPolicy>
+  </appender>
+  <root level="info">
+    <appender-ref ref="logFile"/>
+  </root>
+</configuration>
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/usersync-logback.xml.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/usersync-logback.xml.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/usersync-logback.xml.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/usersync-logback.xml.j2	(date 1719626239000)
@@ -0,0 +1,37 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+
+<configuration>
+  <appender name="logFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
+    <!--See http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
+    <!--and http://logback.qos.ch/manual/appenders.html#TimeBasedRollingPolicy-->
+    <!--for further documentation-->
+    <file>${logdir}/usersync-${hostname}-${user}.log</file>
+    <encoder>
+      <pattern>%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %m%n</pattern>
+    </encoder>
+    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
+      <fileNamePattern>${logdir}/usersync-${hostname}-${user}.log.%d{yyyy-MM-dd}</fileNamePattern>
+      <maxHistory>15</maxHistory>
+      <cleanHistoryOnStart>true</cleanHistoryOnStart>
+    </rollingPolicy>
+  </appender>
+  <root level="info">
+    <appender-ref ref="logFile"/>
+  </root>
+</configuration>
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka.py	(date 1719626239000)
@@ -17,6 +17,7 @@
 limitations under the License.
 
 """
+import collections
 import os
 
 from resource_management.libraries.functions.version import format_stack_version
@@ -29,6 +30,7 @@
 from resource_management.libraries.functions.generate_logfeeder_input_config import generate_logfeeder_input_config
 from resource_management.libraries.functions.stack_features import check_stack_feature
 from resource_management.libraries.functions import StackFeature
+from resource_management.libraries.functions import Direction
 import re
 
 from resource_management.core.logger import Logger
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-solrconfig.xml.j2
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-solrconfig.xml.j2 b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-solrconfig.xml.j2
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/properties/ranger-solrconfig.xml.j2	(date 1719626239000)
@@ -0,0 +1,1877 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--
+     For more details about configurations options that may appear in
+     this file, see http://wiki.apache.org/solr/SolrConfigXml.
+-->
+<config>
+  <!-- In all configuration below, a prefix of "solr." for class names
+       is an alias that causes solr to search appropriate packages,
+       including org.apache.solr.(search|update|request|core|analysis)
+
+       You may also specify a fully qualified Java classname if you
+       have your own custom plugins.
+    -->
+
+  <!-- Controls what version of Lucene various components of Solr
+       adhere to.  Generally, you want to use the latest version to
+       get all bug fixes and improvements. It is highly recommended
+       that you fully re-index after changing this setting as it can
+       affect both how text is indexed and queried.
+  -->
+  <luceneMatchVersion>6.6.2</luceneMatchVersion>
+
+  <!-- <lib/> directives can be used to instruct Solr to load any Jars
+       identified and use them to resolve any "plugins" specified in
+       your solrconfig.xml or schema.xml (ie: Analyzers, Request
+       Handlers, etc...).
+
+       All directories and paths are resolved relative to the
+       instanceDir.
+
+       Please note that <lib/> directives are processed in the order
+       that they appear in your solrconfig.xml file, and are "stacked"
+       on top of each other when building a ClassLoader - so if you have
+       plugin jars with dependencies on other jars, the "lower level"
+       dependency jars should be loaded first.
+
+       If a "./lib" directory exists in your instanceDir, all files
+       found in it are included as if you had used the following
+       syntax...
+
+              <lib dir="./lib" />
+    -->
+
+  <!-- A 'dir' option by itself adds any files found in the directory
+       to the classpath, this is useful for including all jars in a
+       directory.
+
+       When a 'regex' is specified in addition to a 'dir', only the
+       files in that directory which completely match the regex
+       (anchored on both ends) will be included.
+
+       If a 'dir' option (with or without a regex) is used and nothing
+       is found that matches, a warning will be logged.
+
+       The examples below can be used to load some solr-contribs along
+       with their external dependencies.
+    -->
+  <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-dataimporthandler-.*\.jar" />
+
+  <lib dir="${solr.install.dir:../../../..}/contrib/extraction/lib" regex=".*\.jar" />
+  <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-cell-\d.*\.jar" />
+
+  <lib dir="${solr.install.dir:../../../..}/contrib/clustering/lib/" regex=".*\.jar" />
+  <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-clustering-\d.*\.jar" />
+
+  <lib dir="${solr.install.dir:../../../..}/contrib/langid/lib/" regex=".*\.jar" />
+  <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-langid-\d.*\.jar" />
+
+  <lib dir="${solr.install.dir:../../../..}/contrib/velocity/lib" regex=".*\.jar" />
+  <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-velocity-\d.*\.jar" />
+
+  <!-- an exact 'path' can be used instead of a 'dir' to specify a
+       specific jar file.  This will cause a serious error to be logged
+       if it can't be loaded.
+    -->
+  <!--
+     <lib path="../a-jar-that-does-not-exist.jar" />
+  -->
+
+  <!-- Data Directory
+
+       Used to specify an alternate directory to hold all index data
+       other than the default ./data under the Solr home.  If
+       replication is in use, this should match the replication
+       configuration.
+    -->
+  <dataDir>${solr.data.dir:}</dataDir>
+
+
+  <!-- The DirectoryFactory to use for indexes.
+
+       solr.StandardDirectoryFactory is filesystem
+       based and tries to pick the best implementation for the current
+       JVM and platform.  solr.NRTCachingDirectoryFactory, the default,
+       wraps solr.StandardDirectoryFactory and caches small files in memory
+       for better NRT performance.
+
+       One can force a particular implementation via solr.MMapDirectoryFactory,
+       solr.NIOFSDirectoryFactory, or solr.SimpleFSDirectoryFactory.
+
+       solr.RAMDirectoryFactory is memory based, not
+       persistent, and doesn't work with replication.
+    -->
+  <directoryFactory name="DirectoryFactory"
+                    class="${solr.directoryFactory:solr.NRTCachingDirectoryFactory}">
+
+
+    <!-- These will be used if you are using the solr.HdfsDirectoryFactory,
+         otherwise they will be ignored. If you don't plan on using hdfs,
+         you can safely remove this section. -->
+    <!-- The root directory that collection data should be written to. -->
+    <str name="solr.hdfs.home">${solr.hdfs.home:}</str>
+    <!-- The hadoop configuration files to use for the hdfs client. -->
+    <str name="solr.hdfs.confdir">${solr.hdfs.confdir:}</str>
+    <!-- Enable/Disable the hdfs cache. -->
+    <str name="solr.hdfs.blockcache.enabled">${solr.hdfs.blockcache.enabled:true}</str>
+    <!-- Enable/Disable using one global cache for all SolrCores.
+         The settings used will be from the first HdfsDirectoryFactory created. -->
+    <str name="solr.hdfs.blockcache.global">${solr.hdfs.blockcache.global:true}</str>
+
+  </directoryFactory>
+
+  <!-- The CodecFactory for defining the format of the inverted index.
+       The default implementation is SchemaCodecFactory, which is the official Lucene
+       index format, but hooks into the schema to provide per-field customization of
+       the postings lists and per-document values in the fieldType element
+       (postingsFormat/docValuesFormat). Note that most of the alternative implementations
+       are experimental, so if you choose to customize the index format, it's a good
+       idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
+       before upgrading to a newer version to avoid unnecessary reindexing.
+  -->
+  <codecFactory class="solr.SchemaCodecFactory"/>
+
+  <!-- To enable dynamic schema REST APIs, use the following for <schemaFactory>: -->
+
+       <schemaFactory class="ManagedIndexSchemaFactory">
+         <bool name="mutable">true</bool>
+         <str name="managedSchemaResourceName">managed-schema</str>
+       </schemaFactory>
+<!--
+       When ManagedIndexSchemaFactory is specified, Solr will load the schema from
+       the resource named in 'managedSchemaResourceName', rather than from schema.xml.
+       Note that the managed schema resource CANNOT be named schema.xml.  If the managed
+       schema does not exist, Solr will create it after reading schema.xml, then rename
+       'schema.xml' to 'schema.xml.bak'.
+
+       Do NOT hand edit the managed schema - external modifications will be ignored and
+       overwritten as a result of schema modification REST API calls.
+
+       When ManagedIndexSchemaFactory is specified with mutable = true, schema
+       modification REST API calls will be allowed; otherwise, error responses will be
+       sent back for these requests.
+
+  <schemaFactory class="ClassicIndexSchemaFactory"/>
+  -->
+
+  <!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+       Index Config - These settings control low-level behavior of indexing
+       Most example settings here show the default value, but are commented
+       out, to more easily see where customizations have been made.
+
+       Note: This replaces <indexDefaults> and <mainIndex> from older versions
+       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+  <indexConfig>
+    <!-- maxFieldLength was removed in 4.0. To get similar behavior, include a
+         LimitTokenCountFilterFactory in your fieldType definition. E.g.
+     <filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="10000"/>
+    -->
+    <!-- Maximum time to wait for a write lock (ms) for an IndexWriter. Default: 1000 -->
+    <!-- <writeLockTimeout>1000</writeLockTimeout>  -->
+
+    <!-- The maximum number of simultaneous threads that may be
+         indexing documents at once in IndexWriter; if more than this
+         many threads arrive they will wait for others to finish.
+         Default in Solr/Lucene is 8. -->
+    <!-- <maxIndexingThreads>8</maxIndexingThreads>  -->
+
+    <!-- Expert: Enabling compound file will use less files for the index,
+         using fewer file descriptors on the expense of performance decrease.
+         Default in Lucene is "true". Default in Solr is "false" (since 3.6) -->
+    <!-- <useCompoundFile>false</useCompoundFile> -->
+
+    <!-- ramBufferSizeMB sets the amount of RAM that may be used by Lucene
+         indexing for buffering added documents and deletions before they are
+         flushed to the Directory.
+         maxBufferedDocs sets a limit on the number of documents buffered
+         before flushing.
+         If both ramBufferSizeMB and maxBufferedDocs is set, then
+         Lucene will flush based on whichever limit is hit first.
+         The default is 100 MB.  -->
+    <!-- <ramBufferSizeMB>100</ramBufferSizeMB> -->
+    <!-- <maxBufferedDocs>1000</maxBufferedDocs> -->
+
+    <!-- Expert: Merge Policy
+         The Merge Policy in Lucene controls how merging of segments is done.
+         The default since Solr/Lucene 3.3 is TieredMergePolicy.
+         The default since Lucene 2.3 was the LogByteSizeMergePolicy,
+         Even older versions of Lucene used LogDocMergePolicy.
+      -->
+    <!--
+        <mergePolicy class="org.apache.lucene.index.TieredMergePolicy">
+          <int name="maxMergeAtOnce">10</int>
+          <int name="segmentsPerTier">10</int>
+        </mergePolicy>
+      -->
+
+    <!-- Merge Factor
+         The merge factor controls how many segments will get merged at a time.
+         For TieredMergePolicy, mergeFactor is a convenience parameter which
+         will set both MaxMergeAtOnce and SegmentsPerTier at once.
+         For LogByteSizeMergePolicy, mergeFactor decides how many new segments
+         will be allowed before they are merged into one.
+         Default is 10 for both merge policies.
+      -->
+    <!--
+    <mergeFactor>10</mergeFactor>
+      -->
+
+    <!-- Ranger customization. Set to 5 to trigger purging of deleted documents more often -->
+    <mergePolicyFactory class="org.apache.solr.index.TieredMergePolicyFactory">
+      <int name="maxMergeAtOnce">{{ranger_audit_logs_merge_factor}}</int>
+      <int name="segmentsPerTier">{{ranger_audit_logs_merge_factor}}</int>
+    </mergePolicyFactory>
+
+    <!-- Expert: Merge Scheduler
+         The Merge Scheduler in Lucene controls how merges are
+         performed.  The ConcurrentMergeScheduler (Lucene 2.3 default)
+         can perform merges in the background using separate threads.
+         The SerialMergeScheduler (Lucene 2.2 default) does not.
+     -->
+    <!--
+       <mergeScheduler class="org.apache.lucene.index.ConcurrentMergeScheduler"/>
+       -->
+
+    <!-- LockFactory
+
+         This option specifies which Lucene LockFactory implementation
+         to use.
+
+         single = SingleInstanceLockFactory - suggested for a
+                  read-only index or when there is no possibility of
+                  another process trying to modify the index.
+         native = NativeFSLockFactory - uses OS native file locking.
+                  Do not use when multiple solr webapps in the same
+                  JVM are attempting to share a single index.
+         simple = SimpleFSLockFactory  - uses a plain file for locking
+
+         Defaults: 'native' is default for Solr3.6 and later, otherwise
+                   'simple' is the default
+
+         More details on the nuances of each LockFactory...
+         http://wiki.apache.org/lucene-java/AvailableLockFactories
+    -->
+    <lockType>${solr.lock.type:native}</lockType>
+
+    <!-- Unlock On Startup
+
+         If true, unlock any held write or commit locks on startup.
+         This defeats the locking mechanism that allows multiple
+         processes to safely access a lucene index, and should be used
+         with care. Default is "false".
+
+         This is not needed if lock type is 'single'
+     -->
+    <!--
+    <unlockOnStartup>false</unlockOnStartup>
+      -->
+
+    <!-- Commit Deletion Policy
+         Custom deletion policies can be specified here. The class must
+         implement org.apache.lucene.index.IndexDeletionPolicy.
+
+         The default Solr IndexDeletionPolicy implementation supports
+         deleting index commit points on number of commits, age of
+         commit point and optimized status.
+
+         The latest commit point should always be preserved regardless
+         of the criteria.
+    -->
+    <!--
+    <deletionPolicy class="solr.SolrDeletionPolicy">
+    -->
+      <!-- The number of commit points to be kept -->
+      <!-- <str name="maxCommitsToKeep">1</str> -->
+      <!-- The number of optimized commit points to be kept -->
+      <!-- <str name="maxOptimizedCommitsToKeep">0</str> -->
+      <!--
+          Delete all commit points once they have reached the given age.
+          Supports DateMathParser syntax e.g.
+        -->
+      <!--
+         <str name="maxCommitAge">30MINUTES</str>
+         <str name="maxCommitAge">1DAY</str>
+      -->
+    <!--
+    </deletionPolicy>
+    -->
+
+    <!-- Lucene Infostream
+
+         To aid in advanced debugging, Lucene provides an "InfoStream"
+         of detailed information when indexing.
+
+         Setting the value to true will instruct the underlying Lucene
+         IndexWriter to write its info stream to solr's log. By default,
+         this is enabled here, and controlled through log4j.properties.
+      -->
+     <infoStream>true</infoStream>
+  </indexConfig>
+
+
+  <!-- JMX
+
+       This example enables JMX if and only if an existing MBeanServer
+       is found, use this if you want to configure JMX through JVM
+       parameters. Remove this to disable exposing Solr configuration
+       and statistics to JMX.
+
+       For more details see http://wiki.apache.org/solr/SolrJmx
+    -->
+  <jmx />
+  <!-- If you want to connect to a particular server, specify the
+       agentId
+    -->
+  <!-- <jmx agentId="myAgent" /> -->
+  <!-- If you want to start a new MBeanServer, specify the serviceUrl -->
+  <!-- <jmx serviceUrl="service:jmx:rmi:///jndi/rmi://localhost:9999/solr"/>
+    -->
+
+  <!-- The default high-performance update handler -->
+  <updateHandler class="solr.DirectUpdateHandler2">
+
+    <!-- Enables a transaction log, used for real-time get, durability, and
+         and solr cloud replica recovery.  The log can grow as big as
+         uncommitted changes to the index, so use of a hard autoCommit
+         is recommended (see below).
+         "dir" - the target directory for transaction logs, defaults to the
+                solr data directory.  -->
+    <updateLog>
+      <str name="dir">${solr.ulog.dir:}</str>
+    </updateLog>
+
+    <!-- AutoCommit
+
+         Perform a hard commit automatically under certain conditions.
+         Instead of enabling autoCommit, consider using "commitWithin"
+         when adding documents.
+
+         http://wiki.apache.org/solr/UpdateXmlMessages
+
+         maxDocs - Maximum number of documents to add since the last
+                   commit before automatically triggering a new commit.
+
+         maxTime - Maximum amount of time in ms that is allowed to pass
+                   since a document was added before automatically
+                   triggering a new commit.
+         openSearcher - if false, the commit causes recent index changes
+           to be flushed to stable storage, but does not cause a new
+           searcher to be opened to make those changes visible.
+
+         If the updateLog is enabled, then it's highly recommended to
+         have some sort of hard autoCommit to limit the log size.
+      -->
+     <autoCommit>
+       <maxTime>${solr.autoCommit.maxTime:15000}</maxTime>
+       <openSearcher>false</openSearcher>
+     </autoCommit>
+
+    <!-- softAutoCommit is like autoCommit except it causes a
+         'soft' commit which only ensures that changes are visible
+         but does not ensure that data is synced to disk.  This is
+         faster and more near-realtime friendly than a hard commit.
+      -->
+
+     <autoSoftCommit>
+       <maxTime>${solr.autoSoftCommit.maxTime:5000}</maxTime>
+     </autoSoftCommit>
+
+    <!-- Update Related Event Listeners
+
+         Various IndexWriter related events can trigger Listeners to
+         take actions.
+
+         postCommit - fired after every commit or optimize command
+         postOptimize - fired after every optimize command
+      -->
+    <!-- The RunExecutableListener executes an external command from a
+         hook such as postCommit or postOptimize.
+
+         exe - the name of the executable to run
+         dir - dir to use as the current working directory. (default=".")
+         wait - the calling thread waits until the executable returns.
+                (default="true")
+         args - the arguments to pass to the program.  (default is none)
+         env - environment variables to set.  (default is none)
+      -->
+    <!-- This example shows how RunExecutableListener could be used
+         with the script based replication...
+         http://wiki.apache.org/solr/CollectionDistribution
+      -->
+    <!--
+       <listener event="postCommit" class="solr.RunExecutableListener">
+         <str name="exe">solr/bin/snapshooter</str>
+         <str name="dir">.</str>
+         <bool name="wait">true</bool>
+         <arr name="args"> <str>arg1</str> <str>arg2</str> </arr>
+         <arr name="env"> <str>MYVAR=val1</str> </arr>
+       </listener>
+      -->
+
+  </updateHandler>
+
+  <!-- IndexReaderFactory
+
+       Use the following format to specify a custom IndexReaderFactory,
+       which allows for alternate IndexReader implementations.
+
+       ** Experimental Feature **
+
+       Please note - Using a custom IndexReaderFactory may prevent
+       certain other features from working. The API to
+       IndexReaderFactory may change without warning or may even be
+       removed from future releases if the problems cannot be
+       resolved.
+
+
+       ** Features that may not work with custom IndexReaderFactory **
+
+       The ReplicationHandler assumes a disk-resident index. Using a
+       custom IndexReader implementation may cause incompatibility
+       with ReplicationHandler and may cause replication to not work
+       correctly. See SOLR-1366 for details.
+
+    -->
+  <!--
+  <indexReaderFactory name="IndexReaderFactory" class="package.class">
+    <str name="someArg">Some Value</str>
+  </indexReaderFactory >
+  -->
+
+  <!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+       Query section - these settings control query time things like caches
+       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+  <query>
+    <!-- Max Boolean Clauses
+
+         Maximum number of clauses in each BooleanQuery,  an exception
+         is thrown if exceeded.
+
+         ** WARNING **
+
+         This option actually modifies a global Lucene property that
+         will affect all SolrCores.  If multiple solrconfig.xml files
+         disagree on this property, the value at any given moment will
+         be based on the last SolrCore to be initialized.
+
+      -->
+    <maxBooleanClauses>1024</maxBooleanClauses>
+
+
+    <!-- Solr Internal Query Caches
+
+         There are two implementations of cache available for Solr,
+         LRUCache, based on a synchronized LinkedHashMap, and
+         FastLRUCache, based on a ConcurrentHashMap.
+
+         FastLRUCache has faster gets and slower puts in single
+         threaded operation and thus is generally faster than LRUCache
+         when the hit ratio of the cache is high (> 75%), and may be
+         faster under other scenarios on multi-cpu systems.
+    -->
+
+    <!-- Filter Cache
+
+         Cache used by SolrIndexSearcher for filters (DocSets),
+         unordered sets of *all* documents that match a query.  When a
+         new searcher is opened, its caches may be prepopulated or
+         "autowarmed" using data from caches in the old searcher.
+         autowarmCount is the number of items to prepopulate.  For
+         LRUCache, the autowarmed items will be the most recently
+         accessed items.
+
+         Parameters:
+           class - the SolrCache implementation LRUCache or
+               (LRUCache or FastLRUCache)
+           size - the maximum number of entries in the cache
+           initialSize - the initial capacity (number of entries) of
+               the cache.  (see java.util.HashMap)
+           autowarmCount - the number of entries to prepopulate from
+               and old cache.
+      -->
+    <filterCache class="solr.FastLRUCache"
+                 size="512"
+                 initialSize="512"
+                 autowarmCount="0"/>
+
+    <!-- Query Result Cache
+
+         Caches results of searches - ordered lists of document ids
+         (DocList) based on a query, a sort, and the range of documents requested.
+      -->
+    <queryResultCache class="solr.LRUCache"
+                     size="512"
+                     initialSize="512"
+                     autowarmCount="0"/>
+
+    <!-- Document Cache
+
+         Caches Lucene Document objects (the stored fields for each
+         document).  Since Lucene internal document ids are transient,
+         this cache will not be autowarmed.
+      -->
+    <documentCache class="solr.LRUCache"
+                   size="512"
+                   initialSize="512"
+                   autowarmCount="0"/>
+
+    <!-- custom cache currently used by block join -->
+    <cache name="perSegFilter"
+      class="solr.search.LRUCache"
+      size="10"
+      initialSize="0"
+      autowarmCount="10"
+      regenerator="solr.NoOpRegenerator" />
+
+    <!-- Field Value Cache
+
+         Cache used to hold field values that are quickly accessible
+         by document id.  The fieldValueCache is created by default
+         even if not configured here.
+      -->
+    <!--
+       <fieldValueCache class="solr.FastLRUCache"
+                        size="512"
+                        autowarmCount="128"
+                        showItems="32" />
+      -->
+
+    <!-- Custom Cache
+
+         Example of a generic cache.  These caches may be accessed by
+         name through SolrIndexSearcher.getCache(),cacheLookup(), and
+         cacheInsert().  The purpose is to enable easy caching of
+         user/application level data.  The regenerator argument should
+         be specified as an implementation of solr.CacheRegenerator
+         if autowarming is desired.
+      -->
+    <!--
+       <cache name="myUserCache"
+              class="solr.LRUCache"
+              size="4096"
+              initialSize="1024"
+              autowarmCount="1024"
+              regenerator="com.mycompany.MyRegenerator"
+              />
+      -->
+
+
+    <!-- Lazy Field Loading
+
+         If true, stored fields that are not requested will be loaded
+         lazily.  This can result in a significant speed improvement
+         if the usual case is to not load all stored fields,
+         especially if the skipped fields are large compressed text
+         fields.
+    -->
+    <enableLazyFieldLoading>true</enableLazyFieldLoading>
+
+   <!-- Use Filter For Sorted Query
+
+        A possible optimization that attempts to use a filter to
+        satisfy a search.  If the requested sort does not include
+        score, then the filterCache will be checked for a filter
+        matching the query. If found, the filter will be used as the
+        source of document ids, and then the sort will be applied to
+        that.
+
+        For most situations, this will not be useful unless you
+        frequently get the same search repeatedly with different sort
+        options, and none of them ever use "score"
+     -->
+   <!--
+      <useFilterForSortedQuery>true</useFilterForSortedQuery>
+     -->
+
+   <!-- Result Window Size
+
+        An optimization for use with the queryResultCache.  When a search
+        is requested, a superset of the requested number of document ids
+        are collected.  For example, if a search for a particular query
+        requests matching documents 10 through 19, and queryWindowSize is 50,
+        then documents 0 through 49 will be collected and cached.  Any further
+        requests in that range can be satisfied via the cache.
+     -->
+   <queryResultWindowSize>20</queryResultWindowSize>
+
+   <!-- Maximum number of documents to cache for any entry in the
+        queryResultCache.
+     -->
+   <queryResultMaxDocsCached>200</queryResultMaxDocsCached>
+
+   <!-- Query Related Event Listeners
+
+        Various IndexSearcher related events can trigger Listeners to
+        take actions.
+
+        newSearcher - fired whenever a new searcher is being prepared
+        and there is a current searcher handling requests (aka
+        registered).  It can be used to prime certain caches to
+        prevent long request times for certain requests.
+
+        firstSearcher - fired whenever a new searcher is being
+        prepared but there is no current registered searcher to handle
+        requests or to gain autowarming data from.
+
+
+     -->
+    <!-- QuerySenderListener takes an array of NamedList and executes a
+         local query request for each NamedList in sequence.
+      -->
+    <listener event="newSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <!--
+           <lst><str name="q">solr</str><str name="sort">price asc</str></lst>
+           <lst><str name="q">rocks</str><str name="sort">weight asc</str></lst>
+          -->
+      </arr>
+    </listener>
+    <listener event="firstSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <lst>
+          <str name="q">static firstSearcher warming in solrconfig.xml</str>
+        </lst>
+      </arr>
+    </listener>
+
+    <!-- Use Cold Searcher
+
+         If a search request comes in and there is no current
+         registered searcher, then immediately register the still
+         warming searcher and use it.  If "false" then all requests
+         will block until the first searcher is done warming.
+      -->
+    <useColdSearcher>true</useColdSearcher>
+
+    <!-- Max Warming Searchers
+
+         Maximum number of searchers that may be warming in the
+         background concurrently.  An error is returned if this limit
+         is exceeded.
+
+         Recommend values of 1-2 for read-only slaves, higher for
+         masters w/o cache warming.
+      -->
+    <maxWarmingSearchers>2</maxWarmingSearchers>
+
+  </query>
+
+
+  <!-- Request Dispatcher
+
+       This section contains instructions for how the SolrDispatchFilter
+       should behave when processing requests for this SolrCore.
+
+       handleSelect is a legacy option that affects the behavior of requests
+       such as /select?qt=XXX
+
+       handleSelect="true" will cause the SolrDispatchFilter to process
+       the request and dispatch the query to a handler specified by the
+       "qt" param, assuming "/select" isn't already registered.
+
+       handleSelect="false" will cause the SolrDispatchFilter to
+       ignore "/select" requests, resulting in a 404 unless a handler
+       is explicitly registered with the name "/select"
+
+       handleSelect="true" is not recommended for new users, but is the default
+       for backwards compatibility
+    -->
+  <requestDispatcher handleSelect="false" >
+    <!-- Request Parsing
+
+         These settings indicate how Solr Requests may be parsed, and
+         what restrictions may be placed on the ContentStreams from
+         those requests
+
+         enableRemoteStreaming - enables use of the stream.file
+         and stream.url parameters for specifying remote streams.
+
+         multipartUploadLimitInKB - specifies the max size (in KiB) of
+         Multipart File Uploads that Solr will allow in a Request.
+
+         formdataUploadLimitInKB - specifies the max size (in KiB) of
+         form data (application/x-www-form-urlencoded) sent via
+         POST. You can use POST to pass request parameters not
+         fitting into the URL.
+
+         addHttpRequestToContext - if set to true, it will instruct
+         the requestParsers to include the original HttpServletRequest
+         object in the context map of the SolrQueryRequest under the
+         key "httpRequest". It will not be used by any of the existing
+         Solr components, but may be useful when developing custom
+         plugins.
+
+         *** WARNING ***
+         The settings below authorize Solr to fetch remote files, You
+         should make sure your system has some authentication before
+         using enableRemoteStreaming="true"
+
+      -->
+    <requestParsers enableRemoteStreaming="true"
+                    multipartUploadLimitInKB="2048000"
+                    formdataUploadLimitInKB="2048"
+                    addHttpRequestToContext="false"/>
+
+    <!-- HTTP Caching
+
+         Set HTTP caching related parameters (for proxy caches and clients).
+
+         The options below instruct Solr not to output any HTTP Caching
+         related headers
+      -->
+    <httpCaching never304="true" />
+    <!-- If you include a <cacheControl> directive, it will be used to
+         generate a Cache-Control header (as well as an Expires header
+         if the value contains "max-age=")
+
+         By default, no Cache-Control header is generated.
+
+         You can use the <cacheControl> option even if you have set
+         never304="true"
+      -->
+    <!--
+       <httpCaching never304="true" >
+         <cacheControl>max-age=30, public</cacheControl>
+       </httpCaching>
+      -->
+    <!-- To enable Solr to respond with automatically generated HTTP
+         Caching headers, and to response to Cache Validation requests
+         correctly, set the value of never304="false"
+
+         This will cause Solr to generate Last-Modified and ETag
+         headers based on the properties of the Index.
+
+         The following options can also be specified to affect the
+         values of these headers...
+
+         lastModFrom - the default value is "openTime" which means the
+         Last-Modified value (and validation against If-Modified-Since
+         requests) will all be relative to when the current Searcher
+         was opened.  You can change it to lastModFrom="dirLastMod" if
+         you want the value to exactly correspond to when the physical
+         index was last modified.
+
+         etagSeed="..." is an option you can change to force the ETag
+         header (and validation against If-None-Match requests) to be
+         different even if the index has not changed (ie: when making
+         significant changes to your config file)
+
+         (lastModifiedFrom and etagSeed are both ignored if you use
+         the never304="true" option)
+      -->
+    <!--
+       <httpCaching lastModifiedFrom="openTime"
+                    etagSeed="Solr">
+         <cacheControl>max-age=30, public</cacheControl>
+       </httpCaching>
+      -->
+  </requestDispatcher>
+
+  <!-- Request Handlers
+
+       http://wiki.apache.org/solr/SolrRequestHandler
+
+       Incoming queries will be dispatched to a specific handler by name
+       based on the path specified in the request.
+
+       Legacy behavior: If the request path uses "/select" but no Request
+       Handler has that name, and if handleSelect="true" has been specified in
+       the requestDispatcher, then the Request Handler is dispatched based on
+       the qt parameter.  Handlers without a leading '/' are accessed this way
+       like so: http://host/app/[core/]select?qt=name  If no qt is
+       given, then the requestHandler that declares default="true" will be
+       used or the one named "standard".
+
+       If a Request Handler is declared with startup="lazy", then it will
+       not be initialized until the first request that uses it.
+
+    -->
+
+  <requestHandler name="/dataimport" class="solr.DataImportHandler">
+    <lst name="defaults">
+      <str name="config">solr-data-config.xml</str>
+    </lst>
+  </requestHandler>
+
+  <!-- SearchHandler
+
+       http://wiki.apache.org/solr/SearchHandler
+
+       For processing Search Queries, the primary Request Handler
+       provided with Solr is "SearchHandler" It delegates to a sequent
+       of SearchComponents (see below) and supports distributed
+       queries across multiple shards
+    -->
+  <requestHandler name="/select" class="solr.SearchHandler">
+    <!-- default values for query parameters can be specified, these
+         will be overridden by parameters in the request
+      -->
+     <lst name="defaults">
+       <str name="echoParams">explicit</str>
+       <int name="rows">10</int>
+       <str name="df">text</str>
+     </lst>
+    <!-- In addition to defaults, "appends" params can be specified
+         to identify values which should be appended to the list of
+         multi-val params from the query (or the existing "defaults").
+      -->
+    <!-- In this example, the param "fq=instock:true" would be appended to
+         any query time fq params the user may specify, as a mechanism for
+         partitioning the index, independent of any user selected filtering
+         that may also be desired (perhaps as a result of faceted searching).
+
+         NOTE: there is *absolutely* nothing a client can do to prevent these
+         "appends" values from being used, so don't use this mechanism
+         unless you are sure you always want it.
+      -->
+    <!--
+       <lst name="appends">
+         <str name="fq">inStock:true</str>
+       </lst>
+      -->
+    <!-- "invariants" are a way of letting the Solr maintainer lock down
+         the options available to Solr clients.  Any params values
+         specified here are used regardless of what values may be specified
+         in either the query, the "defaults", or the "appends" params.
+
+         In this example, the facet.field and facet.query params would
+         be fixed, limiting the facets clients can use.  Faceting is
+         not turned on by default - but if the client does specify
+         facet=true in the request, these are the only facets they
+         will be able to see counts for; regardless of what other
+         facet.field or facet.query params they may specify.
+
+         NOTE: there is *absolutely* nothing a client can do to prevent these
+         "invariants" values from being used, so don't use this mechanism
+         unless you are sure you always want it.
+      -->
+    <!--
+       <lst name="invariants">
+         <str name="facet.field">cat</str>
+         <str name="facet.field">manu_exact</str>
+         <str name="facet.query">price:[* TO 500]</str>
+         <str name="facet.query">price:[500 TO *]</str>
+       </lst>
+      -->
+    <!-- If the default list of SearchComponents is not desired, that
+         list can either be overridden completely, or components can be
+         prepended or appended to the default list.  (see below)
+      -->
+    <!--
+       <arr name="components">
+         <str>nameOfCustomComponent1</str>
+         <str>nameOfCustomComponent2</str>
+       </arr>
+      -->
+    </requestHandler>
+
+  <!-- A request handler that returns indented JSON by default -->
+  <requestHandler name="/query" class="solr.SearchHandler">
+     <lst name="defaults">
+       <str name="echoParams">explicit</str>
+       <str name="wt">json</str>
+       <str name="indent">true</str>
+       <str name="df">text</str>
+     </lst>
+  </requestHandler>
+
+
+  <!-- realtime get handler, guaranteed to return the latest stored fields of
+       any document, without the need to commit or open a new searcher.  The
+       current implementation relies on the updateLog feature being enabled.
+
+       ** WARNING **
+       Do NOT disable the realtime get handler at /get if you are using
+       SolrCloud otherwise any leader election will cause a full sync in ALL
+       replicas for the shard in question. Similarly, a replica recovery will
+       also always fetch the complete index from the leader because a partial
+       sync will not be possible in the absence of this handler.
+  -->
+  <requestHandler name="/get" class="solr.RealTimeGetHandler">
+     <lst name="defaults">
+       <str name="omitHeader">true</str>
+       <str name="wt">json</str>
+       <str name="indent">true</str>
+     </lst>
+  </requestHandler>
+
+
+  <!-- A Robust Example
+
+       This example SearchHandler declaration shows off usage of the
+       SearchHandler with many defaults declared
+
+       Note that multiple instances of the same Request Handler
+       (SearchHandler) can be registered multiple times with different
+       names (and different init parameters)
+    -->
+  <requestHandler name="/browse" class="solr.SearchHandler">
+    <lst name="defaults">
+      <str name="echoParams">explicit</str>
+
+      <!-- VelocityResponseWriter settings -->
+      <str name="wt">velocity</str>
+      <str name="v.template">browse</str>
+      <str name="v.layout">layout</str>
+
+      <!-- Query settings -->
+      <str name="defType">edismax</str>
+      <str name="q.alt">*:*</str>
+      <str name="rows">10</str>
+      <str name="fl">*,score</str>
+
+      <!-- Faceting defaults -->
+      <str name="facet">on</str>
+      <str name="facet.mincount">1</str>
+    </lst>
+  </requestHandler>
+
+
+  <initParams path="/update/**,/query,/select,/tvrh,/elevate,/spell,/browse">
+    <lst name="defaults">
+      <str name="df">text</str>
+      <str name="update.chain">add-unknown-fields-to-the-schema</str>
+    </lst>
+  </initParams>
+
+  <!-- Update Request Handler.
+
+       http://wiki.apache.org/solr/UpdateXmlMessages
+
+       The canonical Request Handler for Modifying the Index through
+       commands specified using XML, JSON, CSV, or JAVABIN
+
+       Note: Since solr1.1 requestHandlers requires a valid content
+       type header if posted in the body. For example, curl now
+       requires: -H 'Content-type:text/xml; charset=utf-8'
+
+       To override the request content type and force a specific
+       Content-type, use the request parameter:
+         ?update.contentType=text/csv
+
+       This handler will pick a response format to match the input
+       if the 'wt' parameter is not explicit
+    -->
+  <requestHandler name="/update" class="solr.UpdateRequestHandler">
+    <!-- See below for information on defining
+         updateRequestProcessorChains that can be used by name
+         on each Update Request
+      -->
+    <!--
+       <lst name="defaults">
+         <str name="update.chain">dedupe</str>
+       </lst>
+       -->
+  </requestHandler>
+
+  <!-- Solr Cell Update Request Handler
+
+       http://wiki.apache.org/solr/ExtractingRequestHandler
+
+    -->
+  <requestHandler name="/update/extract"
+                  startup="lazy"
+                  class="solr.extraction.ExtractingRequestHandler" >
+    <lst name="defaults">
+      <str name="lowernames">true</str>
+      <str name="uprefix">ignored_</str>
+
+      <!-- capture link hrefs but ignore div attributes -->
+      <str name="captureAttr">true</str>
+      <str name="fmap.a">links</str>
+      <str name="fmap.div">ignored_</str>
+    </lst>
+  </requestHandler>
+
+
+  <!-- Field Analysis Request Handler
+
+       RequestHandler that provides much the same functionality as
+       analysis.jsp. Provides the ability to specify multiple field
+       types and field names in the same request and outputs
+       index-time and query-time analysis for each of them.
+
+       Request parameters are:
+       analysis.fieldname - field name whose analyzers are to be used
+
+       analysis.fieldtype - field type whose analyzers are to be used
+       analysis.fieldvalue - text for index-time analysis
+       q (or analysis.q) - text for query time analysis
+       analysis.showmatch (true|false) - When set to true and when
+           query analysis is performed, the produced tokens of the
+           field value analysis will be marked as "matched" for every
+           token that is produces by the query analysis
+   -->
+  <requestHandler name="/analysis/field"
+                  startup="lazy"
+                  class="solr.FieldAnalysisRequestHandler" />
+
+
+  <!-- Document Analysis Handler
+
+       http://wiki.apache.org/solr/AnalysisRequestHandler
+
+       An analysis handler that provides a breakdown of the analysis
+       process of provided documents. This handler expects a (single)
+       content stream with the following format:
+
+       <docs>
+         <doc>
+           <field name="id">1</field>
+           <field name="name">The Name</field>
+           <field name="text">The Text Value</field>
+         </doc>
+         <doc>...</doc>
+         <doc>...</doc>
+         ...
+       </docs>
+
+    Note: Each document must contain a field which serves as the
+    unique key. This key is used in the returned response to associate
+    an analysis breakdown to the analyzed document.
+
+    Like the FieldAnalysisRequestHandler, this handler also supports
+    query analysis by sending either an "analysis.query" or "q"
+    request parameter that holds the query text to be analyzed. It
+    also supports the "analysis.showmatch" parameter which when set to
+    true, all field tokens that match the query tokens will be marked
+    as a "match".
+  -->
+  <requestHandler name="/analysis/document"
+                  class="solr.DocumentAnalysisRequestHandler"
+                  startup="lazy" />
+
+  <!-- This single handler is equivalent to the following... -->
+  <!--
+     <requestHandler name="/admin/luke"       class="solr.admin.LukeRequestHandler" />
+     <requestHandler name="/admin/system"     class="solr.admin.SystemInfoHandler" />
+     <requestHandler name="/admin/plugins"    class="solr.admin.PluginInfoHandler" />
+     <requestHandler name="/admin/threads"    class="solr.admin.ThreadDumpHandler" />
+     <requestHandler name="/admin/properties" class="solr.admin.PropertiesRequestHandler" />
+     <requestHandler name="/admin/file"       class="solr.admin.ShowFileRequestHandler" >
+    -->
+  <!-- If you wish to hide files under ${solr.home}/conf, explicitly
+       register the ShowFileRequestHandler using the definition below.
+       NOTE: The glob pattern ('*') is the only pattern supported at present, *.xml will
+             not exclude all files ending in '.xml'. Use it to exclude _all_ updates
+    -->
+  <!--
+     <requestHandler name="/admin/file"
+                     class="solr.admin.ShowFileRequestHandler" >
+       <lst name="invariants">
+         <str name="hidden">synonyms.txt</str>
+         <str name="hidden">anotherfile.txt</str>
+         <str name="hidden">*</str>
+       </lst>
+     </requestHandler>
+    -->
+
+  <!--
+    Enabling this request handler (which is NOT a default part of the admin handler) will allow the Solr UI to edit
+    all the config files. This is intended for secure/development use ONLY! Leaving available and publically
+    accessible is a security vulnerability and should be done with extreme caution!
+  -->
+  <!--
+  <requestHandler name="/admin/fileedit" class="solr.admin.EditFileRequestHandler" >
+    <lst name="invariants">
+         <str name="hidden">synonyms.txt</str>
+         <str name="hidden">anotherfile.txt</str>
+    </lst>
+  </requestHandler>
+  -->
+  <!-- ping/healthcheck -->
+  <requestHandler name="/admin/ping" class="solr.PingRequestHandler">
+    <lst name="invariants">
+      <str name="q">solrpingquery</str>
+    </lst>
+    <lst name="defaults">
+      <str name="echoParams">all</str>
+    </lst>
+    <!-- An optional feature of the PingRequestHandler is to configure the
+         handler with a "healthcheckFile" which can be used to enable/disable
+         the PingRequestHandler.
+         relative paths are resolved against the data dir
+      -->
+    <!-- <str name="healthcheckFile">server-enabled.txt</str> -->
+  </requestHandler>
+
+  <!-- Echo the request contents back to the client -->
+  <requestHandler name="/debug/dump" class="solr.DumpRequestHandler" >
+    <lst name="defaults">
+     <str name="echoParams">explicit</str>
+     <str name="echoHandler">true</str>
+    </lst>
+  </requestHandler>
+
+  <!-- Solr Replication
+
+       The SolrReplicationHandler supports replicating indexes from a
+       "master" used for indexing and "slaves" used for queries.
+
+       http://wiki.apache.org/solr/SolrReplication
+
+       It is also necessary for SolrCloud to function (in Cloud mode, the
+       replication handler is used to bulk transfer segments when nodes
+       are added or need to recover).
+
+       https://wiki.apache.org/solr/SolrCloud/
+    -->
+  <requestHandler name="/replication" class="solr.ReplicationHandler" >
+    <!--
+       To enable simple master/slave replication, uncomment one of the
+       sections below, depending on whether this solr instance should be
+       the "master" or a "slave".  If this instance is a "slave" you will
+       also need to fill in the masterUrl to point to a real machine.
+    -->
+    <!--
+       <lst name="master">
+         <str name="replicateAfter">commit</str>
+         <str name="replicateAfter">startup</str>
+         <str name="confFiles">schema.xml,stopwords.txt</str>
+       </lst>
+    -->
+    <!--
+       <lst name="slave">
+         <str name="masterUrl">http://your-master-hostname:8983/solr</str>
+         <str name="pollInterval">00:00:60</str>
+       </lst>
+    -->
+  </requestHandler>
+
+  <!-- Search Components
+
+       Search components are registered to SolrCore and used by
+       instances of SearchHandler (which can access them by name)
+
+       By default, the following components are available:
+
+       <searchComponent name="query"     class="solr.QueryComponent" />
+       <searchComponent name="facet"     class="solr.FacetComponent" />
+       <searchComponent name="mlt"       class="solr.MoreLikeThisComponent" />
+       <searchComponent name="highlight" class="solr.HighlightComponent" />
+       <searchComponent name="stats"     class="solr.StatsComponent" />
+       <searchComponent name="debug"     class="solr.DebugComponent" />
+
+       Default configuration in a requestHandler would look like:
+
+       <arr name="components">
+         <str>query</str>
+         <str>facet</str>
+         <str>mlt</str>
+         <str>highlight</str>
+         <str>stats</str>
+         <str>debug</str>
+       </arr>
+
+       If you register a searchComponent to one of the standard names,
+       that will be used instead of the default.
+
+       To insert components before or after the 'standard' components, use:
+
+       <arr name="first-components">
+         <str>myFirstComponentName</str>
+       </arr>
+
+       <arr name="last-components">
+         <str>myLastComponentName</str>
+       </arr>
+
+       NOTE: The component registered with the name "debug" will
+       always be executed after the "last-components"
+
+     -->
+
+   <!-- Spell Check
+
+        The spell check component can return a list of alternative spelling
+        suggestions.
+
+        http://wiki.apache.org/solr/SpellCheckComponent
+     -->
+  <searchComponent name="spellcheck" class="solr.SpellCheckComponent">
+
+    <str name="queryAnalyzerFieldType">text_general</str>
+
+    <!-- Multiple "Spell Checkers" can be declared and used by this
+         component
+      -->
+
+    <!-- a spellchecker built from a field of the main index -->
+    <lst name="spellchecker">
+      <str name="name">default</str>
+      <str name="field">text</str>
+      <str name="classname">solr.DirectSolrSpellChecker</str>
+      <!-- the spellcheck distance measure used, the default is the internal levenshtein -->
+      <str name="distanceMeasure">internal</str>
+      <!-- minimum accuracy needed to be considered a valid spellcheck suggestion -->
+      <float name="accuracy">0.5</float>
+      <!-- the maximum #edits we consider when enumerating terms: can be 1 or 2 -->
+      <int name="maxEdits">2</int>
+      <!-- the minimum shared prefix when enumerating terms -->
+      <int name="minPrefix">1</int>
+      <!-- maximum number of inspections per result. -->
+      <int name="maxInspections">5</int>
+      <!-- minimum length of a query term to be considered for correction -->
+      <int name="minQueryLength">4</int>
+      <!-- maximum threshold of documents a query term can appear to be considered for correction -->
+      <float name="maxQueryFrequency">0.01</float>
+      <!-- uncomment this to require suggestions to occur in 1% of the documents
+        <float name="thresholdTokenFrequency">.01</float>
+      -->
+    </lst>
+
+    <!-- a spellchecker that can break or combine words.  See "/spell" handler below for usage -->
+    <lst name="spellchecker">
+      <str name="name">wordbreak</str>
+      <str name="classname">solr.WordBreakSolrSpellChecker</str>
+      <str name="field">name</str>
+      <str name="combineWords">true</str>
+      <str name="breakWords">true</str>
+      <int name="maxChanges">10</int>
+    </lst>
+
+    <!-- a spellchecker that uses a different distance measure -->
+    <!--
+       <lst name="spellchecker">
+         <str name="name">jarowinkler</str>
+         <str name="field">spell</str>
+         <str name="classname">solr.DirectSolrSpellChecker</str>
+         <str name="distanceMeasure">
+           org.apache.lucene.search.spell.JaroWinklerDistance
+         </str>
+       </lst>
+     -->
+
+    <!-- a spellchecker that use an alternate comparator
+
+         comparatorClass be one of:
+          1. score (default)
+          2. freq (Frequency first, then score)
+          3. A fully qualified class name
+      -->
+    <!--
+       <lst name="spellchecker">
+         <str name="name">freq</str>
+         <str name="field">lowerfilt</str>
+         <str name="classname">solr.DirectSolrSpellChecker</str>
+         <str name="comparatorClass">freq</str>
+      -->
+
+    <!-- A spellchecker that reads the list of words from a file -->
+    <!--
+       <lst name="spellchecker">
+         <str name="classname">solr.FileBasedSpellChecker</str>
+         <str name="name">file</str>
+         <str name="sourceLocation">spellings.txt</str>
+         <str name="characterEncoding">UTF-8</str>
+         <str name="spellcheckIndexDir">spellcheckerFile</str>
+       </lst>
+      -->
+  </searchComponent>
+
+  <!-- A request handler for demonstrating the spellcheck component.
+
+       NOTE: This is purely as an example.  The whole purpose of the
+       SpellCheckComponent is to hook it into the request handler that
+       handles your normal user queries so that a separate request is
+       not needed to get suggestions.
+
+       IN OTHER WORDS, THERE IS REALLY GOOD CHANCE THE SETUP BELOW IS
+       NOT WHAT YOU WANT FOR YOUR PRODUCTION SYSTEM!
+
+       See http://wiki.apache.org/solr/SpellCheckComponent for details
+       on the request parameters.
+    -->
+  <requestHandler name="/spell" class="solr.SearchHandler" startup="lazy">
+    <lst name="defaults">
+      <str name="df">text</str>
+      <!-- Solr will use suggestions from both the 'default' spellchecker
+           and from the 'wordbreak' spellchecker and combine them.
+           collations (re-written queries) can include a combination of
+           corrections from both spellcheckers -->
+      <str name="spellcheck.dictionary">default</str>
+      <str name="spellcheck.dictionary">wordbreak</str>
+      <str name="spellcheck">on</str>
+      <str name="spellcheck.extendedResults">true</str>
+      <str name="spellcheck.count">10</str>
+      <str name="spellcheck.alternativeTermCount">5</str>
+      <str name="spellcheck.maxResultsForSuggest">5</str>
+      <str name="spellcheck.collate">true</str>
+      <str name="spellcheck.collateExtendedResults">true</str>
+      <str name="spellcheck.maxCollationTries">10</str>
+      <str name="spellcheck.maxCollations">5</str>
+    </lst>
+    <arr name="last-components">
+      <str>spellcheck</str>
+    </arr>
+  </requestHandler>
+
+  <searchComponent name="suggest" class="solr.SuggestComponent">
+        <lst name="suggester">
+      <str name="name">mySuggester</str>
+      <str name="lookupImpl">FuzzyLookupFactory</str>      <!-- org.apache.solr.spelling.suggest.fst -->
+      <str name="dictionaryImpl">DocumentDictionaryFactory</str>     <!-- org.apache.solr.spelling.suggest.HighFrequencyDictionaryFactory -->
+      <str name="field">cat</str>
+      <str name="weightField">price</str>
+      <str name="suggestAnalyzerFieldType">string</str>
+    </lst>
+  </searchComponent>
+
+  <requestHandler name="/suggest" class="solr.SearchHandler" startup="lazy">
+    <lst name="defaults">
+      <str name="suggest">true</str>
+      <str name="suggest.count">10</str>
+    </lst>
+    <arr name="components">
+      <str>suggest</str>
+    </arr>
+  </requestHandler>
+  <!-- Term Vector Component
+
+       http://wiki.apache.org/solr/TermVectorComponent
+    -->
+  <searchComponent name="tvComponent" class="solr.TermVectorComponent"/>
+
+  <!-- A request handler for demonstrating the term vector component
+
+       This is purely as an example.
+
+       In reality you will likely want to add the component to your
+       already specified request handlers.
+    -->
+  <requestHandler name="/tvrh" class="solr.SearchHandler" startup="lazy">
+    <lst name="defaults">
+      <str name="df">text</str>
+      <bool name="tv">true</bool>
+    </lst>
+    <arr name="last-components">
+      <str>tvComponent</str>
+    </arr>
+  </requestHandler>
+
+  <!-- Clustering Component
+
+       You'll need to set the solr.clustering.enabled system property
+       when running solr to run with clustering enabled:
+
+            java -Dsolr.clustering.enabled=true -jar start.jar
+
+       http://wiki.apache.org/solr/ClusteringComponent
+       http://carrot2.github.io/solr-integration-strategies/
+    -->
+  <searchComponent name="clustering"
+                   enable="${solr.clustering.enabled:false}"
+                   class="solr.clustering.ClusteringComponent" >
+    <lst name="engine">
+      <str name="name">lingo</str>
+
+      <!-- Class name of a clustering algorithm compatible with the Carrot2 framework.
+
+           Currently available open source algorithms are:
+           * org.carrot2.clustering.lingo.LingoClusteringAlgorithm
+           * org.carrot2.clustering.stc.STCClusteringAlgorithm
+           * org.carrot2.clustering.kmeans.BisectingKMeansClusteringAlgorithm
+
+           See http://project.carrot2.org/algorithms.html for more information.
+
+           A commercial algorithm Lingo3G (needs to be installed separately) is defined as:
+           * com.carrotsearch.lingo3g.Lingo3GClusteringAlgorithm
+        -->
+      <str name="carrot.algorithm">org.carrot2.clustering.lingo.LingoClusteringAlgorithm</str>
+
+      <!-- Override location of the clustering algorithm's resources
+           (attribute definitions and lexical resources).
+
+           A directory from which to load algorithm-specific stop words,
+           stop labels and attribute definition XMLs.
+
+           For an overview of Carrot2 lexical resources, see:
+           http://download.carrot2.org/head/manual/#chapter.lexical-resources
+
+           For an overview of Lingo3G lexical resources, see:
+           http://download.carrotsearch.com/lingo3g/manual/#chapter.lexical-resources
+       -->
+      <str name="carrot.resourcesDir">clustering/carrot2</str>
+    </lst>
+
+    <!-- An example definition for the STC clustering algorithm. -->
+    <lst name="engine">
+      <str name="name">stc</str>
+      <str name="carrot.algorithm">org.carrot2.clustering.stc.STCClusteringAlgorithm</str>
+    </lst>
+
+    <!-- An example definition for the bisecting kmeans clustering algorithm. -->
+    <lst name="engine">
+      <str name="name">kmeans</str>
+      <str name="carrot.algorithm">org.carrot2.clustering.kmeans.BisectingKMeansClusteringAlgorithm</str>
+    </lst>
+  </searchComponent>
+
+  <!-- A request handler for demonstrating the clustering component
+
+       This is purely as an example.
+
+       In reality you will likely want to add the component to your
+       already specified request handlers.
+    -->
+  <requestHandler name="/clustering"
+                  startup="lazy"
+                  enable="${solr.clustering.enabled:false}"
+                  class="solr.SearchHandler">
+    <lst name="defaults">
+      <bool name="clustering">true</bool>
+      <bool name="clustering.results">true</bool>
+      <!-- Field name with the logical "title" of a each document (optional) -->
+      <str name="carrot.title">name</str>
+      <!-- Field name with the logical "URL" of a each document (optional) -->
+      <str name="carrot.url">id</str>
+      <!-- Field name with the logical "content" of a each document (optional) -->
+      <str name="carrot.snippet">features</str>
+      <!-- Apply highlighter to the title/ content and use this for clustering. -->
+      <bool name="carrot.produceSummary">true</bool>
+      <!-- the maximum number of labels per cluster -->
+      <!--<int name="carrot.numDescriptions">5</int>-->
+      <!-- produce sub clusters -->
+      <bool name="carrot.outputSubClusters">false</bool>
+
+      <!-- Configure the remaining request handler parameters. -->
+      <str name="defType">edismax</str>
+      <str name="qf">
+        text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4
+      </str>
+      <str name="q.alt">*:*</str>
+      <str name="rows">10</str>
+      <str name="fl">*,score</str>
+    </lst>
+    <arr name="last-components">
+      <str>clustering</str>
+    </arr>
+  </requestHandler>
+
+  <!-- Terms Component
+
+       http://wiki.apache.org/solr/TermsComponent
+
+       A component to return terms and document frequency of those
+       terms
+    -->
+  <searchComponent name="terms" class="solr.TermsComponent"/>
+
+  <!-- A request handler for demonstrating the terms component -->
+  <requestHandler name="/terms" class="solr.SearchHandler" startup="lazy">
+     <lst name="defaults">
+      <bool name="terms">true</bool>
+      <bool name="distrib">false</bool>
+    </lst>
+    <arr name="components">
+      <str>terms</str>
+    </arr>
+  </requestHandler>
+
+
+  <!-- Query Elevation Component
+
+       http://wiki.apache.org/solr/QueryElevationComponent
+
+       a search component that enables you to configure the top
+       results for a given query regardless of the normal lucene
+       scoring.
+    -->
+  <searchComponent name="elevator" class="solr.QueryElevationComponent" >
+    <!-- pick a fieldType to analyze queries -->
+    <str name="queryFieldType">string</str>
+    <str name="config-file">elevate.xml</str>
+  </searchComponent>
+
+  <!-- A request handler for demonstrating the elevator component -->
+  <requestHandler name="/elevate" class="solr.SearchHandler" startup="lazy">
+    <lst name="defaults">
+      <str name="echoParams">explicit</str>
+      <str name="df">text</str>
+    </lst>
+    <arr name="last-components">
+      <str>elevator</str>
+    </arr>
+  </requestHandler>
+
+  <!-- Highlighting Component
+
+       http://wiki.apache.org/solr/HighlightingParameters
+    -->
+  <searchComponent class="solr.HighlightComponent" name="highlight">
+    <highlighting>
+      <!-- Configure the standard fragmenter -->
+      <!-- This could most likely be commented out in the "default" case -->
+      <fragmenter name="gap"
+                  default="true"
+                  class="solr.highlight.GapFragmenter">
+        <lst name="defaults">
+          <int name="hl.fragsize">100</int>
+        </lst>
+      </fragmenter>
+
+      <!-- A regular-expression-based fragmenter
+           (for sentence extraction)
+        -->
+      <fragmenter name="regex"
+                  class="solr.highlight.RegexFragmenter">
+        <lst name="defaults">
+          <!-- slightly smaller fragsizes work better because of slop -->
+          <int name="hl.fragsize">70</int>
+          <!-- allow 50% slop on fragment sizes -->
+          <float name="hl.regex.slop">0.5</float>
+          <!-- a basic sentence pattern -->
+          <str name="hl.regex.pattern">[-\w ,/\n\&quot;&apos;]{20,200}</str>
+        </lst>
+      </fragmenter>
+
+      <!-- Configure the standard formatter -->
+      <formatter name="html"
+                 default="true"
+                 class="solr.highlight.HtmlFormatter">
+        <lst name="defaults">
+          <str name="hl.simple.pre"><![CDATA[<em>]]></str>
+          <str name="hl.simple.post"><![CDATA[</em>]]></str>
+        </lst>
+      </formatter>
+
+      <!-- Configure the standard encoder -->
+      <encoder name="html"
+               class="solr.highlight.HtmlEncoder" />
+
+      <!-- Configure the standard fragListBuilder -->
+      <fragListBuilder name="simple"
+                       class="solr.highlight.SimpleFragListBuilder"/>
+
+      <!-- Configure the single fragListBuilder -->
+      <fragListBuilder name="single"
+                       class="solr.highlight.SingleFragListBuilder"/>
+
+      <!-- Configure the weighted fragListBuilder -->
+      <fragListBuilder name="weighted"
+                       default="true"
+                       class="solr.highlight.WeightedFragListBuilder"/>
+
+      <!-- default tag FragmentsBuilder -->
+      <fragmentsBuilder name="default"
+                        default="true"
+                        class="solr.highlight.ScoreOrderFragmentsBuilder">
+        <!--
+        <lst name="defaults">
+          <str name="hl.multiValuedSeparatorChar">/</str>
+        </lst>
+        -->
+      </fragmentsBuilder>
+
+      <!-- multi-colored tag FragmentsBuilder -->
+      <fragmentsBuilder name="colored"
+                        class="solr.highlight.ScoreOrderFragmentsBuilder">
+        <lst name="defaults">
+          <str name="hl.tag.pre"><![CDATA[
+               <b style="background:yellow">,<b style="background:lawgreen">,
+               <b style="background:aquamarine">,<b style="background:magenta">,
+               <b style="background:palegreen">,<b style="background:coral">,
+               <b style="background:wheat">,<b style="background:khaki">,
+               <b style="background:lime">,<b style="background:deepskyblue">]]></str>
+          <str name="hl.tag.post"><![CDATA[</b>]]></str>
+        </lst>
+      </fragmentsBuilder>
+
+      <boundaryScanner name="default"
+                       default="true"
+                       class="solr.highlight.SimpleBoundaryScanner">
+        <lst name="defaults">
+          <str name="hl.bs.maxScan">10</str>
+          <str name="hl.bs.chars">.,!? &#9;&#10;&#13;</str>
+        </lst>
+      </boundaryScanner>
+
+      <boundaryScanner name="breakIterator"
+                       class="solr.highlight.BreakIteratorBoundaryScanner">
+        <lst name="defaults">
+          <!-- type should be one of CHARACTER, WORD(default), LINE and SENTENCE -->
+          <str name="hl.bs.type">WORD</str>
+          <!-- language and country are used when constructing Locale object.  -->
+          <!-- And the Locale object will be used when getting instance of BreakIterator -->
+          <str name="hl.bs.language">en</str>
+          <str name="hl.bs.country">US</str>
+        </lst>
+      </boundaryScanner>
+    </highlighting>
+  </searchComponent>
+
+  <!-- Update Processors
+
+       Chains of Update Processor Factories for dealing with Update
+       Requests can be declared, and then used by name in Update
+       Request Processors
+
+       http://wiki.apache.org/solr/UpdateRequestProcessor
+
+    -->
+
+  <!-- Add unknown fields to the schema
+
+       An example field type guessing update processor that will
+       attempt to parse string-typed field values as Booleans, Longs,
+       Doubles, or Dates, and then add schema fields with the guessed
+       field types.
+
+       This requires that the schema is both managed and mutable, by
+       declaring schemaFactory as ManagedIndexSchemaFactory, with
+       mutable specified as true.
+
+       See http://wiki.apache.org/solr/GuessingFieldTypes
+    -->
+  <updateRequestProcessorChain name="add-unknown-fields-to-the-schema">
+    <processor class="solr.DefaultValueUpdateProcessorFactory">
+        <str name="fieldName">_ttl_</str>
+        <str name="value">+{{ranger_audit_max_retention_days}}DAYS</str>
+    </processor>
+    <processor class="solr.processor.DocExpirationUpdateProcessorFactory">
+        <int name="autoDeletePeriodSeconds">86400</int>
+        <str name="ttlFieldName">_ttl_</str>
+        <str name="expirationFieldName">_expire_at_</str>
+    </processor>
+    <processor class="solr.FirstFieldValueUpdateProcessorFactory">
+      <str name="fieldName">_expire_at_</str>
+    </processor>
+
+    <processor class="solr.RemoveBlankFieldUpdateProcessorFactory"/>
+    <processor class="solr.ParseBooleanFieldUpdateProcessorFactory"/>
+    <processor class="solr.ParseLongFieldUpdateProcessorFactory"/>
+    <processor class="solr.ParseDoubleFieldUpdateProcessorFactory"/>
+    <processor class="solr.ParseDateFieldUpdateProcessorFactory">
+      <arr name="format">
+        <str>yyyy-MM-dd'T'HH:mm:ss.SSSZ</str>
+        <str>yyyy-MM-dd'T'HH:mm:ss,SSSZ</str>
+        <str>yyyy-MM-dd'T'HH:mm:ss.SSS</str>
+        <str>yyyy-MM-dd'T'HH:mm:ss,SSS</str>
+        <str>yyyy-MM-dd'T'HH:mm:ssZ</str>
+        <str>yyyy-MM-dd'T'HH:mm:ss</str>
+        <str>yyyy-MM-dd'T'HH:mmZ</str>
+        <str>yyyy-MM-dd'T'HH:mm</str>
+        <str>yyyy-MM-dd HH:mm:ss.SSSZ</str>
+        <str>yyyy-MM-dd HH:mm:ss,SSSZ</str>
+        <str>yyyy-MM-dd HH:mm:ss.SSS</str>
+        <str>yyyy-MM-dd HH:mm:ss,SSS</str>
+        <str>yyyy-MM-dd HH:mm:ssZ</str>
+        <str>yyyy-MM-dd HH:mm:ss</str>
+        <str>yyyy-MM-dd HH:mmZ</str>
+        <str>yyyy-MM-dd HH:mm</str>
+        <str>yyyy-MM-dd</str>
+      </arr>
+    </processor>
+    <processor class="solr.AddSchemaFieldsUpdateProcessorFactory">
+      <str name="defaultFieldType">key_lower_case</str>
+      <lst name="typeMapping">
+        <str name="valueClass">java.lang.Boolean</str>
+        <str name="fieldType">boolean</str>
+      </lst>
+      <lst name="typeMapping">
+        <str name="valueClass">java.util.Date</str>
+        <str name="fieldType">tdate</str>
+      </lst>
+      <lst name="typeMapping">
+        <str name="valueClass">java.lang.Long</str>
+        <str name="valueClass">java.lang.Integer</str>
+        <str name="fieldType">tlong</str>
+      </lst>
+      <lst name="typeMapping">
+        <str name="valueClass">java.lang.Number</str>
+        <str name="fieldType">tdouble</str>
+      </lst>
+    </processor>
+    <processor class="solr.LogUpdateProcessorFactory"/>
+    <processor class="solr.RunUpdateProcessorFactory"/>
+  </updateRequestProcessorChain>
+
+
+  <!-- Deduplication
+
+       An example dedup update processor that creates the "id" field
+       on the fly based on the hash code of some other fields.  This
+       example has overwriteDupes set to false since we are using the
+       id field as the signatureField and Solr will maintain
+       uniqueness based on that anyway.
+
+    -->
+  <!--
+     <updateRequestProcessorChain name="dedupe">
+       <processor class="solr.processor.SignatureUpdateProcessorFactory">
+         <bool name="enabled">true</bool>
+         <str name="signatureField">id</str>
+         <bool name="overwriteDupes">false</bool>
+         <str name="fields">name,features,cat</str>
+         <str name="signatureClass">solr.processor.Lookup3Signature</str>
+       </processor>
+       <processor class="solr.LogUpdateProcessorFactory" />
+       <processor class="solr.RunUpdateProcessorFactory" />
+     </updateRequestProcessorChain>
+    -->
+
+  <!-- Language identification
+
+       This example update chain identifies the language of the incoming
+       documents using the langid contrib. The detected language is
+       written to field language_s. No field name mapping is done.
+       The fields used for detection are text, title, subject and description,
+       making this example suitable for detecting languages form full-text
+       rich documents injected via ExtractingRequestHandler.
+       See more about langId at http://wiki.apache.org/solr/LanguageDetection
+    -->
+    <!--
+     <updateRequestProcessorChain name="langid">
+       <processor class="org.apache.solr.update.processor.TikaLanguageIdentifierUpdateProcessorFactory">
+         <str name="langid.fl">text,title,subject,description</str>
+         <str name="langid.langField">language_s</str>
+         <str name="langid.fallback">en</str>
+       </processor>
+       <processor class="solr.LogUpdateProcessorFactory" />
+       <processor class="solr.RunUpdateProcessorFactory" />
+     </updateRequestProcessorChain>
+    -->
+
+  <!-- Script update processor
+
+    This example hooks in an update processor implemented using JavaScript.
+
+    See more about the script update processor at http://wiki.apache.org/solr/ScriptUpdateProcessor
+  -->
+  <!--
+    <updateRequestProcessorChain name="script">
+      <processor class="solr.StatelessScriptUpdateProcessorFactory">
+        <str name="script">update-script.js</str>
+        <lst name="params">
+          <str name="config_param">example config parameter</str>
+        </lst>
+      </processor>
+      <processor class="solr.RunUpdateProcessorFactory" />
+    </updateRequestProcessorChain>
+  -->
+
+  <!-- Response Writers
+
+       http://wiki.apache.org/solr/QueryResponseWriter
+
+       Request responses will be written using the writer specified by
+       the 'wt' request parameter matching the name of a registered
+       writer.
+
+       The "default" writer is the default and will be used if 'wt' is
+       not specified in the request.
+    -->
+  <!-- The following response writers are implicitly configured unless
+       overridden...
+    -->
+  <!--
+     <queryResponseWriter name="xml"
+                          default="true"
+                          class="solr.XMLResponseWriter" />
+     <queryResponseWriter name="json" class="solr.JSONResponseWriter"/>
+     <queryResponseWriter name="python" class="solr.PythonResponseWriter"/>
+     <queryResponseWriter name="ruby" class="solr.RubyResponseWriter"/>
+     <queryResponseWriter name="php" class="solr.PHPResponseWriter"/>
+     <queryResponseWriter name="phps" class="solr.PHPSerializedResponseWriter"/>
+     <queryResponseWriter name="csv" class="solr.CSVResponseWriter"/>
+     <queryResponseWriter name="schema.xml" class="solr.SchemaXmlResponseWriter"/>
+    -->
+
+  <queryResponseWriter name="json" class="solr.JSONResponseWriter">
+     <!-- For the purposes of the tutorial, JSON responses are written as
+      plain text so that they are easy to read in *any* browser.
+      If you expect a MIME type of "application/json" just remove this override.
+     -->
+    <str name="content-type">text/plain; charset=UTF-8</str>
+  </queryResponseWriter>
+
+  <!--
+     Custom response writers can be declared as needed...
+    -->
+  <queryResponseWriter name="velocity" class="solr.VelocityResponseWriter" startup="lazy">
+    <str name="template.base.dir">${velocity.template.base.dir:}</str>
+  </queryResponseWriter>
+
+  <!-- XSLT response writer transforms the XML output by any xslt file found
+       in Solr's conf/xslt directory.  Changes to xslt files are checked for
+       every xsltCacheLifetimeSeconds.
+    -->
+  <queryResponseWriter name="xslt" class="solr.XSLTResponseWriter">
+    <int name="xsltCacheLifetimeSeconds">5</int>
+  </queryResponseWriter>
+
+  <!-- Query Parsers
+
+       http://wiki.apache.org/solr/SolrQuerySyntax
+
+       Multiple QParserPlugins can be registered by name, and then
+       used in either the "defType" param for the QueryComponent (used
+       by SearchHandler) or in LocalParams
+    -->
+  <!-- example of registering a query parser -->
+  <!--
+     <queryParser name="myparser" class="com.mycompany.MyQParserPlugin"/>
+    -->
+
+  <!-- Function Parsers
+
+       http://wiki.apache.org/solr/FunctionQuery
+
+       Multiple ValueSourceParsers can be registered by name, and then
+       used as function names when using the "func" QParser.
+    -->
+  <!-- example of registering a custom function parser  -->
+  <!--
+     <valueSourceParser name="myfunc"
+                        class="com.mycompany.MyValueSourceParser" />
+    -->
+
+
+  <!-- Document Transformers
+       http://wiki.apache.org/solr/DocTransformers
+    -->
+  <!--
+     Could be something like:
+     <transformer name="db" class="com.mycompany.LoadFromDatabaseTransformer" >
+       <int name="connection">jdbc://....</int>
+     </transformer>
+
+     To add a constant value to all docs, use:
+     <transformer name="mytrans2" class="org.apache.solr.response.transform.ValueAugmenterFactory" >
+       <int name="value">5</int>
+     </transformer>
+
+     If you want the user to still be able to change it with _value:something_ use this:
+     <transformer name="mytrans3" class="org.apache.solr.response.transform.ValueAugmenterFactory" >
+       <double name="defaultValue">5</double>
+     </transformer>
+
+      If you are using the QueryElevationComponent, you may wish to mark documents that get boosted.  The
+      EditorialMarkerFactory will do exactly that:
+     <transformer name="qecBooster" class="org.apache.solr.response.transform.EditorialMarkerFactory" />
+    -->
+
+
+  <!-- Legacy config for the admin interface -->
+  <admin>
+    <defaultQuery>*:*</defaultQuery>
+  </admin>
+
+</config>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/params.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/params.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/params.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/params.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/params.py	(date 1719626239000)
@@ -18,6 +18,7 @@
 
 """
 import os
+from resource_management.libraries.functions import format
 from resource_management.libraries.script.script import Script
 from resource_management.libraries.functions.version import format_stack_version
 from resource_management.libraries.functions import StackFeature
@@ -25,6 +26,8 @@
 from resource_management.libraries.functions.stack_features import get_stack_feature_version
 from resource_management.libraries.functions.default import default
 from utils import get_bare_principal
+from resource_management.libraries.functions.get_stack_version import get_stack_version
+from resource_management.libraries.functions.is_empty import is_empty
 import status_params
 from resource_management.libraries.resources.hdfs_resource import HdfsResource
 from resource_management.libraries.functions import stack_select
@@ -32,7 +35,7 @@
 from resource_management.libraries.functions import upgrade_summary
 from resource_management.libraries.functions import get_kinit_path
 from resource_management.libraries.functions.get_not_managed_resources import get_not_managed_resources
-from resource_management.libraries.functions.format import format
+from resource_management.libraries.functions.setup_ranger_plugin_xml import get_audit_configs, generate_ranger_service_config
 
 # server configurations
 config = Script.get_config()
@@ -51,6 +54,11 @@
 # get the correct version to use for checking stack features
 version_for_stack_feature_checks = get_stack_feature_version(config)
 
+stack_supports_ranger_kerberos = check_stack_feature(StackFeature.RANGER_KERBEROS_SUPPORT, version_for_stack_feature_checks)
+stack_supports_ranger_audit_db = check_stack_feature(StackFeature.RANGER_AUDIT_DB_SUPPORT, version_for_stack_feature_checks)
+stack_supports_core_site_for_ranger_plugin = check_stack_feature(StackFeature.CORE_SITE_FOR_RANGER_PLUGINS_SUPPORT, version_for_stack_feature_checks)
+stack_supports_kafka_env_include_ranger_script = check_stack_feature(StackFeature.KAFKA_ENV_INCLUDE_RANGER_SCRIPT, version_for_stack_feature_checks)
+
 # When downgrading the 'version' is pointing to the downgrade-target version
 # downgrade_from_version provides the source-version the downgrade is happening from
 downgrade_from_version = upgrade_summary.get_downgrade_from_version("KAFKA")
@@ -59,6 +67,7 @@
 
 # default kafka parameters
 kafka_home = '/usr/lib/kafka'
+kafka_bin = kafka_home+'/bin/kafka'
 conf_dir = "/etc/kafka/conf"
 limits_conf_dir = "/etc/security/limits.d"
 
@@ -73,7 +82,8 @@
 # parameters for 2.2+
 if stack_version_formatted and check_stack_feature(StackFeature.ROLLING_UPGRADE, stack_version_formatted):
   kafka_home = os.path.join(stack_root,  "current", "kafka-broker")
-
+  kafka_bin = os.path.join(kafka_home, "bin", "kafka")
+  conf_dir = os.path.join(kafka_home, "config")
 kafka_start_cmd = format('{kafka_home}/bin/kafka-server-start.sh {conf_dir}/server.properties')
 kafka_stop_cmd = format('{kafka_home}/bin/kafka-server-stop.sh {conf_dir}/server.properties')
 
@@ -194,6 +204,188 @@
     kafka_jaas_principal = None
     kafka_keytab_path = None
 
+# for curl command in ranger plugin to get db connector
+jdk_location = config['ambariLevelParams']['jdk_location']
+
+# ranger kafka plugin section start
+
+# ranger host
+ranger_admin_hosts = default("/clusterHostInfo/ranger_admin_hosts", [])
+has_ranger_admin = not len(ranger_admin_hosts) == 0
+
+# ranger support xml_configuration flag, instead of depending on ranger xml_configurations_supported/ranger-env, using stack feature
+xml_configurations_supported = check_stack_feature(StackFeature.RANGER_XML_CONFIGURATION, version_for_stack_feature_checks)
+
+# ranger kafka plugin enabled property
+enable_ranger_kafka = default("configurations/ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled", "No")
+enable_ranger_kafka = True if enable_ranger_kafka.lower() == 'yes' else False
+
+# ranger kafka-plugin supported flag, instead of dependending on is_supported_kafka_ranger/kafka-env.xml, using stack feature
+is_supported_kafka_ranger = check_stack_feature(StackFeature.KAFKA_RANGER_PLUGIN_SUPPORT, version_for_stack_feature_checks)
+
+# ranger kafka properties
+if enable_ranger_kafka and is_supported_kafka_ranger:
+  # get ranger policy url
+  policymgr_mgr_url = config['configurations']['ranger-kafka-security']['ranger.plugin.kafka.policy.rest.url']
+
+  if not is_empty(policymgr_mgr_url) and policymgr_mgr_url.endswith('/'):
+    policymgr_mgr_url = policymgr_mgr_url.rstrip('/')
+
+  # ranger audit db user
+  xa_audit_db_user = default('/configurations/admin-properties/audit_db_user', 'rangerlogger')
+
+  xa_audit_db_password = ''
+  if not is_empty(config['configurations']['admin-properties']['audit_db_password']) and stack_supports_ranger_audit_db and has_ranger_admin:
+    xa_audit_db_password = config['configurations']['admin-properties']['audit_db_password']
+
+  # ranger kafka service/repository name
+  repo_name = str(config['clusterName']) + '_kafka'
+  repo_name_value = config['configurations']['ranger-kafka-security']['ranger.plugin.kafka.service.name']
+  if not is_empty(repo_name_value) and repo_name_value != "{{repo_name}}":
+    repo_name = repo_name_value
+
+  ranger_env = config['configurations']['ranger-env']
+
+  # create ranger-env config having external ranger credential properties
+  if not has_ranger_admin and enable_ranger_kafka:
+    external_admin_username = default('/configurations/ranger-kafka-plugin-properties/external_admin_username', 'admin')
+    external_admin_password = default('/configurations/ranger-kafka-plugin-properties/external_admin_password', 'admin')
+    external_ranger_admin_username = default('/configurations/ranger-kafka-plugin-properties/external_ranger_admin_username', 'amb_ranger_admin')
+    external_ranger_admin_password = default('/configurations/ranger-kafka-plugin-properties/external_ranger_admin_password', 'amb_ranger_admin')
+    ranger_env = {}
+    ranger_env['admin_username'] = external_admin_username
+    ranger_env['admin_password'] = external_admin_password
+    ranger_env['ranger_admin_username'] = external_ranger_admin_username
+    ranger_env['ranger_admin_password'] = external_ranger_admin_password
+
+  ranger_plugin_properties = config['configurations']['ranger-kafka-plugin-properties']
+  ranger_kafka_audit = config['configurations']['ranger-kafka-audit']
+  ranger_kafka_audit_attrs = config['configurationAttributes']['ranger-kafka-audit']
+  ranger_kafka_security = config['configurations']['ranger-kafka-security']
+  ranger_kafka_security_attrs = config['configurationAttributes']['ranger-kafka-security']
+  ranger_kafka_policymgr_ssl = config['configurations']['ranger-kafka-policymgr-ssl']
+  ranger_kafka_policymgr_ssl_attrs = config['configurationAttributes']['ranger-kafka-policymgr-ssl']
+
+  policy_user = config['configurations']['ranger-kafka-plugin-properties']['policy_user']
+
+  ranger_plugin_config = {
+    'username' : config['configurations']['ranger-kafka-plugin-properties']['REPOSITORY_CONFIG_USERNAME'],
+    'password' : config['configurations']['ranger-kafka-plugin-properties']['REPOSITORY_CONFIG_PASSWORD'],
+    'zookeeper.connect' : config['configurations']['ranger-kafka-plugin-properties']['zookeeper.connect'],
+    'commonNameForCertificate' : config['configurations']['ranger-kafka-plugin-properties']['common.name.for.certificate']
+  }
+
+  atlas_server_hosts = default('/clusterHostInfo/atlas_server_hosts', [])
+  has_atlas_server = not len(atlas_server_hosts) == 0
+  hive_server_hosts = default('/clusterHostInfo/hive_server_hosts', [])
+  has_hive_server = not len(hive_server_hosts) == 0
+  hbase_master_hosts = default('/clusterHostInfo/hbase_master_hosts', [])
+  has_hbase_master = not len(hbase_master_hosts) == 0
+  ranger_tagsync_hosts = default('/clusterHostInfo/ranger_tagsync_hosts', [])
+  has_ranger_tagsync = not len(ranger_tagsync_hosts) == 0
+  storm_nimbus_hosts = default('/clusterHostInfo/nimbus_hosts', [])
+  has_storm_nimbus = not len(storm_nimbus_hosts) == 0
+  spark_jobhistoryserver_hosts = default("/clusterHostInfo/spark2_jobhistoryserver_hosts", [])
+  has_jobhistoryserver = not len(spark_jobhistoryserver_hosts) == 0
+
+  if has_atlas_server:
+    atlas_notification_topics = default('/configurations/application-properties/atlas.notification.topics', 'ATLAS_HOOK,ATLAS_ENTITIES')
+    atlas_notification_topics_list = atlas_notification_topics.split(',')
+    hive_user = default('/configurations/hive-env/hive_user', 'hive')
+    hbase_user = default('/configurations/hbase-env/hbase_user', 'hbase')
+    atlas_user = default('/configurations/atlas-env/metadata_user', 'atlas')
+    rangertagsync_user = default('/configurations/ranger-tagsync-site/ranger.tagsync.dest.ranger.username', 'rangertagsync')
+    spark_user = 'spark_atlas'
+    if len(atlas_notification_topics_list) == 2:
+      atlas_hook = atlas_notification_topics_list[0]
+      atlas_entity = atlas_notification_topics_list[1]
+      ranger_plugin_config['setup.additional.default.policies'] = 'true'
+      ranger_plugin_config['default-policy.1.name'] = atlas_hook
+      ranger_plugin_config['default-policy.1.resource.topic'] = atlas_hook
+      hook_policy_user = []
+      if has_hive_server:
+        hook_policy_user.append(hive_user)
+      if has_hbase_master:
+        hook_policy_user.append(hbase_user)
+      if has_storm_nimbus and kerberos_security_enabled:
+        storm_principal_name = config['configurations']['storm-env']['storm_principal_name']
+        storm_bare_principal_name = get_bare_principal(storm_principal_name)
+        hook_policy_user.append(storm_bare_principal_name)
+      if has_jobhistoryserver:
+        hook_policy_user.append(spark_user)
+      if len(hook_policy_user) > 0:
+        ranger_plugin_config['default-policy.1.policyItem.1.users'] = ",".join(hook_policy_user)
+        ranger_plugin_config['default-policy.1.policyItem.1.accessTypes'] = "publish"
+      ranger_plugin_config['default-policy.1.policyItem.2.users'] = atlas_user
+      ranger_plugin_config['default-policy.1.policyItem.2.accessTypes'] = "consume"
+      ranger_plugin_config['default-policy.2.name'] = atlas_entity
+      ranger_plugin_config['default-policy.2.resource.topic'] = atlas_entity
+      ranger_plugin_config['default-policy.2.policyItem.1.users'] = atlas_user
+      ranger_plugin_config['default-policy.2.policyItem.1.accessTypes'] = "publish"
+      if has_ranger_tagsync:
+        ranger_plugin_config['default-policy.2.policyItem.2.users'] = rangertagsync_user
+        ranger_plugin_config['default-policy.2.policyItem.2.accessTypes'] = "consume"
+
+  if kerberos_security_enabled:
+    ranger_plugin_config['policy.download.auth.users'] = kafka_user
+    ranger_plugin_config['tag.download.auth.users'] = kafka_user
+
+  custom_ranger_service_config = generate_ranger_service_config(ranger_plugin_properties)
+  if len(custom_ranger_service_config) > 0:
+    ranger_plugin_config.update(custom_ranger_service_config)
+
+  kafka_ranger_plugin_repo = {
+    'isEnabled': 'true',
+    'configs': ranger_plugin_config,
+    'description': 'kafka repo',
+    'name': repo_name,
+    'repositoryType': 'kafka',
+    'type': 'kafka',
+    'assetType': '1'
+  }
+
+  downloaded_custom_connector = None
+  previous_jdbc_jar_name = None
+  driver_curl_source = None
+  driver_curl_target = None
+  previous_jdbc_jar = None
+
+  if has_ranger_admin and stack_supports_ranger_audit_db:
+    xa_audit_db_flavor = config['configurations']['admin-properties']['DB_FLAVOR']
+    jdbc_jar_name, previous_jdbc_jar_name, audit_jdbc_url, jdbc_driver = get_audit_configs(config)
+
+    downloaded_custom_connector = format("{tmp_dir}/{jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+    driver_curl_source = format("{jdk_location}/{jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+    driver_curl_target = format("{kafka_home}/libs/{jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+    previous_jdbc_jar = format("{kafka_home}/libs/{previous_jdbc_jar_name}") if stack_supports_ranger_audit_db else None
+
+  xa_audit_db_is_enabled = False
+  if xml_configurations_supported and stack_supports_ranger_audit_db:
+    xa_audit_db_is_enabled = config['configurations']['ranger-kafka-audit']['xasecure.audit.destination.db']
+
+  xa_audit_hdfs_is_enabled = default('/configurations/ranger-kafka-audit/xasecure.audit.destination.hdfs', False)
+  ssl_keystore_password = config['configurations']['ranger-kafka-policymgr-ssl']['xasecure.policymgr.clientssl.keystore.password'] if xml_configurations_supported else None
+  ssl_truststore_password = config['configurations']['ranger-kafka-policymgr-ssl']['xasecure.policymgr.clientssl.truststore.password'] if xml_configurations_supported else None
+  credential_file = format('/etc/ranger/{repo_name}/cred.jceks')
+
+  stack_version = get_stack_version('kafka-broker')
+  setup_ranger_env_sh_source = format('{stack_root}/{stack_version}/ranger-kafka-plugin/install/conf.templates/enable/kafka-ranger-env.sh')
+  setup_ranger_env_sh_target = format("{conf_dir}/kafka-ranger-env.sh")
+
+  # for SQLA explicitly disable audit to DB for Ranger
+  if has_ranger_admin and stack_supports_ranger_audit_db and xa_audit_db_flavor.lower() == 'sqla':
+    xa_audit_db_is_enabled = False
+
+# need this to capture cluster name from where ranger kafka plugin is enabled
+cluster_name = config['clusterName']
+
+# required when Ranger-KMS is SSL enabled
+ranger_kms_hosts = default('/clusterHostInfo/ranger_kms_server_hosts',[])
+has_ranger_kms = len(ranger_kms_hosts) > 0
+is_ranger_kms_ssl_enabled = default('configurations/ranger-kms-site/ranger.service.https.attrib.ssl.enabled',False)
+
+# ranger kafka plugin section end
+
 namenode_hosts = default("/clusterHostInfo/namenode_hosts", [])
 has_namenode = not len(namenode_hosts) == 0
 
@@ -206,6 +398,9 @@
 hadoop_conf_dir = conf_select.get_hadoop_conf_dir() if has_namenode else None
 kinit_path_local = get_kinit_path(default('/configurations/kerberos-env/executable_search_paths', None))
 dfs_type = default("/clusterLevelParams/dfs_type", "")
+ranger_kafka_plugin_impl_path = format("{kafka_home}/libs/ranger-kafka-plugin-impl")
+ranger_kafka_plugin_core_site_path = format("{ranger_kafka_plugin_impl_path}/core-site.xml")
+ranger_kafka_plugin_hdfs_site_path = format("{ranger_kafka_plugin_impl_path}/hdfs-site.xml")
 mount_table_xml_inclusion_file_full_path = None
 mount_table_content = None
 if 'viewfs-mount-table' in config['configurations']:
@@ -221,7 +416,7 @@
 #to create/delete hdfs directory/file/copyfromlocal we need to call params.HdfsResource in code
 HdfsResource = functools.partial(
   HdfsResource,
-  user = hdfs_user,
+  user=hdfs_user,
   hdfs_resource_ignore_file = "/var/lib/ambari-agent/data/.hdfs_resource_ignore",
   security_enabled = kerberos_security_enabled,
   keytab = hdfs_user_keytab,
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/quicklinks/quicklinks.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/quicklinks/quicklinks.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/quicklinks/quicklinks.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/quicklinks/quicklinks.json	(date 1719626239000)
@@ -0,0 +1,41 @@
+{
+  "name": "default",
+  "description": "default quick links configuration",
+  "configuration": {
+    "protocol":
+    {
+      "type":"https",
+      "checks":[
+        {
+          "property":"ranger.service.https.attrib.ssl.enabled",
+          "desired":"true",
+          "site":"ranger-admin-site"
+        },
+        {
+          "property":"ranger.service.http.enabled",
+          "desired":"false",
+          "site":"ranger-admin-site"
+        }
+      ]
+    },
+
+    "links": [
+      {
+        "name": "ranger_admin_ui",
+        "label": "Ranger Admin UI",
+        "component_name" : "RANGER_ADMIN",
+        "requires_user_name": "false",
+        "url": "%@://%@:%@",
+        "attributes": ["authenticated", "sso"],
+        "port":{
+          "http_property": "ranger.service.http.port",
+          "http_default_port": "6080",
+          "https_property": "ranger.service.https.port",
+          "https_default_port": "6182",
+          "regex": "(\\d*)+",
+          "site": "ranger-admin-site"
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/alerts.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/alerts.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/alerts.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/alerts.json	(date 1719626239000)
@@ -0,0 +1,76 @@
+{
+  "RANGER": {
+    "service": [],
+    "RANGER_ADMIN": [
+      {
+        "name": "ranger_admin_process",
+        "label": "Ranger Admin Process",
+        "description": "This host-level alert is triggered if the Ranger Admin Web UI is unreachable.",
+        "interval": 1,
+        "scope": "ANY",
+        "source": {
+          "type": "WEB",
+          "uri": {
+              "http": "{{admin-properties/policymgr_external_url}}/login.jsp",
+              "https": "{{admin-properties/policymgr_external_url}}/login.jsp",
+              "kerberos_keytab": "{{cluster-env/smokeuser_keytab}}",
+              "kerberos_principal": "{{cluster-env/smokeuser_principal_name}}",
+              "https_property": "{{ranger-admin-site/ranger.service.https.attrib.ssl.enabled}}",
+              "https_property_value": "true",
+              "connection_timeout": 5.0
+            },
+          "reporting": {
+            "ok": {
+              "text": "HTTP {0} response in {2:.3f}s"
+            },
+            "warning": {
+              "text": "HTTP {0} response from {1} in {2:.3f}s ({3})"
+            },
+            "critical": {
+              "text": "Connection failed to {1} ({3})"
+            }
+          }
+        }
+      },
+      {
+        "name": "ranger_admin_password_check",
+        "label": "Ranger Admin password check",
+        "description": "This alert is used to ensure that the Ranger Admin password in Ambari is correct.",
+        "interval": 30,
+        "scope": "ANY",
+        "source": {
+          "type": "SCRIPT",
+          "path": "HDP/3.0/services/RANGER/package/alerts/alert_ranger_admin_passwd_check.py",
+          "parameters": []
+        }
+      }
+    ],
+    "RANGER_USERSYNC": [
+      {
+        "name": "ranger_usersync_process",
+        "label": "Ranger Usersync Process",
+        "description": "This host-level alert is triggered if the Ranger Usersync cannot be determined to be up.",
+        "interval": 1,
+        "scope": "HOST",
+        "source": {
+          "type": "PORT",
+          "uri": "{{ranger-ugsync-site/ranger.usersync.port}}",
+          "default_port": 5151,
+          "reporting": {
+            "ok": {
+              "text": "TCP OK - {0:.3f}s response on port {1}"
+            },
+            "warning": {
+              "text": "TCP OK - {0:.3f}s response on port {1}",
+              "value": 1.5
+            },
+            "critical": {
+              "text": "Connection failed: {0} to {1}:{2}",
+              "value": 5.0
+            }
+          }
+        }
+      }
+    ]
+  }
+}
\ No newline at end of file
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka_broker.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka_broker.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka_broker.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka_broker.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/kafka_broker.py	(date 1719626239000)
@@ -16,13 +16,13 @@
 limitations under the License.
 
 """
-from resource_management import Script
+from resource_management import Script, ComponentIsNotRunning, Fail
 from resource_management.core import sudo
 from resource_management.core.logger import Logger
-from resource_management.core.resources.system import Execute, File
-from resource_management.core.exceptions import ComponentIsNotRunning, Fail
+from resource_management.core.resources.system import Execute, File, Directory
 from resource_management.libraries.functions import stack_select
 from resource_management.libraries.functions import Direction
+from resource_management.libraries.functions.version import format_stack_version
 from resource_management.libraries.functions.format import format
 from resource_management.libraries.functions.check_process_status import check_process_status
 from resource_management.libraries.functions import StackFeature
@@ -33,132 +33,141 @@
 
 import upgrade
 from kafka import kafka
+from setup_ranger_kafka import setup_ranger_kafka
+
 
 class KafkaBroker(Script):
 
-  def install(self, env):
-    self.install_packages(env)
+    def install(self, env):
+        self.install_packages(env)
 
-  def configure(self, env, upgrade_type=None):
-    import params
-    env.set_params(params)
-    kafka(upgrade_type=upgrade_type)
+    def configure(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        kafka(upgrade_type=upgrade_type)
 
-  def pre_upgrade_restart(self, env, upgrade_type=None):
-    import params
-    env.set_params(params)
+    def pre_upgrade_restart(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
 
-    if params.version and check_stack_feature(StackFeature.ROLLING_UPGRADE, params.version):
-      stack_select.select_packages(params.version)
+        if params.version and check_stack_feature(StackFeature.ROLLING_UPGRADE, params.version):
+            stack_select.select_packages(params.version)
 
-    # This is extremely important since it should only be called if crossing the HDP 2.3.4.0 boundary.
-    if params.version and params.upgrade_direction:
-      src_version = dst_version = None
-      if params.upgrade_direction == Direction.UPGRADE:
-        src_version = upgrade_summary.get_source_version("KAFKA", default_version =  params.version)
-        dst_version = upgrade_summary.get_target_version("KAFKA", default_version =  params.version)
-      else:
-        # These represent the original values during the UPGRADE direction
-        src_version = upgrade_summary.get_target_version("KAFKA", default_version =  params.version)
-        dst_version = upgrade_summary.get_source_version("KAFKA", default_version =  params.version)
+        # This is extremely important since it should only be called if crossing the HDP 2.3.4.0 boundary.
+        if params.version and params.upgrade_direction:
+            src_version = dst_version = None
+            if params.upgrade_direction == Direction.UPGRADE:
+                src_version = upgrade_summary.get_source_version("KAFKA", default_version=params.version)
+                dst_version = upgrade_summary.get_target_version("KAFKA", default_version=params.version)
+            else:
+                # These represent the original values during the UPGRADE direction
+                src_version = upgrade_summary.get_target_version("KAFKA", default_version=params.version)
+                dst_version = upgrade_summary.get_source_version("KAFKA", default_version=params.version)
 
-      if not check_stack_feature(StackFeature.KAFKA_ACL_MIGRATION_SUPPORT, src_version) and check_stack_feature(StackFeature.KAFKA_ACL_MIGRATION_SUPPORT, dst_version):
-        # Calling the acl migration script requires the configs to be present.
-        self.configure(env, upgrade_type=upgrade_type)
-        upgrade.run_migration(env, upgrade_type)
+            if not check_stack_feature(StackFeature.KAFKA_ACL_MIGRATION_SUPPORT, src_version) and check_stack_feature(
+                    StackFeature.KAFKA_ACL_MIGRATION_SUPPORT, dst_version):
+                # Calling the acl migration script requires the configs to be present.
+                self.configure(env, upgrade_type=upgrade_type)
+                upgrade.run_migration(env, upgrade_type)
 
-  def start(self, env, upgrade_type=None):
-    import params
-    env.set_params(params)
-    self.configure(env, upgrade_type=upgrade_type)
+    def start(self, env, upgrade_type=None):
+        import params
+        env.set_params(params)
+        self.configure(env, upgrade_type=upgrade_type)
 
-    if params.kerberos_security_enabled:
-      if params.version and check_stack_feature(StackFeature.KAFKA_KERBEROS, params.version):
-        kafka_kinit_cmd = format("{kinit_path_local} -kt {kafka_keytab_path} {kafka_jaas_principal};")
-        Execute(kafka_kinit_cmd, user=params.kafka_user)
+        if params.kerberos_security_enabled:
+            if params.version and check_stack_feature(StackFeature.KAFKA_KERBEROS, params.version):
+                kafka_kinit_cmd = format("{kinit_path_local} -kt {kafka_keytab_path} {kafka_jaas_principal};")
+                Execute(kafka_kinit_cmd, user=params.kafka_user)
 
-    daemon_cmd = format('source {params.conf_dir}/kafka-env.sh ; {params.kafka_start_cmd} >>/dev/null 2>>{params.kafka_err_file} & echo $!>{params.kafka_pid_file}')
-    no_op_test = format('ls {params.kafka_pid_file}>/dev/null 2>&1 && ps -p `cat {params.kafka_pid_file}`>/dev/null 2>&1')
-    try:
-      Execute(daemon_cmd,
-              user=params.kafka_user,
-              not_if=no_op_test
-      )
-    except:
-      show_logs(params.kafka_log_dir, params.kafka_user)
-      raise
+        if params.is_supported_kafka_ranger:
+            setup_ranger_kafka()
+        daemon_cmd = format(
+            'source {params.conf_dir}/kafka-env.sh ; {params.kafka_start_cmd} >>/dev/null 2>>{params.kafka_err_file} & echo $!>{params.kafka_pid_file}')
+        no_op_test = format(
+            'ls {params.kafka_pid_file}>/dev/null 2>&1 && ps -p `cat {params.kafka_pid_file}`>/dev/null 2>&1')
+        try:
+            Execute(daemon_cmd,
+                    user=params.kafka_user,
+                    not_if=no_op_test
+                    )
+        except:
+            show_logs(params.kafka_log_dir, params.kafka_user)
+            raise
 
-  def stop(self, env, upgrade_type=None):
-    import os, time, params, signal
+    def stop(self, env, upgrade_type=None):
+        import os, time, params, signal
 
-    env.set_params(params)
-    # Kafka package scripts change permissions on folders, so we have to
-    # restore permissions after installing repo version bits
-    # before attempting to stop Kafka Broker
-    ensure_base_directories()
+        env.set_params(params)
+        # Kafka package scripts change permissions on folders, so we have to
+        # restore permissions after installing repo version bits
+        # before attempting to stop Kafka Broker
+        ensure_base_directories()
 
-    if not params.kafka_pid_file or not os.path.isfile(params.kafka_pid_file):
-      Logger.info("Kafka is not running. No pid file found.")
-      return
-    
-    try:
-      pid = int(sudo.read_file(params.kafka_pid_file))
-    except:
-      Logger.info("Pid file {0} does not exist or does not contain a process id number".format(params.kafka_pid_file))
-      return
+        if not params.kafka_pid_file or not os.path.isfile(params.kafka_pid_file):
+            Logger.info("Kafka is not running. No pid file found.")
+            return
+
+        try:
+            pid = int(sudo.read_file(params.kafka_pid_file))
+        except:
+            Logger.info(
+                "Pid file {0} does not exist or does not contain a process id number".format(params.kafka_pid_file))
+            return
 
-    max_wait = 120
-    for i in range(max_wait):
-      Logger.info("Waiting for Kafka Broker stop, current pid: {0}, seconds: {1}s".format(pid, i + 1))
-      try:
-        sudo.kill(pid, signal.SIGTERM)
-      except OSError, e:
-        Logger.info("Kafka Broker is not running, delete pid file: {0}".format(params.kafka_pid_file))
-        File(params.kafka_pid_file, action = "delete")
-        return
-        
-      time.sleep(1)
+        max_wait = 120
+        for i in range(max_wait):
+            Logger.info("Waiting for Kafka Broker stop, current pid: {0}, seconds: {1}s".format(pid, i + 1))
+            try:
+                sudo.kill(pid, signal.SIGTERM)
+            except OSError, e:
+                Logger.info("Kafka Broker is not running, delete pid file: {0}".format(params.kafka_pid_file))
+                File(params.kafka_pid_file, action="delete")
+                return
+
+            time.sleep(1)
 
-      try:
-        check_process_status(params.kafka_pid_file)
-      except ComponentIsNotRunning, e:
-        File(params.kafka_pid_file, action = "delete")
-        return
-    
-    raise Fail("Cannot stop Kafka Broker after {0} seconds".format(max_wait))
+            try:
+                check_process_status(params.kafka_pid_file)
+            except ComponentIsNotRunning, e:
+                File(params.kafka_pid_file, action="delete")
+                return
+
+        raise Fail("Cannot stop Kafka Broker after {0} seconds".format(max_wait))
 
-
-  def disable_security(self, env):
-    import params
-    if not params.zookeeper_connect:
-      Logger.info("No zookeeper connection string. Skipping reverting ACL")
-      return
-    if not params.secure_acls:
-      Logger.info("The zookeeper.set.acl is false. Skipping reverting ACL")
-      return
-    Execute(
-      "{0} --zookeeper.connect {1} --zookeeper.acl=unsecure".format(params.kafka_security_migrator, params.zookeeper_connect), \
-      user=params.kafka_user, \
-      environment={ 'JAVA_HOME': params.java64_home }, \
-      logoutput=True, \
-      tries=3)
+    def disable_security(self, env):
+        import params
+        if not params.zookeeper_connect:
+            Logger.info("No zookeeper connection string. Skipping reverting ACL")
+            return
+        if not params.secure_acls:
+            Logger.info("The zookeeper.set.acl is false. Skipping reverting ACL")
+            return
+        Execute(
+            "{0} --zookeeper.connect {1} --zookeeper.acl=unsecure".format(params.kafka_security_migrator,
+                                                                          params.zookeeper_connect), \
+            user=params.kafka_user, \
+            environment={'JAVA_HOME': params.java64_home}, \
+            logoutput=True, \
+            tries=3)
 
-  def status(self, env):
-    import status_params
-    env.set_params(status_params)
-    check_process_status(status_params.kafka_pid_file)
-    
-  def get_log_folder(self):
-    import params
-    return params.kafka_log_dir
-  
-  def get_user(self):
-    import params
-    return params.kafka_user
+    def status(self, env):
+        import status_params
+        env.set_params(status_params)
+        check_process_status(status_params.kafka_pid_file)
+
+    def get_log_folder(self):
+        import params
+        return params.kafka_log_dir
+
+    def get_user(self):
+        import params
+        return params.kafka_user
 
-  def get_pid_files(self):
-    import status_params
-    return [status_params.kafka_pid_file]
+    def get_pid_files(self):
+        import status_params
+        return [status_params.kafka_pid_file]
 
+
 if __name__ == "__main__":
-  KafkaBroker().execute()
+    KafkaBroker().execute()
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml
new file mode 100644
--- /dev/null	(date 1723877062746)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/metainfo.xml	(date 1723877062746)
@@ -0,0 +1,181 @@
+<?xml version="1.0"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<metainfo>
+    <schemaVersion>2.0</schemaVersion>
+    <services>
+        <service>
+            <name>RANGER</name>
+            <displayName>Ranger</displayName>
+            <version>2.4.0</version>
+            <comment>Component Ranger Power By JaneTTR . mail: 3832514048@qq.com ,git: https://gitee.com/tt-bigdata/ambari-env</comment>
+            <themes>
+                <theme>
+                    <fileName>credentials.json</fileName>
+                    <default>true</default>
+                </theme>
+                <theme>
+                    <fileName>database.json</fileName>
+                    <default>true</default>
+                </theme>
+                <theme>
+                    <fileName>directories.json</fileName>
+                    <default>true</default>
+                </theme>
+                <theme>
+                    <fileName>theme_version_2.json</fileName>
+                    <default>true</default>
+                </theme>
+            </themes>
+            <components>
+                <component>
+                    <name>RANGER_ADMIN</name>
+                    <displayName>Ranger Admin</displayName>
+                    <category>MASTER</category>
+                    <cardinality>1+</cardinality>
+                    <versionAdvertised>true</versionAdvertised>
+                    <dependencies>
+                        <dependency>
+                            <name>AMBARI_INFRA_SOLR/INFRA_SOLR_CLIENT</name>
+                            <scope>host</scope>
+                            <auto-deploy>
+                                <enabled>true</enabled>
+                            </auto-deploy>
+                        </dependency>
+                    </dependencies>
+                    <commandScript>
+                        <script>scripts/ranger_admin.py</script>
+                        <scriptType>PYTHON</scriptType>
+                        <timeout>600</timeout>
+                    </commandScript>
+                    <configuration-dependencies>
+                        <config-type>admin-properties</config-type>
+                        <config-type>admin-log4j</config-type>
+                        <config-type>ranger-solr-configuration</config-type>
+                    </configuration-dependencies>
+                    <logs>
+                        <log>
+                            <logId>ranger_admin</logId>
+                            <primary>true</primary>
+                        </log>
+                        <log>
+                            <logId>ranger_dbpatch</logId>
+                        </log>
+                    </logs>
+                </component>
+
+                <component>
+                    <name>RANGER_TAGSYNC</name>
+                    <displayName>Ranger Tagsync</displayName>
+                    <category>SLAVE</category>
+                    <cardinality>0-1</cardinality>
+                    <versionAdvertised>true</versionAdvertised>
+                    <commandScript>
+                        <script>scripts/ranger_tagsync.py</script>
+                        <scriptType>PYTHON</scriptType>
+                        <timeout>600</timeout>
+                    </commandScript>
+                    <configuration-dependencies>
+                        <config-type>ranger-tagsync-site</config-type>
+                        <config-type>tagsync-application-properties</config-type>
+                        <config-type>ranger-tagsync-policymgr-ssl</config-type>
+                        <config-type>atlas-tagsync-ssl</config-type>
+                        <config-type>tagsync-log4j</config-type>
+                    </configuration-dependencies>
+                </component>
+
+                <component>
+                    <name>RANGER_USERSYNC</name>
+                    <displayName>Ranger Usersync</displayName>
+                    <category>MASTER</category>
+                    <cardinality>1</cardinality>
+                    <versionAdvertised>true</versionAdvertised>
+                    <auto-deploy>
+                        <enabled>true</enabled>
+                        <co-locate>RANGER/RANGER_ADMIN</co-locate>
+                    </auto-deploy>
+                    <commandScript>
+                        <script>scripts/ranger_usersync.py</script>
+                        <scriptType>PYTHON</scriptType>
+                        <timeout>600</timeout>
+                    </commandScript>
+                    <logs>
+                        <log>
+                            <logId>ranger_usersync</logId>
+                            <primary>true</primary>
+                        </log>
+                    </logs>
+                    <configuration-dependencies>
+                        <config-type>ranger-ugsync-site</config-type>
+                        <config-type>usersync-log4j</config-type>
+                    </configuration-dependencies>
+                </component>
+
+            </components>
+
+            <configuration-dependencies>
+                <config-type>ranger-admin-site</config-type>
+            </configuration-dependencies>
+
+            <commandScript>
+                <script>scripts/service_check.py</script>
+                <scriptType>PYTHON</scriptType>
+                <timeout>300</timeout>
+            </commandScript>
+
+
+            <osSpecifics>
+                <osSpecific>
+                    <osFamily>any</osFamily>
+                    <packages>
+                        <package>
+                            <name>ranger_${stack_version}-admin</name>
+                        </package>
+                        <package>
+                            <name>ranger_${stack_version}-usersync</name>
+                        </package>
+                        <package>
+                            <name>ranger_${stack_version}-tagsync</name>
+                            <condition>should_install_ranger_tagsync</condition>
+                        </package>
+                        <package>
+                            <name>ambari-infra-solr-client</name>
+                            <condition>should_install_infra_solr_client</condition>
+                        </package>
+                    </packages>
+                </osSpecific>
+            </osSpecifics>
+
+            <sso>
+                <supported>true</supported>
+                <enabledConfiguration>ranger-admin-site/ranger.sso.enabled</enabledConfiguration>
+            </sso>
+
+            <quickLinksConfigurations>
+                <quickLinksConfiguration>
+                    <fileName>quicklinks.json</fileName>
+                    <default>true</default>
+                </quickLinksConfiguration>
+            </quickLinksConfigurations>
+
+        </service>
+    </services>
+
+</metainfo>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/service_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/service_check.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/service_check.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/service_check.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/package/scripts/service_check.py	(date 1719626239000)
@@ -34,9 +34,12 @@
     # Produce some messages and check if consumer reads same no.of messages.
 
     kafka_config = self.read_kafka_config()
-    topic = "ambari_kafka_service_check"
-    create_topic_cmd_created_output = "Created topic ambari_kafka_service_check."
-    create_topic_cmd_exists_output = "Topic \'ambari_kafka_service_check\' already exists."
+    #topic = "ambari_kafka_service_check"
+    topic = "ambari-kafka-service-check"
+    create_topic_cmd_created_output = "Created topic \"ambari-kafka-service-check\"."
+    create_topic_cmd_created_output2 = "Created topic ambari-kafka-service-check."
+    create_topic_cmd_exists_output = "Topic \"ambari-kafka-service-check\" already exists."
+    create_topic_cmd_exists_output2 = "Topic 'ambari-kafka-service-check' already exists."
     source_cmd = format("source {conf_dir}/kafka-env.sh")
     topic_exists_cmd = format(source_cmd + " ; " + "{kafka_home}/bin/kafka-topics.sh --zookeeper {kafka_config[zookeeper.connect]} --topic {topic} --list")
     topic_exists_cmd_code, topic_exists_cmd_out = shell.call(topic_exists_cmd, logoutput=True, quiet=False, user=params.kafka_user)
@@ -53,7 +56,7 @@
       create_topic_cmd = format("{kafka_home}/bin/kafka-topics.sh --zookeeper {kafka_config[zookeeper.connect]} --create --topic {topic} --partitions 1 --replication-factor 1")
       command = source_cmd + " ; " + create_topic_cmd
       Logger.info("Running kafka create topic command: %s" % command)
-      call_and_match_output(command, format("({create_topic_cmd_created_output})|({create_topic_cmd_exists_output})"), "Failed to check that topic exists", user=params.kafka_user)
+      call_and_match_output(command, format("({create_topic_cmd_created_output})|({create_topic_cmd_created_output2})|({create_topic_cmd_exists_output})|({create_topic_cmd_exists_output})"), "Failed to check that topic exists", user=params.kafka_user)
 
     under_rep_cmd = format("{kafka_home}/bin/kafka-topics.sh --describe --zookeeper {kafka_config[zookeeper.connect]} --under-replicated-partitions")
     under_rep_cmd_code, under_rep_cmd_out = shell.call(under_rep_cmd, logoutput=True, quiet=False, user=params.kafka_user)
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/stack_advisor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/stack_advisor.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/stack_advisor.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/stack_advisor.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/stack_advisor.py	(date 1723876037267)
@@ -1,4 +1,5 @@
 #!/usr/bin/env ambari-python-wrap
+# coding=utf-8
 """
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
@@ -131,6 +132,8 @@
             putYarnPropertyAttribute("yarn.timeline-service.http-authentication.proxyuser.{0}.hosts".format(old_ambari_user), 'delete', 'true')
             putYarnPropertyAttribute("yarn.timeline-service.http-authentication.proxyuser.{0}.groups".format(old_ambari_user), 'delete', 'true')
 
+    yarnSA = self.getServiceAdvisor('YARN')
+    yarnSA.getServiceConfigurationRecommendations(configurations, clusterData, services, hosts)
 
   def recommendMapReduce2Configurations(self, configurations, clusterData, services, hosts):
     putMapredProperty = self.putProperty(configurations, "mapred-site", services)
@@ -361,6 +364,10 @@
     # recommendations for "hadoop.proxyuser.*.hosts", "hadoop.proxyuser.*.groups" properties in core-site
     self.recommendHadoopProxyUsers(configurations, services, hosts)
 
+    hdfsSA = self.getServiceAdvisor('HDFS')
+    hdfsSA.getServiceConfigurationRecommendations(configurations, clusterData, services, hosts)
+
+
   def recommendHbaseConfigurations(self, configurations, clusterData, services, hosts):
     # recommendations for HBase env config
 
@@ -391,6 +398,9 @@
       and services['configurations']['hbase-env']['properties']['hbase_user'] != services['configurations']['hbase-site']['properties']['hbase.superuser']:
       putHbaseSiteProperty("hbase.superuser", services['configurations']['hbase-env']['properties']['hbase_user'])
 
+    hbaseSA = self.getServiceAdvisor('HBASE')
+    hbaseSA.getServiceConfigurationRecommendations(configurations, clusterData, services, hosts)
+
 
   def getAmsMemoryRecommendation(self, services, hosts):
     # MB per sink in hbase heapsize
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/hdfs_snamenode.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/hdfs_snamenode.py b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/hdfs_snamenode.py
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/hdfs_snamenode.py	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/HDFS/package/scripts/hdfs_snamenode.py	(date 1719626239000)
@@ -23,6 +23,7 @@
 from resource_management.libraries.functions.check_process_status import check_process_status
 from ambari_commons.os_family_impl import OsFamilyImpl, OsFamilyFuncImpl
 from ambari_commons import OSConst
+from setup_ranger_hdfs import setup_ranger_hdfs
 
 @OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)
 def snamenode(action=None, format=False):
@@ -46,6 +47,7 @@
          group=params.user_group)
       pass
   elif action == "start" or action == "stop":
+    setup_ranger_hdfs(None)
     import params
     service(
       action=action,
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-env.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-env.xml	(date 1719626239000)
@@ -0,0 +1,680 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true" supports_adding_forbidden="true">
+  <property>
+    <name>ranger_user</name>
+    <value>ranger</value>
+    <property-type>USER</property-type>
+    <display-name>Ranger User</display-name>
+    <description>Ranger username</description>
+    <value-attributes>
+      <type>user</type>
+      <overridable>false</overridable>
+      <user-groups>
+        <property>
+          <type>ranger-env</type>
+          <name>ranger_group</name>
+        </property>
+        <property>
+          <type>cluster-env</type>
+          <name>user_group</name>
+        </property>
+      </user-groups>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_group</name>
+    <value>ranger</value>
+    <property-type>GROUP</property-type>
+    <display-name>Ranger Group</display-name>
+    <description>Ranger group</description>
+    <value-attributes>
+      <type>user</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_admin_username</name>
+    <value>amb_ranger_admin</value>
+    <property-type>TEXT</property-type>
+    <display-name>Ranger Admin username for Ambari</display-name>
+    <description>This is the ambari user created for creating repositories and policies in Ranger Admin for each plugin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>ranger_admin_password</name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger Admin user's password for Ambari</display-name>
+    <description>This is the ambari user password created for creating repositories and policies in Ranger Admin for each plugin. Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are &quot; &apos; &#92; &#96; &#180;.</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>admin_username</name>
+    <value>admin</value>
+    <display-name>Ranger Admin username</display-name>
+    <description>This is the username for default admin user that is used for creating ambari user in Ranger Admin</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>admin_password</name>
+    <value></value>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger Admin user's password</display-name>
+    <description>This is the password for default admin user that is used for creating ambari user in Ranger Admin. Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are &quot; &apos; &#92; &#96; &#180;.</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_pid_dir</name>
+    <display-name>Ranger PID Dir</display-name>
+    <value>/var/run/ranger</value>
+    <description/>
+    <value-attributes>
+      <type>directory</type>
+      <overridable>false</overridable>
+      <editable-only-at-install>true</editable-only-at-install>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-hdfs-plugin-enabled</name>
+    <value>No</value>
+    <display-name>HDFS Ranger Plugin</display-name>
+    <description>Enable HDFS Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-hive-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Hive Ranger Plugin</display-name>
+    <description>Enable Hive Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-hbase-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Hbase Ranger Plugin</display-name>
+    <description>Enable HBase Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-storm-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Storm Ranger Plugin</display-name>
+    <description>Enable Storm Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-knox-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Knox Ranger Plugin</display-name>
+    <description>Enable Knox Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xml_configurations_supported</name>
+    <value>true</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>create_db_dbuser</name>
+    <value>true</value>
+    <display-name>Setup Database and Database User</display-name>
+    <description>If set to Yes, Ambari will create and setup Ranger Database and Database User. This will require to specify Database Admin user and password</description>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_privelege_user_jdbc_url</name>
+    <display-name>JDBC connect string for root user</display-name>
+    <description>JDBC connect string - auto populated based on other values. This is to be used by root user</description>
+    <value>jdbc:mysql://localhost</value>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>DB_FLAVOR</name>
+      </property>
+      <property>
+        <type>admin-properties</type>
+        <name>db_host</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-yarn-plugin-enabled</name>
+    <value>No</value>
+    <display-name>YARN Ranger Plugin</display-name>
+    <description>Enable YARN Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-kafka-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Kafka Ranger Plugin</display-name>
+    <description>Enable Kafka Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.solr</name>
+    <value>true</value>
+    <display-name>Audit to Solr</display-name>
+    <description>Enable Audit to Solr for all ranger supported services. This property is overridable at service level</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>is_solrCloud_enabled</name>
+    <display-name>SolrCloud</display-name>
+    <description>SolrCloud uses zookeeper for distributed search and indexing</description>
+    <value>false</value>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs</name>
+    <value>true</value>
+    <display-name>Audit to HDFS</display-name>
+    <description>Enable Audit to HDFS for all ranger supported services. This property is overridable at service level</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>xasecure.audit.destination.hdfs.dir</name>
+    <value>hdfs://localhost:8020</value>
+    <display-name>Destination HDFS Directory</display-name>
+    <description>HDFS folder to write audit to, make sure all service user has required permissions. This property is overridable at service level</description>
+    <depends-on>
+      <property>
+        <type>core-site</type>
+        <name>fs.defaultFS</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_solr_config_set</name>
+    <value>ranger_audits</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_solr_collection_name</name>
+    <value>ranger_audits</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_solr_shards</name>
+    <value>1</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_solr_replication_factor</name>
+    <value>1</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-atlas-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Atlas Ranger Plugin</display-name>
+    <description>Enable Atlas Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>is_external_solrCloud_enabled</name>
+    <display-name>External SolrCloud</display-name>
+    <value>false</value>
+    <description>Using Externally managed solr cloud ?</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>is_external_solrCloud_kerberos</name>
+    <display-name>External SolrCloud kerberos</display-name>
+    <value>false</value>
+    <description>Is Externally managed solr cloud kerberos ?</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger-nifi-plugin-enabled</name>
+    <value>No</value>
+    <display-name>NIFI Ranger Plugin</display-name>
+    <description>Enable NIFI Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>is_nested_groupsync_enabled</name>
+    <display-name>Sync Nested Groups</display-name>
+    <description/>
+    <value>false</value>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>ranger-env template</display-name>
+    <description>This is the jinja template for Ranger env</description>
+    <value/>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value-attributes>
+      <property-file-name>ranger-env.sh.j2</property-file-name>
+      <property-file-type>text</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>rangerusersync_user_password</name>
+    <value></value>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger Usersync user's password</display-name>
+    <description>This is the rangerusersync user password. Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are &quot; &apos; &#92; &#96; &#180;.</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>rangertagsync_user_password</name>
+    <value></value>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger Tagsync user's password</display-name>
+    <description>This is the rangertagsync user password. Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are &quot; &apos; &#92; &#96; &#180;.</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>keyadmin_user_password</name>
+    <value></value>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger KMS keyadmin user's password</display-name>
+    <description>This is the keyadmin user password. Password should be minimum 10 characters with minimum one alphabet and one numeric. Unsupported special characters are &quot; &apos; &#92; &#96; &#180;.</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_admin_max_heap_size</name>
+    <value>1g</value>
+    <display-name>Ranger Admin heap size.</display-name>
+    <description>Ranger Admin maximum heap size limit.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_usersync_max_heap_size</name>
+    <value>1g</value>
+    <display-name>Ranger Usersync max heap size.</display-name>
+    <description>Ranger Usersync maximum heap size limit.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_tagsync_max_heap_size</name>
+    <value>1g</value>
+    <display-name>Ranger Tagsync max heap size.</display-name>
+    <description>Ranger Tagsync maximum heap size limit.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger-ozone-plugin-enabled</name>
+    <value>No</value>
+    <display-name>OZONE Ranger Plugin</display-name>
+    <description>Enable OZONE Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger-spark3-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Spark3 Ranger Plugin</display-name>
+    <description>Enable Spark3 Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger-kudu-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Kudu Ranger Plugin</display-name>
+    <description>Enable Kudu Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+ <property>
+    <name>ranger-doris-plugin-enabled</name>
+    <value>No</value>
+    <display-name>Doris Ranger Plugin</display-name>
+    <description>Enable Doris Ranger plugin</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>Yes</value>
+          <label>ON</label>
+        </entry>
+        <entry>
+          <value>No</value>
+          <label>OFF</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-env.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-env.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-env.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-env.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-env.xml	(date 1719626239000)
@@ -20,111 +20,116 @@
  */
 -->
 <configuration supports_adding_forbidden="true">
-    <property>
-        <name>kafka_user</name>
-        <display-name>Kafka User</display-name>
-        <value>kafka</value>
-        <property-type>USER</property-type>
-        <description />
-        <value-attributes>
-            <type>user</type>
-            <overridable>false</overridable>
-            <user-groups>
-                <property>
-                    <type>cluster-env</type>
-                    <name>user_group</name>
-                </property>
-            </user-groups>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kafka_keytab</name>
-        <description>Kafka keytab path</description>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kafka_principal_name</name>
-        <description>Kafka principal name</description>
-        <property-type>KERBEROS_PRINCIPAL</property-type>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kafka_log_dir</name>
-        <display-name>Kafka Log directory</display-name>
-        <value>/var/log/kafka</value>
-        <description />
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kafka_pid_dir</name>
-        <value>/var/run/kafka</value>
-        <display-name>Kafka PID dir</display-name>
-        <description />
-        <value-attributes>
-            <type>directory</type>
-            <editable-only-at-install>true</editable-only-at-install>
-            <overridable>false</overridable>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kafka_user_nofile_limit</name>
-        <value>128000</value>
-        <description>Max open files limit setting for KAFKA user.</description>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kerberos_merge_advertised_listeners</name>
-        <value>true</value>
-        <value-attributes>
-            <type>boolean</type>
-        </value-attributes>
-        <description>Merge default Ambari-managed "advertised.listeners" value with user defined value.</description>
-        <on-ambari-upgrade add="true" />
-    </property>
-    <property>
-        <name>kafka_user_nproc_limit</name>
-        <value>65536</value>
-        <description>Max number of processes limit setting for KAFKA user.</description>
-        <on-ambari-upgrade add="false" />
-    </property>
-
-    <property>
-        <name>content</name>
-        <display-name>kafka-env template</display-name>
-        <description>This is the jinja template for kafka-env.sh file</description>
-        <value>
-  #!/bin/bash
-  
-  # Set KAFKA specific environment variables here.
-  
-  # The java implementation to use.
-  export JAVA_HOME={{java64_home}}
-  export PATH=$PATH:$JAVA_HOME/bin
-  export PID_DIR={{kafka_pid_dir}}
-  export LOG_DIR={{kafka_log_dir}}
-  {% if kerberos_security_enabled or kafka_other_sasl_enabled %}
-  export KAFKA_OPTS="-Djavax.security.auth.useSubjectCredsOnly=false {{kafka_kerberos_params}}"
-  {% else %}
-  export KAFKA_OPTS={{kafka_kerberos_params}}
-  {% endif %}
-  # Add kafka sink to classpath and related depenencies
-  if [ -e "/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar" ]; then
-    export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar
-    export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/lib/*
-  fi
-  {% if stack_supports_kafka_env_include_ranger_script %}
-  if [ -f /etc/kafka/conf/kafka-ranger-env.sh ]; then
-  . /etc/kafka/conf/kafka-ranger-env.sh
-  fi
-  {% else %}
-        export CLASSPATH=$CLASSPATH:{{conf_dir}}
-  {% endif %}
-      </value>
-        <value-attributes>
-            <type>content</type>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-</configuration>
\ No newline at end of file
+  <property>
+    <name>kafka_user</name>
+    <display-name>Kafka User</display-name>
+    <value>kafka</value>
+    <property-type>USER</property-type>
+    <description/>
+    <value-attributes>
+      <type>user</type>
+      <overridable>false</overridable>
+      <user-groups>
+        <property>
+          <type>cluster-env</type>
+          <name>user_group</name>
+        </property>
+      </user-groups>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kafka_keytab</name>
+    <description>Kafka keytab path</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kafka_principal_name</name>
+    <description>Kafka principal name</description>
+    <property-type>KERBEROS_PRINCIPAL</property-type>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kafka_log_dir</name>
+    <display-name>Kafka Log directory</display-name>
+    <value>/var/log/kafka</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kafka_pid_dir</name>
+    <value>/var/run/kafka</value>
+    <display-name>Kafka PID dir</display-name>
+    <description/>
+    <value-attributes>
+      <type>directory</type>
+      <editable-only-at-install>true</editable-only-at-install>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kafka_user_nofile_limit</name>
+    <value>128000</value>
+    <description>Max open files limit setting for KAFKA user.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kerberos_merge_advertised_listeners</name>
+    <value>true</value>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <description>Merge default Ambari-managed "advertised.listeners" value with user defined value.</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+  <property>
+    <name>kafka_user_nproc_limit</name>
+    <value>65536</value>
+    <description>Max number of processes limit setting for KAFKA user.</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <!-- kafka-env.sh -->
+  <property>
+    <name>content</name>
+    <display-name>kafka-env template</display-name>
+    <description>This is the jinja template for kafka-env.sh file</description>
+    <value>
+#!/bin/bash
+
+# Set KAFKA specific environment variables here.
+
+# The java implementation to use.
+export JAVA_HOME={{java64_home}}
+export PATH=$PATH:$JAVA_HOME/bin
+export PID_DIR={{kafka_pid_dir}}
+export LOG_DIR={{kafka_log_dir}}
+{% if kerberos_security_enabled or kafka_other_sasl_enabled %}
+export KAFKA_KERBEROS_PARAMS="-Djavax.security.auth.useSubjectCredsOnly=false {{kafka_kerberos_params}}"
+{% else %}
+export KAFKA_KERBEROS_PARAMS={{kafka_kerberos_params}}
+{% endif %}
+# Add kafka sink to classpath and related depenencies
+if [ -e "/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar" ]; then
+  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar
+  export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/lib/*
+fi
+{% if stack_supports_kafka_env_include_ranger_script %}
+if [ -f /etc/kafka/conf/kafka-ranger-env.sh ]; then
+. /etc/kafka/conf/kafka-ranger-env.sh
+fi
+{% else %}
+      export CLASSPATH=$CLASSPATH:{{conf_dir}}
+{% endif %}
+    </value>
+    <value-attributes>
+      <type>content</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>is_supported_kafka_ranger</name>
+    <value>true</value>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-log4j.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-log4j.xml	(date 1719626239000)
@@ -0,0 +1,145 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="false">
+  <property>
+    <name>ranger_xa_log_maxfilesize</name>
+    <value>256</value>
+   <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Ranger Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+   </property>
+   <property>
+    <name>ranger_xa_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Ranger Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>admin-log4j template</display-name>
+    <description>admin-log4j.properties</description>
+    <value>
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+
+log4j.rootLogger = warn,xa_log_appender
+
+
+# xa_logger
+log4j.appender.xa_log_appender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.xa_log_appender.file=${logdir}/xa_portal.log
+log4j.appender.xa_log_appender.datePattern='.'yyyy-MM-dd
+log4j.appender.xa_log_appender.append=true
+log4j.appender.xa_log_appender.layout=org.apache.log4j.PatternLayout
+log4j.appender.xa_log_appender.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %C{6} (%F:%L) - %m%n
+log4j.appender.xa_log_appender.MaxFileSize={{ranger_xa_log_maxfilesize}}MB
+
+# xa_log_appender : category and additivity
+log4j.category.org.springframework=warn,xa_log_appender
+log4j.additivity.org.springframework=false
+
+log4j.category.org.apache.ranger=info,xa_log_appender
+log4j.additivity.org.apache.ranger=false
+
+log4j.category.xa=info,xa_log_appender
+log4j.additivity.xa=false
+
+# perf_logger
+log4j.appender.perf_appender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.perf_appender.file=${logdir}/ranger_admin_perf.log
+log4j.appender.perf_appender.datePattern='.'yyyy-MM-dd
+log4j.appender.perf_appender.append=true
+log4j.appender.perf_appender.layout=org.apache.log4j.PatternLayout
+log4j.appender.perf_appender.layout.ConversionPattern=%d{ISO8601} [%t] %m%n
+
+
+# sql_appender
+log4j.appender.sql_appender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.sql_appender.file=${logdir}/xa_portal_sql.log
+log4j.appender.sql_appender.datePattern='.'yyyy-MM-dd
+log4j.appender.sql_appender.append=true
+log4j.appender.sql_appender.layout=org.apache.log4j.PatternLayout
+log4j.appender.sql_appender.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %C{6} (%F:%L) - %m%n
+
+# sql_appender : category and additivity
+log4j.category.org.hibernate.SQL=warn,sql_appender
+log4j.additivity.org.hibernate.SQL=false
+
+log4j.category.jdbc.sqlonly=fatal,sql_appender
+log4j.additivity.jdbc.sqlonly=false
+
+log4j.category.jdbc.sqltiming=warn,sql_appender
+log4j.additivity.jdbc.sqltiming=false
+
+log4j.category.jdbc.audit=fatal,sql_appender
+log4j.additivity.jdbc.audit=false
+
+log4j.category.jdbc.resultset=fatal,sql_appender
+log4j.additivity.jdbc.resultset=false
+
+log4j.category.jdbc.connection=fatal,sql_appender
+log4j.additivity.jdbc.connection=false
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  
+  <property>
+    <name>logback-content</name>
+    <display-name>logback.xml template</display-name>
+    <description>logback.xml</description>
+    <value/>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value-attributes>
+      <property-file-name>admin-logback.xml.j2</property-file-name>
+      <property-file-type>xml</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-log4j.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-log4j.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-log4j.xml	(date 1719626239000)
@@ -1,6 +1,8 @@
 <?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 <!--
- censed to the Apache Software Foundation (ASF) under one
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
  * regarding copyright ownership.  The ASF licenses this file
@@ -17,152 +19,152 @@
  * limitations under the License.
  */
 -->
-<configuration supports_adding_forbidden="false" supports_final="false">
-    <property>
-        <name>kafka_log_maxfilesize</name>
-        <value>256</value>
-        <description>The maximum size of backup file before the log is rotated</description>
-        <display-name>Kafka Log: backup file size</display-name>
-        <value-attributes>
-            <unit>MB</unit>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>kafka_log_maxbackupindex</name>
-        <value>20</value>
-        <description>The number of backup files</description>
-        <display-name>Kafka Log: # of backup files</display-name>
-        <value-attributes>
-            <type>int</type>
-            <minimum>0</minimum>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>controller_log_maxfilesize</name>
-        <value>256</value>
-        <description>The maximum size of backup file before the log is rotated</description>
-        <display-name>Kafka Controller Log: backup file size</display-name>
-        <value-attributes>
-            <unit>MB</unit>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>controller_log_maxbackupindex</name>
-        <value>20</value>
-        <description>The number of backup files</description>
-        <display-name>Kafka Controller Log: # of backup files</display-name>
-        <value-attributes>
-            <type>int</type>
-            <minimum>0</minimum>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-    <property>
-        <name>content</name>
-        <display-name>kafka-log4j template</display-name>
-        <description>Custom log4j.properties</description>
-        <value>
- #
- #
- # Licensed to the Apache Software Foundation (ASF) under one
- # or more contributor license agreements.  See the NOTICE file
- # distributed with this work for additional information
- # regarding copyright ownership.  The ASF licenses this file
- # to you under the Apache License, Version 2.0 (the
- # "License"); you may not use this file except in compliance
- # with the License.  You may obtain a copy of the License at
- #
- #   http://www.apache.org/licenses/LICENSE-2.0
- #
- # Unless required by applicable law or agreed to in writing,
- # software distributed under the License is distributed on an
- # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- # KIND, either express or implied.  See the License for the
- # specific language governing permissions and limitations
- # under the License.
- #
- #
- #
- kafka.logs.dir=logs
- 
- log4j.rootLogger=INFO, stdout
- 
- log4j.appender.stdout=org.apache.log4j.ConsoleAppender
- log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
- log4j.appender.stdout.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
- 
- log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender
- log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH
- log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log
- log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout
- log4j.appender.kafkaAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
- log4j.appender.kafkaAppender.MaxFileSize = {{kafka_log_maxfilesize}}MB
- log4j.appender.kafkaAppender.MaxBackupIndex = {{kafka_log_maxbackupindex}}
- 
- log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender
- log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH
- log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log
- log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout
- log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
- 
- log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender
- log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH
- log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log
- log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout
- log4j.appender.requestAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
- 
- log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender
- log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH
- log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log
- log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout
- log4j.appender.cleanerAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
- 
- log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender
- log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH
- log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log
- log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout
- log4j.appender.controllerAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
- log4j.appender.controllerAppender.MaxFileSize = {{controller_log_maxfilesize}}MB
- log4j.appender.controllerAppender.MaxBackupIndex = {{controller_log_maxbackupindex}}
- # Turn on all our debugging info
- #log4j.logger.kafka.producer.async.DefaultEventHandler=DEBUG, kafkaAppender
- #log4j.logger.kafka.client.ClientUtils=DEBUG, kafkaAppender
- #log4j.logger.kafka.perf=DEBUG, kafkaAppender
- #log4j.logger.kafka.perf.ProducerPerformance$ProducerThread=DEBUG, kafkaAppender
- #log4j.logger.org.I0Itec.zkclient.ZkClient=DEBUG
- log4j.logger.kafka=INFO, kafkaAppender
- log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender
- log4j.additivity.kafka.network.RequestChannel$=false
- 
- #log4j.logger.kafka.network.Processor=TRACE, requestAppender
- #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender
- #log4j.additivity.kafka.server.KafkaApis=false
- log4j.logger.kafka.request.logger=WARN, requestAppender
- log4j.additivity.kafka.request.logger=false
- 
- log4j.logger.kafka.controller=TRACE, controllerAppender
- log4j.additivity.kafka.controller=false
- 
- log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender
- log4j.additivity.kafka.log.LogCleaner=false
- 
- log4j.logger.state.change.logger=TRACE, stateChangeAppender
- log4j.additivity.state.change.logger=false
- 
-    </value>
-        <value-attributes>
-            <type>content</type>
-            <show-property-name>false</show-property-name>
-        </value-attributes>
-        <depends-on>
-            <property>
-                <type>ranger-kafka-plugin-properties</type>
-                <name>ranger-kafka-plugin-enabled</name>
-            </property>
-        </depends-on>
-        <on-ambari-upgrade add="false" />
-    </property>
-</configuration>
\ No newline at end of file
+<configuration supports_final="false" supports_adding_forbidden="false">
+   <property>
+    <name>kafka_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Kafka Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>kafka_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Kafka Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>controller_log_maxfilesize</name>
+    <value>256</value>
+    <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Kafka Controller Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>controller_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Kafka Controller Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>kafka-log4j template</display-name>
+    <description>Custom log4j.properties</description>
+    <value>
+#
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+#
+#
+kafka.logs.dir=logs
+
+log4j.rootLogger=INFO, stdout
+
+log4j.appender.stdout=org.apache.log4j.ConsoleAppender
+log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
+log4j.appender.stdout.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
+
+log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH
+log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log
+log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout
+log4j.appender.kafkaAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
+log4j.appender.kafkaAppender.MaxFileSize = {{kafka_log_maxfilesize}}MB
+log4j.appender.kafkaAppender.MaxBackupIndex = {{kafka_log_maxbackupindex}}
+
+log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH
+log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log
+log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout
+log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
+
+log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH
+log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log
+log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout
+log4j.appender.requestAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
+
+log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH
+log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log
+log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout
+log4j.appender.cleanerAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
+
+log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH
+log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log
+log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout
+log4j.appender.controllerAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n
+log4j.appender.controllerAppender.MaxFileSize = {{controller_log_maxfilesize}}MB
+log4j.appender.controllerAppender.MaxBackupIndex = {{controller_log_maxbackupindex}}
+# Turn on all our debugging info
+#log4j.logger.kafka.producer.async.DefaultEventHandler=DEBUG, kafkaAppender
+#log4j.logger.kafka.client.ClientUtils=DEBUG, kafkaAppender
+#log4j.logger.kafka.perf=DEBUG, kafkaAppender
+#log4j.logger.kafka.perf.ProducerPerformance$ProducerThread=DEBUG, kafkaAppender
+#log4j.logger.org.I0Itec.zkclient.ZkClient=DEBUG
+log4j.logger.kafka=INFO, kafkaAppender
+log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender
+log4j.additivity.kafka.network.RequestChannel$=false
+
+#log4j.logger.kafka.network.Processor=TRACE, requestAppender
+#log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender
+#log4j.additivity.kafka.server.KafkaApis=false
+log4j.logger.kafka.request.logger=WARN, requestAppender
+log4j.additivity.kafka.request.logger=false
+
+log4j.logger.kafka.controller=TRACE, controllerAppender
+log4j.additivity.kafka.controller=false
+
+log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender
+log4j.additivity.kafka.log.LogCleaner=false
+
+log4j.logger.state.change.logger=TRACE, stateChangeAppender
+log4j.additivity.state.change.logger=false
+
+   </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-kafka-plugin-properties</type>
+        <name>ranger-kafka-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-log4j.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-log4j.xml	(date 1719626239000)
@@ -0,0 +1,104 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="false">
+  <property>
+    <name>ranger_tagsync_log_maxfilesize</name>
+    <value>256</value>
+   <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Ranger tagsync Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+   </property>
+    <property>
+     <name>ranger_tagsync_log_number_of_backup_files</name>
+     <value>20</value>
+     <description>The number of backup files</description>
+     <display-name>Ranger tagsync Log: # of backup files</display-name>
+     <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>tagsync-log4j template</display-name>
+    <description>tagsync-log4j.properties</description>
+    <value>
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+
+log4j.rootLogger = info,logFile
+
+# logFile
+log4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.logFile.file=${logdir}/tagsync.log
+log4j.appender.logFile.datePattern='.'yyyy-MM-dd
+log4j.appender.logFile.layout=org.apache.log4j.PatternLayout
+log4j.appender.logFile.MaxFileSize={{ranger_tagsync_log_maxfilesize}}MB
+log4j.appender.logFile.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %L %m%n
+
+# console
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.Target=System.out
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %L %m%n
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  
+  <property>
+    <name>logback-content</name>
+    <display-name>logback.xml template</display-name>
+    <description>logback.xml</description>
+    <value/>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value-attributes>
+      <property-file-name>tagsync-logback.xml.j2</property-file-name>
+      <property-file-type>xml</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  
+  
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-broker.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-broker.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-broker.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-broker.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka-broker.xml	(date 1719626239000)
@@ -1,6 +1,8 @@
 <?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 <!--
- censed to the Apache Software Foundation (ASF) under one
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
  * regarding copyright ownership.  The ASF licenses this file
@@ -29,7 +31,7 @@
     <value-attributes>
       <type>directories</type>
     </value-attributes>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>zookeeper.connect</name>
@@ -40,7 +42,7 @@
      string in the form hostname1:port1,hostname2:port2,hostname3:port3/chroot/path which would put all this cluster's data under the
       path /chroot/path. Note that consumers must use the same connection string.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>message.max.bytes</name>
@@ -50,7 +52,7 @@
       It is important that this property be in sync with the maximum fetch size your consumers use or
       else an unruly producer will be able to publish messages too large for consumers to consume.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>num.network.threads</name>
@@ -59,7 +61,7 @@
       The number of network threads that the server uses for handling network requests.
       You probably don't need to change this.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>num.io.threads</name>
@@ -67,13 +69,13 @@
     <description>
       The number of I/O threads that the server uses for executing requests. You should have at least as many threads as you have disks.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>queued.max.requests</name>
     <value>500</value>
     <description>The number of requests that can be queued up for processing by the I/O threads before the network threads stop reading in new requests.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>socket.send.buffer.bytes</name>
@@ -81,7 +83,7 @@
     <description>
       The SO_SNDBUFF buffer the server prefers for socket connections.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>socket.receive.buffer.bytes</name>
@@ -89,7 +91,7 @@
     <description>
       The SO_RCVBUFF buffer the server prefers for socket connections.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>socket.request.max.bytes</name>
@@ -97,7 +99,7 @@
     <description>
       The maximum request size the server will allow. This prevents the server from running out of memory and should be smaller than the Java heap size.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>num.partitions</name>
@@ -105,7 +107,7 @@
     <description>
         The default number of partitions per topic.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.segment.bytes</name>
@@ -114,7 +116,7 @@
       The maximum request size the server will allow.
       This prevents the server from running out of memory and should be smaller than the Java heap size.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.roll.hours</name>
@@ -122,7 +124,7 @@
     <description>
       This setting will force Kafka to roll a new log segment even if the log.segment.bytes size has not been reached.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.retention.bytes</name>
@@ -130,7 +132,7 @@
     <description>
       The amount of data to retain in the log for each topic-partitions. Note that this is the limit per-partition so multiply by the number of partitions to get the total data retained for the topic. Also note that if both log.retention.hours and log.retention.bytes are both set we delete a segment when either limit is exceeded.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.retention.hours</name>
@@ -138,7 +140,7 @@
     <description>
       The number of hours to keep a log segment before it is deleted, i.e. the default data retention window for all topics. Note that if both log.retention.hours and log.retention.bytes are both set we delete a segment when either limit is exceeded.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.cleanup.interval.mins</name>
@@ -146,7 +148,7 @@
     <description>The frequency in minutes that the log cleaner checks whether any log segment is eligible for deletion to meet the retention policies.
     </description>
     <deleted>true</deleted>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.retention.check.interval.ms</name>
@@ -154,7 +156,7 @@
     <description>
       The frequency in milliseconds that the log cleaner checks whether any log segment is eligible for deletion to meet the retention policies.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.index.size.max.bytes</name>
@@ -164,7 +166,7 @@
       sparse file with this much space and shrink it down when the log rolls. If the index fills up we will roll a new log segment
       even if we haven't reached the log.segment.bytes limit.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>log.index.interval.bytes</name>
@@ -172,7 +174,7 @@
     <description>
       The byte interval at which we add an entry to the offset index. When executing a fetch request the server must do a linear scan for up to this many bytes to find the correct position in the log to begin and end the fetch. So setting this value to be larger will mean larger index files (and a bit more memory usage) but less scanning. However the server will never add more than one index entry per log append (even if more than log.index.interval worth of messages are appended). In general you probably don't need to mess with this value.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>auto.create.topics.enable</name>
@@ -180,31 +182,31 @@
     <description>
       Enable auto creation of topic on the server. If this is set to true then attempts to produce, consume, or fetch metadata for a non-existent topic will automatically create it with the default replication factor and number of partitions.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>controller.socket.timeout.ms</name>
     <value>30000</value>
     <description>The socket timeout for commands from the partition management controller to the replicas.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>controller.message.queue.size</name>
     <value>10</value>
     <description>The buffer size for controller-to-broker-channels</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>default.replication.factor</name>
     <value>1</value>
     <description>The default replication factor for automatically created topics.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.lag.time.max.ms</name>
     <value>10000</value>
     <description>If a follower hasn't sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.lag.max.messages</name>
@@ -212,38 +214,38 @@
     <description>
       If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.socket.timeout.ms</name>
     <value>30000</value>
     <description>The socket timeout for network requests to the leader for replicating data.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.socket.receive.buffer.bytes</name>
     <value>65536</value>
     <description>The socket receive buffer for network requests to the leader for replicating data.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.fetch.max.bytes</name>
     <value>1048576</value>
     <description>The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.fetch.wait.max.ms</name>
     <value>500</value>
     <description>The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.fetch.min.bytes</name>
     <value>1</value>
     <description>Minimum bytes expected for each fetch response for the fetch requests from the replica to the leader. If not enough bytes, wait up to replica.fetch.wait.max.ms for this many bytes to arrive.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>num.replica.fetchers</name>
@@ -251,49 +253,49 @@
     <description>
       Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>replica.high.watermark.checkpoint.interval.ms</name>
     <value>5000</value>
     <description>The frequency with which each replica saves its high watermark to disk to handle recovery.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>fetch.purgatory.purge.interval.requests</name>
     <value>10000</value>
     <description>The purge interval (in number of requests) of the fetch request purgatory.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>producer.purgatory.purge.interval.requests</name>
     <value>10000</value>
     <description>The purge interval (in number of requests) of the producer request purgatory.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>zookeeper.session.timeout.ms</name>
     <value>30000</value>
     <description>Zookeeper session timeout. If the server fails to heartbeat to zookeeper within this period of time it is considered dead. If you set this too low the server may be falsely considered dead; if you set it too high it may take too long to recognize a truly dead server.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>zookeeper.connection.timeout.ms</name>
     <value>25000</value>
     <description>The maximum amount of time that the client waits to establish a connection to zookeeper.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>zookeeper.sync.time.ms</name>
     <value>2000</value>
     <description>How far a ZK follower can be behind a ZK leader.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>controlled.shutdown.max.retries</name>
     <value>3</value>
     <description>Number of retries to complete the controlled shutdown successfully before executing an unclean shutdown.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>controlled.shutdown.retry.backoff.ms</name>
@@ -301,7 +303,7 @@
     <description>
       Backoff time between shutdown retries.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.metrics.reporters</name>
@@ -312,7 +314,7 @@
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
     </value-attributes>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.ganglia.metrics.reporter.enabled</name>
@@ -320,158 +322,158 @@
     <description>
       kafka ganglia metrics reporter enable
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.ganglia.metrics.host</name>
     <value>localhost</value>
     <description> Ganglia host </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.ganglia.metrics.port</name>
     <value>8671</value>
     <description> Ganglia port </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.ganglia.metrics.group</name>
     <value>kafka</value>
     <description>Ganglia group name </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.reporter.enabled</name>
     <value>true</value>
     <description>Kafka timeline metrics reporter enable</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.hosts</name>
     <value>{{ams_collector_hosts}}</value>
     <description>Timeline host</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.port</name>
     <value>{{metric_collector_port}}</value>
     <description>Timeline port</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.protocol</name>
     <value>{{metric_collector_protocol}}</value>
     <description>Timeline protocol(http or https)</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.truststore.path</name>
     <value>{{metric_truststore_path}}</value>
     <description>Location of the trust store file.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.truststore.type</name>
     <value>{{metric_truststore_type}}</value>
     <description>Optional. Default value is "jks".</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.truststore.password</name>
     <value>{{metric_truststore_password}}</value>
     <description>Password to open the trust store file.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.reporter.sendInterval</name>
     <value>5900</value>
     <description>Timeline metrics reporter send interval</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.maxRowCacheSize</name>
     <value>10000</value>
     <description>Timeline metrics reporter send interval</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.host_in_memory_aggregation</name>
     <value>{{host_in_memory_aggregation}}</value>
     <description>if set to "true" host metrics will be aggregated in memory on each host</description>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.host_in_memory_aggregation_port</name>
     <value>{{host_in_memory_aggregation_port}}</value>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>kafka.timeline.metrics.host_in_memory_aggregation_protocol</name>
     <value>{{host_in_memory_aggregation_protocol}}</value>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>listeners</name>
     <value>PLAINTEXT://localhost:9092</value>
     <description>host and port where kafka broker will be accepting connections. localhost will be substituted with hostname. This property managed with Ambari and every user-defined listeners need to be placed to raw.listeners property.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>raw.listeners</name>
-    <value />
+    <value></value>
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
     </value-attributes>
     <description>User-defined listeners that will be unchanged by Ambari. Value of this property will be merged with Ambari-managed listeners.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>controlled.shutdown.enable</name>
     <value>true</value>
     <description>Enable controlled shutdown of the broker. If enabled, the broker will move all leaders on it to some other brokers before shutting itself down. This reduces the unavailability window during shutdown.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>auto.leader.rebalance.enable</name>
     <value>true</value>
     <description>Enables auto leader balancing. A background thread checks and triggers leader balance if required at regular intervals</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>num.recovery.threads.per.data.dir</name>
     <value>1</value>
     <description>The number of threads per data directory to be used for log recovery at startup and flushing at shutdown</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>min.insync.replicas</name>
     <value>1</value>
     <description>define the minimum number of replicas in ISR needed to satisfy a produce request with required.acks=-1 (or all)</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>leader.imbalance.per.broker.percentage</name>
     <value>10</value>
     <description>The ratio of leader imbalance allowed per broker. The controller would trigger a leader balance if it goes above this value per broker. The value is specified in percentage.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>leader.imbalance.check.interval.seconds</name>
     <value>300</value>
     <description>The frequency with which the partition rebalance check is triggered by the controller</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offset.metadata.max.bytes</name>
     <value>4096</value>
     <description>The maximum size for a metadata entry associated with an offset commit</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.load.buffer.size</name>
     <value>5242880</value>
     <description>Batch size for reading from the offsets segments when loading offsets into the cache.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.topic.replication.factor</name>
@@ -480,62 +482,62 @@
     To ensure that the effective replication factor of the offsets topic is the configured value,
     the number of alive brokers has to be at least the replication factor at the time of the
     first request for the offsets topic. If not, either the offsets topic creation will fail or it will get a replication factor of min(alive brokers, configured replication factor).</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.topic.num.partitions</name>
     <value>50</value>
     <description>The number of partitions for the offset commit topic (should not change after deployment)</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.topic.segment.bytes</name>
     <value>104857600</value>
     <description>The offsets topic segment bytes should be kept relatively small in order to facilitate faster log compaction and cache loads</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.topic.compression.codec</name>
     <value>0</value>
     <description>Compression codec for the offsets topic - compression may be used to achieve \"atomic\" commits. Default is NoCompression. For Gzip add value 1 , SnappyCompression add value 2, LZ4CompressionCodec 3.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.retention.minutes</name>
     <value>86400000</value>
     <description>Log retention window in minutes for offsets topic</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.retention.check.interval.ms</name>
     <value>600000</value>
     <description>Frequency at which to check for stale offsets</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.commit.timeout.ms</name>
     <value>5000</value>
     <description>Offset commit will be delayed until all replicas for the offsets topic receive the commit or this timeout is reached. This is similar to the producer request timeout.</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>offsets.commit.required.acks</name>
     <value>-1</value>
     <description>The required acks before the commit can be accepted. In general, the default (-1) should not be overridden</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>delete.topic.enable</name>
     <value>true</value>
     <description>Enables delete topic. Delete topic through the admin tool will have no effect if this config is turned off</description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>compression.type</name>
     <description>Specify the final compression type for a given topic. This configuration accepts the standard compression codecs ('gzip', 'snappy', lz4). It additionally accepts 'uncompressed' which is equivalent to no compression; and 'producer' which means retain the original compression codec set by the producer.</description>
     <value>producer</value>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>external.kafka.metrics.exclude.prefix</name>
@@ -543,7 +545,7 @@
     <description>
       Exclude metrics starting with these prefixes from being collected.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>external.kafka.metrics.include.prefix</name>
@@ -551,84 +553,97 @@
     <description>
       These metrics would be included even if the exclude prefix omits them.
     </description>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>authorizer.class.name</name>
+    <description>
+      Kafka authorizer class
+    </description>
+    <depends-on>
+      <property>
+        <type>ranger-kafka-plugin-properties</type>
+        <name>ranger-kafka-plugin-enabled</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
   </property>
   <property>
     <name>sasl.enabled.mechanisms</name>
     <value>GSSAPI</value>
     <description>The list of SASL mechanisms enabled in the Kafka server. The list may contain any mechanism for which a security provider is available. Only GSSAPI is enabled by default.</description>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>security.inter.broker.protocol</name>
     <value>PLAINTEXT</value>
     <description>Security protocol used to communicate between brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL. It is an error to set this and inter.broker.listener.name properties at the same time.</description>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>sasl.mechanism.inter.broker.protocol</name>
     <value>GSSAPI</value>
     <description>SASL mechanism used for inter-broker communication. Default is GSSAPI.</description>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>ssl.client.auth</name>
     <value>none</value>
     <description>Configures kafka broker to request client authentication.</description>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>ssl.key.password</name>
-    <value />
+    <value/>
     <description>The password of private key in the key store file.</description>
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
       <type>password</type>
     </value-attributes>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>ssl.keystore.location</name>
-    <value />
+    <value/>
     <description>The location of key store file.</description>
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
     </value-attributes>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>ssl.keystore.password</name>
-    <value />
+    <value/>
     <description>The store password for key store file.</description>
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
       <type>password</type>
     </value-attributes>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>ssl.truststore.location</name>
-    <value />
+    <value/>
     <description>The location of trust store file.</description>
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
     </value-attributes>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>ssl.truststore.password</name>
-    <value />
+    <value/>
     <description>The password for trust store file. If a password is not set access to the truststore is still available, but integrity checking is disabled</description>
     <value-attributes>
       <empty-value-valid>true</empty-value-valid>
       <type>password</type>
     </value-attributes>
-    <on-ambari-upgrade add="true" />
+    <on-ambari-upgrade add="true"/>
   </property>
   <property>
     <name>producer.metrics.enable</name>
     <value>false</value>
-    <on-ambari-upgrade add="false" />
+    <on-ambari-upgrade add="false"/>
     <depends-on>
       <property>
         <type>streams-messaging-manager-common</type>
@@ -636,4 +651,4 @@
       </property>
     </depends-on>
   </property>
-</configuration>
\ No newline at end of file
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/usersync-log4j.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/usersync-log4j.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/usersync-log4j.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/usersync-log4j.xml	(date 1719626239000)
@@ -0,0 +1,103 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_adding_forbidden="false">
+  <property>
+    <name>ranger_usersync_log_maxfilesize</name>
+    <value>256</value>
+   <description>The maximum size of backup file before the log is rotated</description>
+    <display-name>Ranger usersync Log: backup file size</display-name>
+    <value-attributes>
+      <unit>MB</unit>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+   </property>
+   <property>
+    <name>ranger_usersync_log_maxbackupindex</name>
+    <value>20</value>
+    <description>The number of backup files</description>
+    <display-name>Ranger usersync Log: # of backup files</display-name>
+    <value-attributes>
+      <type>int</type>
+      <minimum>0</minimum>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>usersync-log4j template</display-name>
+    <description>usersync-log4j.properties</description>
+    <value>
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+log4j.rootLogger = info,logFile
+
+# logFile
+log4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender
+log4j.appender.logFile.file=${logdir}/usersync.log
+log4j.appender.logFile.datePattern='.'yyyy-MM-dd
+log4j.appender.logFile.layout=org.apache.log4j.PatternLayout
+log4j.appender.logFile.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %m%n
+log4j.appender.logFile.MaxFileSize={{ranger_usersync_log_maxfilesize}}MB
+
+# console
+log4j.appender.console=org.apache.log4j.ConsoleAppender
+log4j.appender.console.Target=System.out
+log4j.appender.console.layout=org.apache.log4j.PatternLayout
+log4j.appender.console.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss} %5p %c{1} [%t] - %m%n
+    </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  
+  <property>
+    <name>logback-content</name>
+    <display-name>logback.xml template</display-name>
+    <description>logback.xml</description>
+    <value/>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value-attributes>
+      <property-file-name>usersync-logback.xml.j2</property-file-name>
+      <property-file-type>xml</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  
+  
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_jaas_conf.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_jaas_conf.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_jaas_conf.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_jaas_conf.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_jaas_conf.xml	(date 1719626239000)
@@ -19,98 +19,98 @@
  * limitations under the License.
  */
 -->
-<configuration supports_adding_forbidden="true" supports_final="false">
-    <property>
-        <name>content</name>
-        <display-name>kafka_jaas template</display-name>
-        <description>Kafka jaas config</description>
-        <value>
-          /**
-          * Example of SASL/PLAIN Configuration
-          *
-          * KafkaServer {
-          *   org.apache.kafka.common.security.plain.PlainLoginModule required
-          *   username="admin"
-          *   password="admin-secret"
-          *   user_admin="admin-secret"
-          *   user_alice="alice-secret";
-          *   };
-          *
-          * Example of SASL/SCRAM
-          *
-          * KafkaServer {
-          *   org.apache.kafka.common.security.scram.ScramLoginModule required
-          *   username="admin"
-          *   password="admin-secret"
-          *   };
-          *
-          * Example of Enabling multiple SASL mechanisms in a broker:
-          *
-          *   KafkaServer {
-          *
-          *    com.sun.security.auth.module.Krb5LoginModule required
-          *    useKeyTab=true
-          *    storeKey=true
-          *    keyTab="/etc/security/keytabs/kafka_server.keytab"
-          *    principal="kafka/kafka1.hostname.com@EXAMPLE.COM";
-          *
-          *    org.apache.kafka.common.security.plain.PlainLoginModule required
-          *    username="admin"
-          *    password="admin-secret"
-          *    user_admin="admin-secret"
-          *    user_alice="alice-secret";
-          *
-          *    org.apache.kafka.common.security.scram.ScramLoginModule required
-          *    username="scram-admin"
-          *    password="scram-admin-secret";
-          *    };
-          *
-          **/
-  
-          {% if kerberos_security_enabled %}
-  
-          KafkaServer {
-          com.sun.security.auth.module.Krb5LoginModule required
-          useKeyTab=true
-          keyTab="{{kafka_keytab_path}}"
-          storeKey=true
-          useTicketCache=false
-          serviceName="{{kafka_bare_jaas_principal}}"
-          principal="{{kafka_jaas_principal}}";
-          };
-          KafkaClient {
-          com.sun.security.auth.module.Krb5LoginModule required
-          useTicketCache=true
-          renewTicket=true
-          serviceName="{{kafka_bare_jaas_principal}}";
-          };
-          Client {
-          com.sun.security.auth.module.Krb5LoginModule required
-          useKeyTab=true
-          keyTab="{{kafka_keytab_path}}"
-          storeKey=true
-          useTicketCache=false
-          serviceName="zookeeper"
-          principal="{{kafka_jaas_principal}}";
-          };
-          com.sun.security.jgss.krb5.initiate {
-          com.sun.security.auth.module.Krb5LoginModule required
-          renewTGT=false
-          doNotPrompt=true
-          useKeyTab=true
-          keyTab="{{kafka_keytab_path}}"
-          storeKey=true
-          useTicketCache=false
-          serviceName="{{kafka_bare_jaas_principal}}"
-          principal="{{kafka_jaas_principal}}";
-          };
-  
-          {% endif %}
-     </value>
-        <value-attributes>
-            <type>content</type>
-            <show-property-name>false</show-property-name>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-</configuration>
\ No newline at end of file
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <property>
+    <name>content</name>
+    <display-name>kafka_jaas template</display-name>
+    <description>Kafka jaas config</description>
+    <value>
+        /**
+        * Example of SASL/PLAIN Configuration
+        *
+        * KafkaServer {
+        *   org.apache.kafka.common.security.plain.PlainLoginModule required
+        *   username="admin"
+        *   password="admin-secret"
+        *   user_admin="admin-secret"
+        *   user_alice="alice-secret";
+        *   };
+        *
+        * Example of SASL/SCRAM
+        *
+        * KafkaServer {
+        *   org.apache.kafka.common.security.scram.ScramLoginModule required
+        *   username="admin"
+        *   password="admin-secret"
+        *   };
+        *
+        * Example of Enabling multiple SASL mechanisms in a broker:
+        *
+        *   KafkaServer {
+        *
+        *    com.sun.security.auth.module.Krb5LoginModule required
+        *    useKeyTab=true
+        *    storeKey=true
+        *    keyTab="/etc/security/keytabs/kafka_server.keytab"
+        *    principal="kafka/kafka1.hostname.com@EXAMPLE.COM";
+        *
+        *    org.apache.kafka.common.security.plain.PlainLoginModule required
+        *    username="admin"
+        *    password="admin-secret"
+        *    user_admin="admin-secret"
+        *    user_alice="alice-secret";
+        *
+        *    org.apache.kafka.common.security.scram.ScramLoginModule required
+        *    username="scram-admin"
+        *    password="scram-admin-secret";
+        *    };
+        *
+        **/
+
+        {% if kerberos_security_enabled %}
+
+        KafkaServer {
+        com.sun.security.auth.module.Krb5LoginModule required
+        useKeyTab=true
+        keyTab="{{kafka_keytab_path}}"
+        storeKey=true
+        useTicketCache=false
+        serviceName="{{kafka_bare_jaas_principal}}"
+        principal="{{kafka_jaas_principal}}";
+        };
+        KafkaClient {
+        com.sun.security.auth.module.Krb5LoginModule required
+        useTicketCache=true
+        renewTicket=true
+        serviceName="{{kafka_bare_jaas_principal}}";
+        };
+        Client {
+        com.sun.security.auth.module.Krb5LoginModule required
+        useKeyTab=true
+        keyTab="{{kafka_keytab_path}}"
+        storeKey=true
+        useTicketCache=false
+        serviceName="zookeeper"
+        principal="{{kafka_jaas_principal}}";
+        };
+        com.sun.security.jgss.krb5.initiate {
+        com.sun.security.auth.module.Krb5LoginModule required
+        renewTGT=false
+        doNotPrompt=true
+        useKeyTab=true
+        keyTab="{{kafka_keytab_path}}"
+        storeKey=true
+        useTicketCache=false
+        serviceName="{{kafka_bare_jaas_principal}}"
+        principal="{{kafka_jaas_principal}}";
+        };
+
+        {% endif %}
+   </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/admin-properties.xml	(date 1719626239000)
@@ -0,0 +1,167 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="false">
+  <property>
+    <name>SQL_CONNECTOR_JAR</name>
+    <value>{{driver_curl_target}}</value>
+    <display-name>Location of Sql Connector Jar</display-name>
+    <description>Location of DB client library (please check the location of the jar file)</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>admin-properties</type>
+        <name>DB_FLAVOR</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false" update="false"/>
+  </property>
+  <property>
+    <name>db_root_user</name>
+    <value>root</value>
+    <display-name>Database Administrator (DBA) username</display-name>
+    <description>Database admin user. This user should have DBA permission to create the Ranger Database and Ranger Database User</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>db_root_password</name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <display-name>Database Administrator (DBA) password</display-name>
+    <description>Database password for the database admin username</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_host</name>
+    <value/>
+    <display-name>Ranger DB host</display-name>
+    <description>Database host</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_name</name>
+    <value>ranger</value>
+    <display-name>Ranger DB name</display-name>
+    <description>Database name</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>db_user</name>
+    <value>rangeradmin</value>
+    <display-name>Ranger DB username</display-name>
+    <description>Database username used for the Ranger schema</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property require-input="true">
+    <name>db_password</name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <display-name>Ranger DB password</display-name>
+    <description>Database password for the Ranger schema</description>
+    <value-attributes>
+      <type>password</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>DB_FLAVOR</name>
+    <value>MYSQL</value>
+    <display-name>DB FLAVOR</display-name>
+    <description>The database type to be used (mysql/oracle)</description>
+    <value-attributes>
+      <overridable>false</overridable>
+      <type>value-list</type>
+      <entries>
+        <entry>
+          <value>MYSQL</value>
+          <label>MYSQL</label>
+        </entry>
+        <entry>
+          <value>ORACLE</value>
+          <label>ORACLE</label>
+        </entry>
+        <entry>
+          <value>POSTGRES</value>
+          <label>POSTGRES</label>
+        </entry>
+        <entry>
+          <value>MSSQL</value>
+          <label>MSSQL</label>
+        </entry>
+        <entry>
+          <value>SQLA</value>
+          <label>SQL Anywhere</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>policymgr_external_url</name>
+    <value/>
+    <display-name>External URL</display-name>
+    <description>Policy Manager external url eg: http://RANGER_HOST:6080</description>
+    <value-attributes>
+      <overridable>false</overridable>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.service.http.enabled</name>
+      </property>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.service.http.port</name>
+      </property>
+      <property>
+        <type>ranger-admin-site</type>
+        <name>ranger.service.https.port</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>PATCH_RETRY_INTERVAL</name>
+    <value>120</value>
+    <display-name>Retry Interval</display-name>
+    <description>Retry interval to apply database patches in seconds.</description>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_client_jaas_conf.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_client_jaas_conf.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_client_jaas_conf.xml
--- a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_client_jaas_conf.xml	(revision 2c8fe82f38027e96384cf8087337cbdcab29cbac)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/KAFKA/configuration/kafka_client_jaas_conf.xml	(date 1719626239000)
@@ -19,25 +19,25 @@
  * limitations under the License.
  */
 -->
-<configuration supports_adding_forbidden="true" supports_final="false">
-    <property>
-        <name>content</name>
-        <display-name>kafka_client_jaas template</display-name>
-        <description>Kafka client jaas config</description>
-        <value>
-  {% if kerberos_security_enabled %}
-  KafkaClient {
-  com.sun.security.auth.module.Krb5LoginModule required
-  useTicketCache=true
-  renewTicket=true
-  serviceName="{{kafka_bare_jaas_principal}}";
-  };
-  {% endif %}
-     </value>
-        <value-attributes>
-            <type>content</type>
-            <show-property-name>false</show-property-name>
-        </value-attributes>
-        <on-ambari-upgrade add="false" />
-    </property>
-</configuration>
\ No newline at end of file
+<configuration supports_final="false" supports_adding_forbidden="true">
+  <property>
+    <name>content</name>
+    <display-name>kafka_client_jaas template</display-name>
+    <description>Kafka client jaas config</description>
+    <value>
+{% if kerberos_security_enabled %}
+KafkaClient {
+com.sun.security.auth.module.Krb5LoginModule required
+useTicketCache=true
+renewTicket=true
+serviceName="{{kafka_bare_jaas_principal}}";
+};
+{% endif %}
+   </value>
+    <value-attributes>
+      <type>content</type>
+      <show-property-name>false</show-property-name>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/atlas-tagsync-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/atlas-tagsync-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/atlas-tagsync-ssl.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/atlas-tagsync-ssl.xml	(date 1719626239000)
@@ -0,0 +1,78 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <description>password for keystore</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>java truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file{{atlas_tagsync_credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false" />
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file{{atlas_tagsync_credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false" />
+  </property>
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-admin-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-admin-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-admin-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-admin-site.xml	(date 1719626239000)
@@ -0,0 +1,855 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<configuration supports_final="true">
+    <property>
+        <name>ranger.service.host</name>
+        <value>{{ranger_host}}</value>
+        <description>Host where ranger service to be installed</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.http.enabled</name>
+        <value>true</value>
+        <display-name>HTTP enabled</display-name>
+        <description>Enable HTTP</description>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.http.port</name>
+        <value>6080</value>
+        <description>HTTP port</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.https.port</name>
+        <value>6182</value>
+        <description>HTTPS port (if SSL is enabled)</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.https.attrib.ssl.enabled</name>
+        <value>false</value>
+        <description>true/false, set to true if using SSL</description>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.https.attrib.clientAuth</name>
+        <value>want</value>
+        <description>Needs to be set to want for two way SSL</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.https.attrib.keystore.keyalias</name>
+        <value>rangeradmin</value>
+        <description>Alias for Ranger Admin key in keystore</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.https.attrib.keystore.pass</name>
+        <value>xasecure</value>
+        <property-type>PASSWORD</property-type>
+        <description>Password for keystore</description>
+        <value-attributes>
+            <type>password</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.https.attrib.keystore.file</name>
+        <value>/etc/security/serverKeys/ranger-admin-keystore.jks</value>
+        <description>Ranger admin keystore (specify full path)</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.externalurl</name>
+        <value>{{ranger_external_url}}</value>
+        <display-name>External URL</display-name>
+        <description>URL to be used by clients to access ranger admin</description>
+        <value-attributes>
+            <visible>false</visible>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.jpa.jdbc.driver</name>
+        <value>com.mysql.jdbc.Driver</value>
+        <display-name>Driver class name for a JDBC Ranger database</display-name>
+        <description>JDBC driver class name. Example: For MySQL / MariaDB: com.mysql.jdbc.Driver, For Oracle:
+            oracle.jdbc.OracleDriver
+        </description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>admin-properties</type>
+                <name>DB_FLAVOR</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.jpa.jdbc.url</name>
+        <value>jdbc:mysql://localhost</value>
+        <display-name>JDBC connect string for a Ranger database</display-name>
+        <description>JDBC connect string</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>admin-properties</type>
+                <name>DB_FLAVOR</name>
+            </property>
+            <property>
+                <type>admin-properties</type>
+                <name>db_host</name>
+            </property>
+            <property>
+                <type>admin-properties</type>
+                <name>db_name</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.jpa.jdbc.user</name>
+        <value>{{ranger_db_user}}</value>
+        <description>JDBC user</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.jpa.jdbc.password</name>
+        <value>_</value>
+        <property-type>PASSWORD</property-type>
+        <description>JDBC password</description>
+        <value-attributes>
+            <type>password</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.jpa.jdbc.credential.alias</name>
+        <value>rangeradmin</value>
+        <description>Alias name for storing JDBC password</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.credential.provider.path</name>
+        <value>/etc/ranger/admin/rangeradmin.jceks</value>
+        <description>File for credential store, provide full file path</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.audit.source.type</name>
+        <value>solr</value>
+        <description>db or solr, based on the audit destination used</description>
+        <depends-on>
+            <property>
+                <type>ranger-env</type>
+                <name>xasecure.audit.destination.solr</name>
+            </property>
+            <property>
+                <type>ranger-env</type>
+                <name>xasecure.audit.destination.db</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.audit.solr.urls</name>
+        <value>http://solr_host:8983/solr/ranger_audit</value>
+        <description>Solr url for audit. 请修改 solr_host</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.authentication.method</name>
+        <value>UNIX</value>
+        <display-name>Authentication method</display-name>
+        <description>Ranger admin Authentication - UNIX/PAM/LDAP/AD/NONE</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>ranger-ugsync-site</type>
+                <name>ranger.usersync.source.impl.class</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.url</name>
+        <display-name>​LDAP URL</display-name>
+        <value>{{ranger_ug_ldap_url}}</value>
+        <description>LDAP Server URL, only used if Authentication method is LDAP</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.user.dnpattern</name>
+        <display-name>Ldap User DN Pattern</display-name>
+        <value>uid={0},ou=users,dc=xasecure,dc=net</value>
+        <description>LDAP user DN, only used if Authentication method is LDAP</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.group.searchbase</name>
+        <display-name>Group Search Base</display-name>
+        <value>{{ranger_ug_ldap_group_searchbase}}</value>
+        <description>LDAP group searchbase, only used if Authentication method is LDAP</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.group.searchfilter</name>
+        <display-name>Group Search Filter</display-name>
+        <value>{{ranger_ug_ldap_group_searchfilter}}</value>
+        <description>LDAP group search filter, only used if Authentication method is LDAP</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.group.roleattribute</name>
+        <display-name>Ldap Group Role Attribute</display-name>
+        <value>cn</value>
+        <description>LDAP group role attribute, only used if Authentication method is LDAP</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.base.dn</name>
+        <display-name>Ldap Base DN</display-name>
+        <value>dc=example,dc=com</value>
+        <description>The Distinguished Name (DN) of the starting point for directory server searches.</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.bind.dn</name>
+        <display-name>Bind User</display-name>
+        <value>{{ranger_ug_ldap_bind_dn}}</value>
+        <description>Full distinguished name (DN), including common name (CN), of an LDAP user account that has
+            privileges to search for users.
+        </description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.bind.password</name>
+        <display-name>​Bind User Password</display-name>
+        <value></value>
+        <property-type>PASSWORD</property-type>
+        <description>Password for the account that can search for users</description>
+        <value-attributes>
+            <type>password</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.referral</name>
+        <display-name>Ldap Referral</display-name>
+        <value>ignore</value>
+        <description>Set to follow if multiple LDAP servers are configured to return continuation references for
+            results. Set to ignore (default) if no referrals should be followed. Possible values are follow|throw|ignore
+        </description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.domain</name>
+        <display-name>Domain Name (Only for AD)</display-name>
+        <value/>
+        <description>AD domain, only used if Authentication method is AD</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.url</name>
+        <display-name>AD URL</display-name>
+        <value>{{ranger_ug_ldap_url}}</value>
+        <description>AD URL, only used if Authentication method is AD</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.base.dn</name>
+        <display-name>AD Base DN</display-name>
+        <value>dc=example,dc=com</value>
+        <description>The Distinguished Name (DN) of the starting point for directory server searches.</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.bind.dn</name>
+        <display-name>AD Bind DN</display-name>
+        <value>{{ranger_ug_ldap_bind_dn}}</value>
+        <description>Full distinguished name (DN), including common name (CN), of an LDAP user account that has
+            privileges to search for users.
+        </description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.bind.password</name>
+        <display-name>AD Bind Password</display-name>
+        <value></value>
+        <property-type>PASSWORD</property-type>
+        <description>Password for the account that can search for users</description>
+        <value-attributes>
+            <type>password</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.referral</name>
+        <display-name>AD Referral</display-name>
+        <value>ignore</value>
+        <description>Set to follow if multiple LDAP servers are configured to return continuation references for
+            results. Set to ignore (default) if no referrals should be followed. Possible values are follow|throw|ignore
+        </description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.unixauth.remote.login.enabled</name>
+        <value>true</value>
+        <display-name>Allow remote Login</display-name>
+        <description>Remote login enabled? - only used if Authentication method is UNIX</description>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+            <type>value-list</type>
+            <overridable>false</overridable>
+            <entries>
+                <entry>
+                    <value>true</value>
+                    <label>Yes</label>
+                </entry>
+                <entry>
+                    <value>false</value>
+                    <label>No</label>
+                </entry>
+            </entries>
+            <selection-cardinality>1</selection-cardinality>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.unixauth.service.hostname</name>
+        <display-name>Unix Auth Service Hostname</display-name>
+        <value>{{ugsync_host}}</value>
+        <description>Host where unix authentication service is running - only used if Authentication method is UNIX
+        </description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.unixauth.service.port</name>
+        <display-name>Unix Auth Service Port</display-name>
+        <value>5151</value>
+        <description>Port for unix authentication service - only used if Authentication method is UNIX</description>
+        <value-attributes>
+            <type>int</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.jpa.jdbc.dialect</name>
+        <value>{{jdbc_dialect}}</value>
+        <description>JDBC dialect used for policy DB</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.audit.solr.username</name>
+        <value>ranger_solr</value>
+        <description>Solr username</description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.audit.solr.password</name>
+        <value>NONE</value>
+        <property-type>PASSWORD</property-type>
+        <description>Solr password</description>
+        <value-attributes>
+            <type>password</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.sso.providerurl</name>
+        <value/>
+        <display-name>SSO provider url</display-name>
+        <description>Example: https://KNOX_HOST:KNOX_PORT/gateway/TOPOLOGY_NAME/knoxsso/api/v1/websso</description>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>gateway-site</type>
+                <name>gateway.port</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.sso.publicKey</name>
+        <value/>
+        <display-name>SSO public key</display-name>
+        <description>Public key for SSO cookie verification</description>
+        <value-attributes>
+            <type>multiLine</type>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.sso.enabled</name>
+        <value>false</value>
+        <display-name>Enable Ranger SSO</display-name>
+        <description/>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.sso.browser.useragent</name>
+        <value>Mozilla,chrome</value>
+        <display-name>SSO browser useragent</display-name>
+        <description>Comma seperated browser agent</description>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.binddn.credential.alias</name>
+        <value>ranger.ldap.bind.password</value>
+        <description></description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.binddn.credential.alias</name>
+        <value>ranger.ldap.ad.bind.password</value>
+        <description></description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.admin.kerberos.token.valid.seconds</name>
+        <value>30</value>
+        <description/>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.admin.kerberos.cookie.domain</name>
+        <value>{{ranger_host}}</value>
+        <description/>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.admin.kerberos.cookie.path</name>
+        <value>/</value>
+        <description/>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.spnego.kerberos.principal</name>
+        <value>*</value>
+        <description/>
+        <property-type>KERBEROS_PRINCIPAL</property-type>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.spnego.kerberos.keytab</name>
+        <value/>
+        <description/>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.admin.kerberos.principal</name>
+        <value/>
+        <description/>
+        <property-type>KERBEROS_PRINCIPAL</property-type>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.admin.kerberos.keytab</name>
+        <value/>
+        <description/>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.lookup.kerberos.principal</name>
+        <value/>
+        <description/>
+        <property-type>KERBEROS_PRINCIPAL</property-type>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.lookup.kerberos.keytab</name>
+        <value/>
+        <description/>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.truststore.file</name>
+        <value>/etc/ranger/admin/conf/ranger-admin-keystore.jks</value>
+        <display-name>ranger.truststore.file</display-name>
+        <description>Ranger trust-store file-path</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.truststore.password</name>
+        <value>changeit</value>
+        <property-type>PASSWORD</property-type>
+        <value-attributes>
+            <type>password</type>
+        </value-attributes>
+        <display-name>ranger.truststore.password</display-name>
+        <description>Ranger trust-store password</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.audit.solr.zookeepers</name>
+        <value>NONE</value>
+        <description>Solr Zookeeper string</description>
+        <depends-on>
+            <property>
+                <type>infra-solr-env</type>
+                <name>infra_solr_znode</name>
+            </property>
+            <property>
+                <type>ranger-env</type>
+                <name>is_solrCloud_enabled</name>
+            </property>
+            <property>
+                <type>ranger-env</type>
+                <name>is_external_solrCloud_enabled</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.ldap.ad.user.searchfilter</name>
+        <display-name>AD User Search Filter</display-name>
+        <value>(sAMAccountName={0})</value>
+        <description>Search filter used for Bind Authentication</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.ldap.user.searchfilter</name>
+        <display-name>User Search Filter</display-name>
+        <value>(uid={0})</value>
+        <description>Search filter used for Bind Authentication</description>
+        <value-attributes>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.kms.service.user.hdfs</name>
+        <value/>
+        <description/>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>hadoop-env</type>
+                <name>hdfs_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.kms.service.user.hive</name>
+        <value/>
+        <description/>
+        <value-attributes>
+            <empty-value-valid>true</empty-value-valid>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>hive-env</type>
+                <name>hive_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.hdfs.serviceuser</name>
+        <value>hdfs</value>
+        <depends-on>
+            <property>
+                <type>hadoop-env</type>
+                <name>hdfs_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.hive.serviceuser</name>
+        <value>hive</value>
+        <depends-on>
+            <property>
+                <type>hive-env</type>
+                <name>hive_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.hbase.serviceuser</name>
+        <value>hbase</value>
+        <depends-on>
+            <property>
+                <type>hbase-env</type>
+                <name>hbase_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.yarn.serviceuser</name>
+        <value>yarn</value>
+        <depends-on>
+            <property>
+                <type>yarn-env</type>
+                <name>yarn_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.knox.serviceuser</name>
+        <value>knox</value>
+        <depends-on>
+            <property>
+                <type>knox-env</type>
+                <name>knox_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.storm.serviceuser</name>
+        <value>storm</value>
+        <depends-on>
+            <property>
+                <type>storm-env</type>
+                <name>storm_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.kafka.serviceuser</name>
+        <value>kafka</value>
+        <depends-on>
+            <property>
+                <type>kafka-env</type>
+                <name>kafka_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.atlas.serviceuser</name>
+        <value>atlas</value>
+        <depends-on>
+            <property>
+                <type>atlas-env</type>
+                <name>metadata_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.kms.serviceuser</name>
+        <value>kms</value>
+        <depends-on>
+            <property>
+                <type>kms-env</type>
+                <name>kms_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.is.solr.kerberised</name>
+        <value>{{ranger_is_solr_kerberised}}</value>
+        <value-attributes>
+            <visible>false</visible>
+        </value-attributes>
+        <description/>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.truststore.alias</name>
+        <value>trustStoreAlias</value>
+        <description></description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.service.https.attrib.keystore.credential.alias</name>
+        <value>keyStoreCredentialAlias</value>
+        <description></description>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.ldap.starttls</name>
+        <value>false</value>
+        <description></description>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <depends-on>
+            <property>
+                <type>ranger-ugsync-site</type>
+                <name>ranger.usersync.ldap.starttls</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.logs.base.dir</name>
+        <display-name>Ranger Admin Log Dir</display-name>
+        <value>/var/log/ranger/admin</value>
+        <description>Ranger Admin Log Dir</description>
+        <value-attributes>
+            <type>directory</type>
+            <overridable>false</overridable>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+
+    <property>
+        <name>ranger.authentication.allow.trustedproxy</name>
+        <value>false</value>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.audit.solr.bootstrap.enabled</name>
+        <description>Enable Ranger Audit Solr Bootstrap. Keep this config disable for Ambari based installation.</description>
+        <value>false</value>
+        <value-attributes>
+            <overridable>false</overridable>
+            <type>boolean</type>
+        </value-attributes>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.admin.cookie.name</name>
+        <description>RANGERADMINSESSIONID</description>
+        <value>false</value>
+        <on-ambari-upgrade add="false"/>
+    </property>
+    <property>
+        <name>ranger.plugins.ozone.serviceuser</name>
+        <value>ozone</value>
+        <depends-on>
+            <property>
+                <type>ozone-env</type>
+                <name>ozone_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.plugins.doris.serviceuser</name>
+        <value>doris</value>
+        <depends-on>
+            <property>
+                <type>doris-env</type>
+                <name>doris_user</name>
+            </property>
+        </depends-on>
+        <on-ambari-upgrade add="true"/>
+    </property>
+    <property>
+        <name>ranger.admin.login.autolock.enabled</name>
+        <value>false</value>
+        <description>Enable Ranger Admin Portal Account Locking</description>
+        <on-ambari-upgrade add="true"/>
+    </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-ugsync-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-ugsync-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-ugsync-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-ugsync-site.xml	(date 1719626239000)
@@ -0,0 +1,610 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+-->
+<configuration supports_final="true">
+  <property>
+    <name>ranger.usersync.port</name>
+    <value>5151</value>
+    <description>Port for unix authentication service, run within usersync</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ssl</name>
+    <value>false</value>
+    <description>SSL enabled? (ranger admin -&gt; usersync communication)</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.keystore.password</name>
+    <value>UnIx529p</value>
+    <property-type>PASSWORD</property-type>
+    <description>Keystore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>Truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.passwordvalidator.path</name>
+    <value>./native/credValidator.uexe</value>
+    <description>Native program for password validation</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.enabled</name>
+    <display-name>Enable User Sync</display-name>
+    <value>true</value>
+    <description>Should users and groups be synchronized to Ranger Database? Required to setup Ranger policies</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.sink.impl.class</name>
+    <value>org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder</value>
+    <description>Class to be used as sink (to sync users into ranger admin)</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.policymanager.baseURL</name>
+    <value>{{ranger_external_url}}</value>
+    <description>URL to be used by clients to access ranger admin, use FQDN</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.policymanager.maxrecordsperapicall</name>
+    <value>1000</value>
+    <description>How many records to be returned per API call</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.policymanager.mockrun</name>
+    <value>false</value>
+    <description>Is user sync doing mock run?</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.unix.minUserId</name>
+    <display-name>Minimum User ID</display-name>
+    <value>500</value>
+    <description>Only sync users above this user id (applicable for UNIX)</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.unix.group.file</name>
+    <display-name>Group File</display-name>
+    <value>/etc/group</value>
+    <description>Location of the groups file on the linux server</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.unix.password.file</name>
+    <display-name>Password File</display-name>
+    <value>/etc/passwd</value>
+    <description>Location of the password file on the linux server</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.sleeptimeinmillisbetweensynccycle</name>
+    <value>60000</value>
+    <description>Sleeptime interval in milliseconds, if &lt; 6000 then default to 1 min</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.source.impl.class</name>
+    <value>org.apache.ranger.unixusersync.process.UnixUserGroupBuilder</value>
+    <display-name>Sync Source</display-name>
+    <description>For Ldap: org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder, For Unix: org.apache.ranger.unixusersync.process.UnixUserGroupBuilder, org.apache.ranger.unixusersync.process.FileSourceUserGroupBuilder</description>
+    <value-attributes>
+      <type>value-list</type>
+      <empty-value-valid>true</empty-value-valid>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>org.apache.ranger.unixusersync.process.UnixUserGroupBuilder</value>
+          <label>UNIX</label>
+        </entry>
+        <entry>
+          <value>org.apache.ranger.unixusersync.process.FileSourceUserGroupBuilder</value>
+          <label>FILE</label>
+        </entry>
+        <entry>
+          <value>org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder</value>
+          <label>LDAP/AD</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.filesource.file</name>
+    <display-name>File Name</display-name>
+    <value>/tmp/usergroup.txt</value>
+    <description>Path to the file with the users and groups information. Example: /tmp/usergroup.json or /tmp/usergroup.csv or /tmp/usergroup.txt</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.filesource.text.delimiter</name>
+    <display-name>Delimiter</display-name>
+    <value>,</value>
+    <description>Delimiter used in file, if File based user sync is used</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.url</name>
+    <display-name>LDAP/AD URL</display-name>
+    <value/>
+    <description>LDAP server URL. Example: value = ldap://localhost:389 or ldaps//localhost:636</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.binddn</name>
+    <display-name>​Bind User</display-name>
+    <value/>
+    <description>Full distinguished name (DN), including common name (CN), of an LDAP user account that has privileges to search for users. This user is used for searching the users. This could be read-only LDAP user. Example: cn=admin,dc=example,dc=com</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.ldapbindpassword</name>
+    <display-name>Bind User Password</display-name>
+    <value/>
+    <property-type>PASSWORD</property-type>
+    <description>Password for the LDAP bind user used for searching users.</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.bindalias</name>
+    <value>testldapalias</value>
+    <description>Set as ranger.usersync.ldap.bindalias (string as is)</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.usersync.ldap.searchBase</name>
+    <value>dc=hadoop,dc=apache,dc=org</value>
+    <description>"# search base for users and groups
+# sample value would be dc=hadoop,dc=apache,dc=org
+# From Ranger Release 0.6.0 multiple Ous can be configured with ; (semicolon) separated"</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.user.searchbase</name>
+    <display-name>User Search Base</display-name>
+    <value/>
+    <description>"# search base for users
+# sample value would be ou=users,dc=hadoop,dc=apache,dc=org
+# overrides value specified in ranger.usersync.ldap.searchBase
+# From Ranger Release 0.6.0 multiple Ous can be configured with ; (semicolon) separated eg: cn=users,dc=example,dc=com;ou=example1,ou=example2"</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.user.searchscope</name>
+    <display-name>User Search Scope</display-name>
+    <value>sub</value>
+    <description>"# search scope for the users, only base, one and sub are supported values
+# please customize the value to suit your deployment
+# default value: sub"</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.user.objectclass</name>
+    <display-name>User Object Class​</display-name>
+    <value>person</value>
+    <description>LDAP User Object Class. Example: person or user</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.user.searchfilter</name>
+    <display-name>​User Search Filter</display-name>
+    <value/>
+    <description>"optional additional filter constraining the users selected for syncing
+# a sample value would be (dept=eng)
+# please customize the value to suit your deployment
+# default value is empty"</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.user.nameattribute</name>
+    <display-name>Username Attribute</display-name>
+    <value/>
+    <description>LDAP user name attribute. Example: sAMAccountName in AD, uid or cn in OpenLDAP</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.referral</name>
+    <value>ignore</value>
+    <description>Set to follow if multiple LDAP servers are configured to return continuation references for results. Set to ignore (default) if no referrals should be followed</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.user.groupnameattribute</name>
+    <display-name>User Group Name Attribute</display-name>
+    <value>memberof, ismemberof</value>
+    <description>LDAP user group name attribute. Generally it is the same as username attribute. Example: sAMAccountName in AD, uid or cn in OpenLDAP</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.username.caseconversion</name>
+    <value>none</value>
+    <description>User name case conversion</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.groupname.caseconversion</name>
+    <value>none</value>
+    <description>Group name case conversion</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.logdir</name>
+    <display-name>Ranger Usersync Log Dir</display-name>
+    <value>/var/log/ranger/usersync</value>
+    <description>User sync log directory</description>
+    <value-attributes>
+      <type>directory</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.usermapsyncenabled</name>
+    <value>true</value>
+    <display-name>Group User Map Sync</display-name>
+    <description>Sync specific groups for users?</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.searchbase</name>
+    <display-name>Group Search Base</display-name>
+    <value/>
+    <description>"# search base for groups
+# sample value would be ou=groups,dc=hadoop,dc=apache,dc=org
+# overrides value specified in ranger.usersync.ldap.searchBase,  ranger.usersync.ldap.user.searchbase
+# if a value is not specified, takes the value of  ranger.usersync.ldap.searchBase
+# if  ranger.usersync.ldap.searchBase is also not specified, takes the value of ranger.usersync.ldap.user.searchbase"
+# From Ranger Release 0.6.0 multiple Ous can be configured with ; (semicolon) separated eg: ou=groups,DC=example,DC=com;ou=group1,ou=group2"
+</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.searchscope</name>
+    <value/>
+    <description>"# search scope for the groups, only base, one and sub are supported values
+# please customize the value to suit your deployment
+# default value: sub"</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.objectclass</name>
+    <display-name>Group Object Class</display-name>
+    <value/>
+    <description>LDAP Group object class. Example: group</description>
+    <on-ambari-upgrade add="true"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.searchfilter</name>
+    <value/>
+    <display-name>Group Search Filter</display-name>
+    <description>"# optional additional filter constraining the groups selected for syncing
+# a sample value would be (dept=eng)
+# please customize the value to suit your deployment
+# default value is empty"</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.nameattribute</name>
+    <display-name>Group Name Attribute</display-name>
+    <value/>
+    <description>LDAP group name attribute. Example: cn</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.memberattributename</name>
+    <display-name>Group Member Attribute</display-name>
+    <value/>
+    <description>LDAP group member attribute name. Example: member</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.pagedresultsenabled</name>
+    <value>true</value>
+    <description>Results can be paged?</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.pagedresultssize</name>
+    <value>500</value>
+    <description>Page size</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.kerberos.principal</name>
+    <value/>
+    <description/>
+    <property-type>KERBEROS_PRINCIPAL</property-type>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.kerberos.keytab</name>
+    <value/>
+    <description/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.policymgr.username</name>
+    <value>rangerusersync</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.policymgr.alias</name>
+    <value>ranger.usersync.policymgr.password</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.search.first.enabled</name>
+    <display-name>Enable Group Search First</display-name>
+    <value>false</value>
+    <description/>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.user.searchenabled</name>
+    <display-name>Enable User Search</display-name>
+    <value>false</value>
+    <description/>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.deltasync</name>
+    <display-name>Incremental Sync</display-name>
+    <value>true</value>
+    <description>Enable Incremental Sync</description>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.group.searchenabled</name>
+    <display-name>Enable Group Sync</display-name>
+    <value>true</value>
+    <description>"# do we want to do ldapsearch to find groups instead of relying on user entry attributes
+    # valid values: true, false
+    # any value other than true would be treated as false
+    # default value: false"</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>ranger-ugsync-site</type>
+        <name>ranger.usersync.ldap.deltasync</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.keystore.file</name>
+    <value>/usr/bigtop/current/ranger-usersync/conf/unixauthservice.jks</value>
+    <description>Keystore file used for usersync</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.truststore.file</name>
+    <value>/usr/bigtop/current/ranger-usersync/conf/mytruststore.jks</value>
+    <description>Truststore used for usersync, required if usersync -&gt; ranger admin communication is SSL enabled</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.bindkeystore</name>
+    <value/>
+    <description>Set same value as ranger.usersync.keystore.file property i.e default value /usr/bigtop/current/ranger-usersync/conf/ugsync.jceks</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.credstore.filename</name>
+    <value>/usr/bigtop/current/ranger-usersync/conf/ugsync.jceks</value>
+    <description>Credential store file name for user sync, specify full path</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.policymgr.keystore</name>
+    <value>/usr/bigtop/current/ranger-usersync/conf/ugsync.jceks</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.grouphierarchylevels</name>
+    <display-name>Group Hierarchy Levels</display-name>
+    <value>0</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.ldap.starttls</name>
+    <display-name>Enable LDAP STARTTLS</display-name>
+    <value>false</value>
+    <description>Enable LDAP STARTTLS</description>
+    <value-attributes>
+      <type>value-list</type>
+      <overridable>false</overridable>
+      <entries>
+        <entry>
+          <value>true</value>
+          <label>Yes</label>
+        </entry>
+        <entry>
+          <value>false</value>
+          <label>No</label>
+        </entry>
+      </entries>
+      <selection-cardinality>1</selection-cardinality>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.usersync.cookie.enabled</name>
+    <value>true</value>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-site.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-site.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-site.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-site.xml	(date 1719626239000)
@@ -0,0 +1,211 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration supports_final="true">
+  <property>
+    <name>ranger.tagsync.logdir</name>
+    <display-name>Ranger Tagsync Log Dir</display-name>
+    <value>/var/log/ranger/tagsync</value>
+    <description>Ranger Tagsync Log Dir</description>
+    <value-attributes>
+      <type>directory</type>
+      <overridable>false</overridable>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.dest.ranger.endpoint</name>
+    <value>{{ranger_external_url}}</value>
+    <description>Ranger TagAdmin REST URL</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlas</name>
+    <display-name>Enable Atlas Tag Source</display-name>
+    <value>false</value>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <depends-on>
+      <property>
+        <type>application-properties</type>
+        <name>atlas.server.bind.address</name>
+      </property>
+    </depends-on>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlasrest</name>
+    <display-name>Enable AtlasRest Tag Source</display-name>
+    <value>false</value>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.file</name>
+    <display-name>Enable File Tag Source</display-name>
+    <value>false</value>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.file.check.interval.millis</name>
+    <display-name>File Source: File update polling interval</display-name>
+    <value/>
+    <description/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlasrest.download.interval.millis</name>
+    <display-name>AtlasREST Source: Atlas source download interval</display-name>
+    <value>60000</value>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>ranger.tagsync.source.file.filename</name>
+    <display-name>File Source: Filename</display-name>
+    <value/>
+    <description>File Source Filename</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlasrest.endpoint</name>
+    <display-name>AtlasREST Source: Atlas endpoint</display-name>
+    <value/>
+    <description/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+    <depends-on>
+      <property>
+        <type>application-properties</type>
+        <name>atlas.server.http.port</name>
+      </property>
+      <property>
+        <type>application-properties</type>
+        <name>atlas.server.https.port</name>
+      </property>
+      <property>
+        <type>application-properties</type>
+        <name>atlas.enableTLS</name>
+      </property>
+    </depends-on>
+  </property>
+  <property>
+    <name>ranger.tagsync.kerberos.principal</name>
+    <value/>
+    <description/>
+    <property-type>KERBEROS_PRINCIPAL</property-type>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.kerberos.keytab</name>
+    <value/>
+    <description/>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.dest.ranger.username</name>
+    <value>rangertagsync</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlasrest.username</name>
+    <value>admin</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.atlas.default.cluster.name</name>
+    <value>{{cluster_name}}</value>
+    <description>Capture cluster name</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.keystore.filename</name>
+    <value>/usr/bigtop/current/ranger-tagsync/conf/rangertagsync.jceks</value>
+    <description>Keystore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlasrest.keystore.filename</name>
+    <value>/usr/bigtop/current/ranger-tagsync/conf/atlasuser.jceks</value>
+    <description>Tagsync atlasrest keystore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.dest.ranger.ssl.config.filename</name>
+    <value>{{stack_root}}/current/ranger-tagsync/conf/ranger-policymgr-ssl.xml</value>
+    <description>Keystore and truststore information used for tagsync, required if tagsync -&gt; ranger admin communication is SSL enabled</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.source.atlasrest.ssl.config.filename</name>
+    <value>{{stack_root}}/current/ranger-tagsync/conf/atlas-tagsync-ssl.xml</value>
+    <description>Keystore and truststore information used for tagsync, required if tagsync to atlas communication is SSL enabled</description>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger.tagsync.cookie.enabled</name>
+    <value>true</value>
+    <description/>
+    <value-attributes>
+      <type>boolean</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-solr-configuration.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-solr-configuration.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-solr-configuration.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-solr-configuration.xml	(date 1719626239000)
@@ -0,0 +1,59 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>ranger_audit_max_retention_days</name>
+    <display-name>Max Retention Days</display-name>
+    <description>Days to retain audit logs in Solr</description>
+    <value>90</value>
+    <value-attributes>
+      <type>int</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>ranger_audit_logs_merge_factor</name>
+    <display-name>Merge Factor</display-name>
+    <description>
+      The mergeFactor value tells Lucene how many segments of equal size to build before merging them into a
+      single segment. High value merge factor (e.g. 25) improves indexing speed, but slows down searching. Low value
+      (e.g. 5) improves searching, but slows down indexing.
+    </description>
+    <value>5</value>
+    <value-attributes>
+      <type>int</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>content</name>
+    <display-name>solr-config template</display-name>
+    <description>the jinja template for solrconfig.xml file used for ranger audit logs</description>
+    <value/>
+    <property-type>VALUE_FROM_PROPERTY_FILE</property-type>
+    <value-attributes>
+      <property-file-name>ranger-solrconfig.xml.j2</property-file-name>
+      <property-file-type>xml</property-file-type>
+    </value-attributes>
+    <on-ambari-upgrade add="false" />
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-policymgr-ssl.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-policymgr-ssl.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-policymgr-ssl.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/ranger-tagsync-policymgr-ssl.xml	(date 1719626239000)
@@ -0,0 +1,78 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore</name>
+    <value/>
+    <description>Java Keystore files</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.password</name>
+    <value>myKeyFilePassword</value>
+    <property-type>PASSWORD</property-type>
+    <description>password for keystore</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore</name>
+    <value/>
+    <description>java truststore file</description>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.password</name>
+    <value>changeit</value>
+    <property-type>PASSWORD</property-type>
+    <description>java truststore password</description>
+    <value-attributes>
+      <type>password</type>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.keystore.credential.file</name>
+    <value>jceks://file{{ranger_tagsync_credential_file}}</value>
+    <description>java keystore credential file</description>
+    <on-ambari-upgrade add="false" />
+  </property>
+
+  <property>
+    <name>xasecure.policymgr.clientssl.truststore.credential.file</name>
+    <value>jceks://file{{ranger_tagsync_credential_file}}</value>
+    <description>java truststore credential file</description>
+    <on-ambari-upgrade add="false" />
+  </property>
+
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-application-properties.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-application-properties.xml b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-application-properties.xml
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/configuration/tagsync-application-properties.xml	(date 1719626239000)
@@ -0,0 +1,62 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>atlas.kafka.entities.group.id</name>
+    <display-name>Atlas Source: Kafka consumer group</display-name>
+    <value>ranger_entities_consumer</value>
+    <description/>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.kafka.bootstrap.servers</name>
+    <display-name>Atlas Source: Kafka endpoint</display-name>
+    <value>localhost:6667</value>
+    <description/>
+    <depends-on>
+      <property>
+        <type>kafka-broker</type>
+        <name>port</name>
+      </property>
+    </depends-on>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+  <property>
+    <name>atlas.kafka.zookeeper.connect</name>
+    <display-name>Atlas Source: Zookeeper endpoint</display-name>
+    <value>localhost:2181</value>
+    <description/>
+    <depends-on>
+      <property>
+        <type>zoo.cfg</type>
+        <name>clientPort</name>
+      </property>
+    </depends-on>
+    <value-attributes>
+      <empty-value-valid>true</empty-value-valid>
+    </value-attributes>
+    <on-ambari-upgrade add="false"/>
+  </property>
+</configuration>
Index: ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/kerberos.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/kerberos.json b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/kerberos.json
new file mode 100644
--- /dev/null	(date 1719626239000)
+++ b/ambari-server/src/main/resources/stacks/BIGTOP/3.2.0/services/RANGER/kerberos.json	(date 1719626239000)
@@ -0,0 +1,160 @@
+{
+  "services": [
+    {
+      "name": "RANGER",
+      "identities": [
+        {
+          "name": "ranger_spnego",
+          "reference": "/spnego"
+        },
+        {
+          "name": "ranger_smokeuser",
+          "reference": "/smokeuser"
+        }
+      ],
+      "configurations": [
+        {
+          "ranger-admin-site": {
+            "xasecure.audit.jaas.Client.loginModuleName": "com.sun.security.auth.module.Krb5LoginModule",
+            "xasecure.audit.jaas.Client.loginModuleControlFlag": "required",
+            "xasecure.audit.jaas.Client.option.useKeyTab": "true",
+            "xasecure.audit.jaas.Client.option.storeKey": "false",
+            "xasecure.audit.jaas.Client.option.serviceName": "solr"
+          }
+        }
+      ],
+      "components": [
+        {
+          "name": "RANGER_ADMIN",
+          "identities": [
+            {
+              "name": "rangeradmin",
+              "principal": {
+                "value": "rangeradmin/_HOST@${realm}",
+                "type" : "service",
+                "configuration": "ranger-admin-site/ranger.admin.kerberos.principal",
+                "local_username" : "${ranger-env/ranger_user}"
+              },
+              "keytab": {
+                "file": "${keytab_dir}/rangeradmin.service.keytab",
+                "owner": {
+                  "name": "${ranger-env/ranger_user}",
+                  "access": "r"
+                },
+                "configuration": "ranger-admin-site/ranger.admin.kerberos.keytab"
+              }
+            },
+            {
+              "name": "rangerlookup",
+              "principal": {
+                "value": "rangerlookup/_HOST@${realm}",
+                "configuration": "ranger-admin-site/ranger.lookup.kerberos.principal",
+                "type" : "service"
+              },
+              "keytab": {
+                "file": "${keytab_dir}/rangerlookup.service.keytab",
+                "owner": {
+                  "name": "${ranger-env/ranger_user}",
+                  "access": "r"
+                },
+                "configuration": "ranger-admin-site/ranger.lookup.kerberos.keytab"
+              }
+            },
+            {
+              "name": "ranger_ranger_admin_spnego",
+              "reference": "/spnego",
+              "keytab": {
+                "configuration": "ranger-admin-site/ranger.spnego.kerberos.keytab"
+              }
+            },
+            {
+              "name": "ranger_ranger_admin_rangeradmin",
+              "reference": "/RANGER/RANGER_ADMIN/rangeradmin",
+              "principal": {
+                "configuration": "ranger-admin-site/xasecure.audit.jaas.Client.option.principal"
+              },
+              "keytab": {
+                "configuration": "ranger-admin-site/xasecure.audit.jaas.Client.option.keyTab"
+              }
+            },
+            {
+              "name": "ranger_ranger_admin_infra-solr",
+              "reference": "/AMBARI_INFRA_SOLR/INFRA_SOLR/infra-solr",
+              "when" : {
+                "contains" : ["services", "AMBARI_INFRA_SOLR"]
+              }
+            }
+          ]
+        },
+        {
+          "name": "RANGER_USERSYNC",
+          "identities": [
+            {
+              "name": "rangerusersync",
+              "principal": {
+                "value": "rangerusersync/_HOST@${realm}",
+                "type" : "service",
+                "configuration" : "ranger-ugsync-site/ranger.usersync.kerberos.principal",
+                "local_username" : "rangerusersync"
+              },
+              "keytab": {
+                "file": "${keytab_dir}/rangerusersync.service.keytab",
+                "owner": {
+                  "name": "${ranger-env/ranger_user}",
+                  "access": "r"
+                },
+                "configuration": "ranger-ugsync-site/ranger.usersync.kerberos.keytab"
+              }
+            }
+          ]
+        },
+        {
+          "name": "RANGER_TAGSYNC",
+          "identities": [
+            {
+              "name": "rangertagsync",
+              "principal": {
+                "value": "rangertagsync/_HOST@${realm}",
+                "type" : "service",
+                "configuration": "ranger-tagsync-site/ranger.tagsync.kerberos.principal",
+                "local_username" : "rangertagsync"
+              },
+              "keytab": {
+                "file": "${keytab_dir}/rangertagsync.service.keytab",
+                "owner": {
+                  "name": "${ranger-env/ranger_user}",
+                  "access": "r"
+                },
+                "configuration": "ranger-tagsync-site/ranger.tagsync.kerberos.keytab"
+              }
+            },
+            {
+              "name": "ranger_ranger_tagsync_rangertagsync",
+              "reference": "/RANGER/RANGER_TAGSYNC/rangertagsync",
+              "principal": {
+                "configuration": "tagsync-application-properties/atlas.jaas.KafkaClient.option.principal"
+              },
+              "keytab": {
+                "configuration": "tagsync-application-properties/atlas.jaas.KafkaClient.option.keyTab"
+              }
+            }
+          ],
+          "configurations": [
+            {
+              "tagsync-application-properties": {
+                "atlas.jaas.KafkaClient.loginModuleName": "com.sun.security.auth.module.Krb5LoginModule",
+                "atlas.jaas.KafkaClient.loginModuleControlFlag": "required",
+                "atlas.jaas.KafkaClient.option.useKeyTab": "true",
+                "atlas.jaas.KafkaClient.option.storeKey": "true",
+                "atlas.jaas.KafkaClient.option.serviceName": "kafka",
+                "atlas.kafka.sasl.kerberos.service.name": "kafka",
+                "atlas.kafka.security.protocol": "SASL_PLAINTEXT",
+                "atlas.authentication.method.kerberos": "true"
+              }
+            }
+          ]
+        }
+      ]
+    }
+  ]
+}
